{"id2": 1001, "id3": 2, "content": "The recent advancements in artificial intelligence (AI) combined with theextensive amount of data generated by todays clinical systems, has led to thedevelopment of imaging AI solutions across the whole value chain of medicalimaging, including image reconstruction, medical image segmentation,image-based diagnosis and treatment planning. Notwithstanding the successes andfuture potential of AI in medical imaging, many stakeholders are concerned ofthe potential risks and ethical implications of imaging AI solutions, which areperceived as complex, opaque, and difficult to comprehend, utilise, and trustin critical clinical applications. Despite these concerns and risks, there arecurrently no concrete guidelines and best practices for guiding future AIdevelopments in medical imaging towards increased trust, safety and adoption.To bridge this gap, this paper introduces a careful selection of guidingprinciples drawn from the accumulated experiences, consensus, and bestpractices from five large European projects on AI in Health Imaging. Theseguiding principles are named FUTURE-AI and its building blocks consist of (i)Fairness, (ii) Universality, (iii) Traceability, (iv) Usability, (v) Robustnessand (vi) Explainability. In a step-by-step approach, these guidelines arefurther translated into a framework of concrete recommendations for specifying,developing, evaluating, and deploying technically, clinically and ethicallytrustworthy AI solutions into clinical practice."}
{"id2": 1002, "id3": 7, "content": "Semantic segmentation of fine-resolution urban scene images plays a vitalrole in extensive practical applications, such as land cover mapping, urbanchange detection, environmental protection and economic assessment. Driven byrapid developments in deep learning technologies, convolutional neural networks(CNNs) have dominated the semantic segmentation task for many years.Convolutional neural networks adopt hierarchical feature representation andhave strong local context extraction. However, the local property of theconvolution layer limits the network from capturing global information that iscrucial for improving fine-resolution image segmentation. Recently, Transformercomprise a hot topic in the computer vision domain. Vision Transformerdemonstrates the great capability of global information modelling, boostingmany vision tasks, such as image classification, object detection andespecially semantic segmentation. In this paper, we propose an efficient hybridTransformer (EHT) for semantic segmentation of urban scene images. EHT takesadvantage of CNNs and Transformer, learning global-local context to strengthenthe feature representation. Extensive experiments demonstrate that EHT hashigher efficiency with competitive accuracy compared with state-of-the-artbenchmark methods. Specifically, the proposed EHT achieves a 67.0% mIoU on theUAVid test set and outperforms other lightweight models significantly. The codewill be available soon."}
{"id2": 1003, "id3": 13, "content": "Deep learning has become in recent years a cornerstone tool fueling keyinnovations in the industry, such as autonomous driving. To attain goodperformances, the neural network architecture used for a given application mustbe chosen with care. These architectures are often handcrafted and thereforeprone to human biases and sub-optimal selection. Neural Architecture Search(NAS) is a framework introduced to mitigate such risks by jointly optimizingthe network architectures and its weights. Albeit its novelty, it was appliedon complex tasks with significant results - e.g. semantic image segmentation.In this technical paper, we aim to evaluate its ability to tackle a challengingoperational task: semantic segmentation of objects of interest in satelliteimagery. Designing a NAS framework is not trivial and has strong dependenciesto hardware constraints. We therefore motivate our NAS approach selection andprovide corresponding implementation details. We also present novel ideas tocarry out other such use-case studies."}
{"id2": 1004, "id3": 22, "content": "Detection faults in seismic data is a crucial step for seismic structuralinterpretation, reservoir characterization and well placement. Some recentworks regard it as an image segmentation task. The task of image segmentationrequires huge labels, especially 3D seismic data, which has a complex structureand lots of noise. Therefore, its annotation requires expert experience and ahuge workload. In this study, we present lambda-BCE and lambda-smooth L1loss toeffectively train 3D-CNN by some slices from 3D seismic data, so that the modelcan learn the segmentation of 3D seismic data from a few 2D slices. In order tofully extract information from limited data and suppress seismic noise, wepropose an attention module that can be used for active supervision trainingand embedded in the network. The attention heatmap label is generated by theoriginal label, and letting it supervise the attention module using thelambda-smooth L1loss. The experiment demonstrates the effectiveness of our lossfunction, the method can extract 3D seismic features from a few 2D slicelabels. And it also shows the advanced performance of the attention module,which can significantly suppress the noise in the seismic data while increasingthe models sensitivity to the foreground. Finally, on the public test set, weonly use the 2D slice labels training that accounts for 3.3% of the 3D volumelabel, and achieve similar performance to the 3D volume label training."}
{"id2": 1005, "id3": 33, "content": "Transformers have shown impressive performance in various natural languageprocessing and computer vision tasks, due to the capability of modelinglong-range dependencies. Recent progress has demonstrated to combine suchtransformers with CNN-based semantic image segmentation models is verypromising. However, it is not well studied yet on how well a pure transformerbased approach can achieve for image segmentation. In this work, we explore anovel framework for semantic image segmentation, which is encoder-decoder basedFully Transformer Networks (FTN). Specifically, we first propose a PyramidGroup Transformer (PGT) as the encoder for progressively learning hierarchicalfeatures, while reducing the computation complexity of the standard visualtransformer(ViT). Then, we propose a Feature Pyramid Transformer (FPT) to fusesemantic-level and spatial-level information from multiple levels of the PGTencoder for semantic image segmentation. Surprisingly, this simple baseline canachieve new state-of-the-art results on multiple challenging semanticsegmentation benchmarks, including PASCAL Context, ADE20K and COCO-Stuff. Thesource code will be released upon the publication of this work."}
{"id2": 1006, "id3": 38, "content": "Pre-training a recognition model with contrastive learning on a large datasetof unlabeled data has shown great potential to boost the performance of adownstream task, e.g., image classification. However, in domains such asmedical imaging, collecting unlabeled data can be challenging and expensive. Inthis work, we propose to adapt contrastive learning to work with meta-labelannotations, for improving the models performance in medical imagesegmentation even when no additional unlabeled data is available. Meta-labelssuch as the location of a 2D slice in a 3D MRI scan or the type of device used,often come for free during the acquisition process. We use the meta-labels forpre-training the image encoder as well as to regularize a semi-supervisedtraining, in which a reduced set of annotated data is used for training.Finally, to fully exploit the weak annotations, a self-paced learning approachis used to help the learning and discriminate useful labels from noise. Resultson three different medical image segmentation datasets show that our approach:i) highly boosts the performance of a model trained on a few scans, ii)outperforms previous contrastive and semi-supervised approaches, and iii)reaches close to the performance of a model trained on the full data."}
{"id2": 1007, "id3": 40, "content": "Membership inference attacks (MIA) try to detect if data samples were used totrain a neural network model, e.g. to detect copyright abuses. We show thatmodels with higher dimensional input and output are more vulnerable to MIA, andaddress in more detail models for image translation and semantic segmentation,including medical image segmentation. We show that reconstruction-errors canlead to very effective MIA attacks as they are indicative of memorization.Unfortunately, reconstruction error alone is less effective at discriminatingbetween non-predictable images used in training and easy to predict images thatwere never seen before. To overcome this, we propose using a novelpredictability error that can be computed for each sample, and its computationdoes not require a training set. Our membership error, obtained by subtractingthe predictability error from the reconstruction error, is shown to achievehigh MIA accuracy on an extensive number of benchmarks."}
{"id2": 1008, "id3": 45, "content": "The random walker method for image segmentation is a popular tool forsemi-automatic image segmentation, especially in the biomedical field. However,its linear asymptotic run time and memory requirements make application to 3Ddatasets of increasing sizes impractical. We propose a hierarchical frameworkthat, to the best of our knowledge, is the first attempt to overcome theserestrictions for the random walker algorithm and achieves sublinear run timeand constant memory complexity. The goal of this framework is -- rather thanimproving the segmentation quality compared to the baseline method -- to makeinteractive segmentation on out-of-core datasets possible. The method isevaluated quantitavely on synthetic data and the CT-ORG dataset where theexpected improvements in algorithm run time while maintaining high segmentationquality are confirmed. The incremental (i.e., interaction update) run time isdemonstrated to be in seconds on a standard PC even for volumes of hundreds ofGigabytes in size. In a small case study the applicability to large real worldfrom current biomedical research is demonstrated. An implementation of thepresented method is publicly available in version 5.2 of the widely used volumerendering and processing software Voreen (https://www.uni-muenster.de/Voreen/)."}
{"id2": 1009, "id3": 54, "content": "Shape modelling (with methods that output shapes) is a new and important taskin Bayesian nonparametrics and bioinformatics. In this work, we focus onBayesian nonparametric methods for capturing shapes by partitioning a spaceusing curves. In related work, the classical Mondrian process is used topartition spaces recursively with axis-aligned cuts, and is widely applied inmulti-dimensional and relational data. The Mondrian process outputshyper-rectangles. Recently, the random tessellation process was introduced as ageneralization of the Mondrian process, partitioning a domain with non-axisaligned cuts in an arbitrary dimensional space, and outputting polytopes.Motivated by these processes, in this work, we propose a novel parallelizedBayesian nonparametric approach to partition a domain with curves, enablingcomplex data-shapes to be acquired. We apply our method to HIV-1-infected humanmacrophage image dataset, and also simulated datasets sets to illustrate ourapproach. We compare to support vector machines, random forests andstate-of-the-art computer vision methods such as simple linear iterativeclustering super pixel image segmentation. We develop an R package that isavailable at url https://github.com/ShufeiGe/Shape-Modeling-with-Spline-Partitions ."}
{"id2": 1010, "id3": 72, "content": "In a class of piecewise-constant image segmentation models, we propose toincorporate a weighted difference of anisotropic and isotropic total variation(AITV) to regularize the partition boundaries in an image. In particular, wereplace the total variation regularization in the Chan-Vese segmentation modeland a fuzzy region competition model by the proposed AITV. To deal with thenonconvex nature of AITV, we apply the difference-of-convex algorithm (DCA), inwhich the subproblems can be minimized by the primal-dual hybrid gradientmethod with linesearch. The convergence of the DCA scheme is analyzed. Inaddition, a generalization to color image segmentation is discussed. In thenumerical experiments, we compare the proposed models with the classic convexapproaches and the two-stage segmentation methods (smoothing and thenthresholding) on various images, showing that our models are effective in imagesegmentation and robust with respect to impulsive noises."}
{"id2": 1011, "id3": 90, "content": "A strong visual object tracker nowadays relies on its well-crafted modules,which typically consist of manually-designed network architectures to deliverhigh-quality tracking results. Not surprisingly, the manual design processbecomes a particularly challenging barrier, as it demands sufficient priorexperience, enormous effort, intuition and perhaps some good luck. Meanwhile,neural architecture search has gaining grounds in practical applications suchas image segmentation, as a promising method in tackling the issue of automatedsearch of feasible network structures. In this work, we propose a novelcell-level differentiable architecture search mechanism to automate the networkdesign of the tracking module, aiming to adapt backbone features to theobjective of a tracking network during offline training. The proposed approachis simple, efficient, and with no need to stack a series of modules toconstruct a network. Our approach is easy to be incorporated into existingtrackers, which is empirically validated using different differentiablearchitecture search-based methods and tracking objectives. Extensiveexperimental evaluations demonstrate the superior performance of our approachover five commonly-used benchmarks. Meanwhile, our automated searching processtakes 41 (18) hours for the second (first) order DARTS method on theTrackingNet dataset."}
{"id2": 1012, "id3": 95, "content": "Tensor networks provide an efficient approximation of operations involvinghigh dimensional tensors and have been extensively used in modelling quantummany-body systems. More recently, supervised learning has been attempted withtensor networks, primarily focused on tasks such as image classification. Inthis work, we propose a novel formulation of tensor networks for supervisedimage segmentation which allows them to operate on high resolution medicalimages. We use the matrix product state (MPS) tensor network on non-overlappingpatches of a given input image to predict the segmentation mask by learning apixel-wise linear classification rule in a high dimensional space. The proposedmodel is end-to-end trainable using backpropagation. It is implemented as aStrided Tensor Network to reduce the parameter complexity. The performance ofthe proposed method is evaluated on two public medical imaging datasets andcompared to relevant baselines. The evaluation shows that the strided tensornetwork yields competitive performance compared to CNN-based models while usingfewer resources. Additionally, based on the experiments we discuss thefeasibility of using fully linear models for segmentation tasks."}
{"id2": 1013, "id3": 105, "content": "High-resolution image segmentation remains challenging and error-prone due tothe enormous size of intermediate feature maps. Conventional methods avoid thisproblem by using patch based approaches where each patch is segmentedindependently. However, independent patch segmentation induces errors,particularly at the patch boundary due to the lack of contextual information invery high-resolution images where the patch size is much smaller compared tothe full image. To overcome these limitations, in this paper, we propose anovel framework to segment a particular patch by incorporating contextualinformation from its neighboring patches. This allows the segmentation networkto see the target patch with a wider field of view without the need of largerfeature maps. Comparative analysis from a number of experiments shows that ourproposed framework is able to segment high resolution images with significantlyimproved mean Intersection over Union and overall accuracy."}
{"id2": 1014, "id3": 111, "content": "Learning multi-modal representations is an essential step towards real-worldrobotic applications, and various multi-modal fusion models have been developedfor this purpose. However, we observe that existing models, whose objectivesare mostly based on joint training, often suffer from learning inferiorrepresentations of each modality. We name this problem Modality Failure, andhypothesize that the imbalance of modalities and the implicit bias of commonobjectives in fusion method prevent encoders of each modality from sufficientfeature learning. To this end, we propose a new multi-modal learning method,Uni-Modal Teacher, which combines the fusion objective and uni-modaldistillation to tackle the modality failure problem. We show that our methodnot only drastically improves the representation of each modality, but alsoimproves the overall multi-modal task performance. Our method can beeffectively generalized to most multi-modal fusion approaches. We achieve morethan 3% improvement on the VGGSound audio-visual classification task, as wellas improving performance on the NYU depth V2 RGB-D image segmentation task."}
{"id2": 1015, "id3": 130, "content": "While cloud/sky image segmentation has extensive real-world applications, alarge amount of labelled data is needed to train a highly accurate models toperform the task. Scarcity of such volumes of cloud/sky images withcorresponding ground-truth binary maps makes it highly difficult to train suchcomplex image segmentation models. In this paper, we demonstrate theeffectiveness of using Generative Adversarial Networks (GANs) to generate datato augment the training set in order to increase the prediction accuracy ofimage segmentation model. We further present a way to estimate ground-truthbinary maps for the GAN-generated images to facilitate their effective use asaugmented images. Finally, we validate our work with different statisticaltechniques."}
{"id2": 1016, "id3": 139, "content": "Currently, developments of deep learning techniques are providinginstrumental to identify, classify, and quantify patterns in medical images.Segmentation is one of the important applications in medical image analysis. Inthis regard, U-Net is the predominant approach to medical image segmentationtasks. However, we found that those U-Net based models have limitations inseveral aspects, for example, millions of parameters in the U-Net consumingconsiderable computation resource and memory, lack of global information, andmissing some tough objects. Therefore, we applied two modifications to improvethe U-Net model: 1) designed and added the dilated channel-wise CNN module, 2)simplified the U shape network. Based on these two modifications, we proposed anovel light-weight architecture -- Channel-wise Feature Pyramid Network forMedicine (CFPNet-M). To evaluate our method, we selected five datasets withdifferent modalities: thermography, electron microscopy, endoscopy, dermoscopy,and digital retinal images. And we compared its performance with several modelshaving different parameter scales. This paper also involves our previousstudies of DC-UNet and some commonly used light-weight neural networks. Weapplied the Tanimoto similarity instead of the Jaccard index for gray-levelimage measurements. By comparison, CFPNet-M achieves comparable segmentationresults on all five medical datasets with only 0.65 million parameters, whichis about 2% of U-Net, and 8.8 MB memory. Meanwhile, the inference speed canreach 80 FPS on a single RTX 2070Ti GPU with the 256 by 192 pixels input size."}
{"id2": 1017, "id3": 146, "content": "Interactive single-image segmentation is ubiquitous in the scientific andcommercial imaging software. In this work, we focus on the single-imagesegmentation problem only with some seeds such as scribbles. Inspired by thedynamic receptive field in the human beings visual system, we propose theGaussian dynamic convolution (GDC) to fast and efficiently aggregate thecontextual information for neural networks. The core idea is randomly selectingthe spatial sampling area according to the Gaussian distribution offsets. OurGDC can be easily used as a module to build lightweight or complex segmentationnetworks. We adopt the proposed GDC to address the typical single-imagesegmentation tasks. Furthermore, we also build a Gaussian dynamic pyramidPooling to show its potential and generality in common semantic segmentation.Experiments demonstrate that the GDC outperforms other existing convolutions onthree benchmark segmentation datasets including Pascal-Context, Pascal-VOC2012, and Cityscapes. Additional experiments are also conducted to illustratethat the GDC can produce richer and more vivid features compared with otherconvolutions. In general, our GDC is conducive to the convolutional neuralnetworks to form an overall impression of the image."}
{"id2": 1018, "id3": 164, "content": "Minimal paths are regarded as a powerful and efficient tool for boundarydetection and image segmentation due to its global optimality and thewell-established numerical solutions such as fast marching method. In thispaper, we introduce a flexible interactive image segmentation model based onthe Eikonal partial differential equation (PDE) framework in conjunction withregion-based homogeneity enhancement. A key ingredient in the introduced modelis the construction of local geodesic metrics, which are capable of integratinganisotropic and asymmetric edge features, implicit region-based homogeneityfeatures and/or curvature regularization. The incorporation of the region-basedhomogeneity features into the metrics considered relies on an implicitrepresentation of these features, which is one of the contributions of thiswork. Moreover, we also introduce a way to build simple closed contours as theconcatenation of two disjoint open curves. Experimental results prove that theproposed model indeed outperforms state-of-the-art minimal paths-based imagesegmentation approaches."}
{"id2": 1019, "id3": 168, "content": "Citrus segmentation is a key step of automatic citrus picking. While mostcurrent image segmentation approaches achieve good segmentation results bypixel-wise segmentation, these supervised learning-based methods require alarge amount of annotated data, and do not consider the continuous temporalchanges of citrus position in real-world applications. In this paper, we firsttrain a simple CNN with a small number of labelled citrus images in asupervised manner, which can roughly predict the citrus location from eachframe. Then, we extend a state-of-the-art unsupervised learning approach topre-learn the citruss potential movements between frames from unlabelledcitruss videos. To take advantages of both networks, we employ the multimodaltransformer to combine supervised learned static information and unsupervisedlearned movement information. The experimental results show that combing bothnetwork allows the prediction accuracy reached at 88.3$ %$ IOU and 93.6$ %$precision, outperforming the original supervised baseline 1.2$ %$ and 2.4$ %$.Compared with most of the existing citrus segmentation methods, our method usesa small amount of supervised data and a large number of unsupervised data,while learning the pixel level location information and the temporalinformation of citrus changes to enhance the segmentation effect."}
{"id2": 1020, "id3": 186, "content": "The task of image segmentation is inherently noisy due to ambiguitiesregarding the exact location of boundaries between anatomical structures. Weargue that this information can be extracted from the expert annotations at noextra cost, and when integrated into state-of-the-art neural networks, it canlead to improved calibration between soft probabilistic predictions and theunderlying uncertainty. We built upon label smoothing (LS) where a network istrained on blurred versions of the ground truth labels which has been shownto be effective for calibrating output predictions. However, LS is not takingthe local structure into account and results in overly smoothed predictionswith low confidence even for non-ambiguous regions. Here, we propose SpatiallyVarying Label Smoothing (SVLS), a soft labeling technique that captures thestructural uncertainty in semantic segmentation. SVLS also naturally lendsitself to incorporate inter-rater uncertainty when multiple labelmaps areavailable. The proposed approach is extensively validated on four clinicalsegmentation tasks with different imaging modalities, number of classes andsingle and multi-rater expert annotations. The results demonstrate that SVLS,despite its simplicity, obtains superior boundary prediction with improveduncertainty and model calibration."}
{"id2": 1021, "id3": 212, "content": "Conventional deformable registration methods aim at solving an optimizationmodel carefully designed on image pairs and their computational costs areexceptionally high. In contrast, recent deep learning based approaches canprovide fast deformation estimation. These heuristic network architectures arefully data-driven and thus lack explicit geometric constraints, e.g.,topology-preserving, which are indispensable to generate plausibledeformations. We design a new deep learning based framework to optimize adiffeomorphic model via multi-scale propagation in order to integrateadvantages and avoid limitations of these two categories of approaches.Specifically, we introduce a generic optimization model to formulatediffeomorphic registration and develop a series of learnable architectures toobtain propagative updating in the coarse-to-fine feature space. Moreover, wepropose a novel bilevel self-tuned training strategy, allowing efficient searchof task-specific hyper-parameters. This training strategy increases theflexibility to various types of data while reduces computational and humanburdens. We conduct two groups of image registration experiments on 3D volumedatasets including image-to-atlas registration on brain MRI data andimage-to-image registration on liver CT data. Extensive results demonstrate thestate-of-the-art performance of the proposed method with diffeomorphicguarantee and extreme efficiency. We also apply our framework to challengingmulti-modal image registration, and investigate how our registration to supportthe down-streaming tasks for medical image analysis including multi-modalfusion and image segmentation."}
{"id2": 1022, "id3": 252, "content": "Deep co-training has recently been proposed as an effective approach forimage segmentation when annotated data is scarce. In this paper, we improveexisting approaches for semi-supervised segmentation with a self-paced andself-consistent co-training method. To help distillate information fromunlabeled images, we first design a self-paced learning strategy forco-training that lets jointly-trained neural networks focus oneasier-to-segment regions first, and then gradually consider harder ones.Thisis achieved via an end-to-end differentiable loss inthe form of a generalizedJensen Shannon Divergence(JSD). Moreover, to encourage predictions fromdifferent networks to be both consistent and confident, we enhance thisgeneralized JSD loss with an uncertainty regularizer based on entropy. Therobustness of individual models is further improved using a self-ensemblingloss that enforces their prediction to be consistent across different trainingiterations. We demonstrate the potential of our method on three challengingimage segmentation problems with different image modalities, using smallfraction of labeled data. Results show clear advantages in terms of performancecompared to the standard co-training baselines and recently proposedstate-of-the-art approaches for semi-supervised segmentation"}
{"id2": 1023, "id3": 272, "content": "We present a fully convolutional neural network (ConvNet), named RatLesNetv2,for segmenting lesions in rodent magnetic resonance (MR) brain images.RatLesNetv2 architecture resembles an autoencoder and it incorporates residualblocks that facilitate its optimization. RatLesNetv2 is trained end to end onthree-dimensional images and it requires no preprocessing. We evaluatedRatLesNetv2 on an exceptionally large dataset composed of 916 T2-weighted ratbrain MRI scans of 671 rats at nine different lesion stages that were used tostudy focal cerebral ischemia for drug development. In addition, we comparedits performance with three other ConvNets specifically designed for medicalimage segmentation. RatLesNetv2 obtained similar to higher Dice coefficientvalues than the other ConvNets and it produced much more realistic and compactsegmentations with notably fewer holes and lower Hausdorff distance. The Dicescores of RatLesNetv2 segmentations also exceeded inter-rater agreement ofmanual segmentations. In conclusion, RatLesNetv2 could be used for automatedlesion segmentation, reducing human workload and improving reproducibility.RatLesNetv2 is publicly available at https://github.com/jmlipman/RatLesNetv2."}
{"id2": 1024, "id3": 273, "content": "Unsupervised domain adaptation (UDA) for cross-modality medical imagesegmentation has shown great progress by domain-invariant feature learning orimage appearance translation. Adapted feature learning usually cannot detectdomain shifts at the pixel level and is not able to achieve good results indense semantic segmentation tasks. Image appearance translation, e.g. CycleGAN,translates images into different styles with good appearance, despite itspopulation, its semantic consistency is hardly to maintain and results in poorcross-modality segmentation. In this paper, we propose intra- andcross-modality semantic consistency (ICMSC) for UDA and our key insight is thatthe segmentation of synthesised images in different styles should beconsistent. Specifically, our model consists of an image translation module anda domain-specific segmentation module. The image translation module is astandard CycleGAN, while the segmentation module contains two domain-specificsegmentation networks. The intra-modality semantic consistency (IMSC) forcesthe reconstructed image after a cycle to be segmented in the same way as theoriginal input image, while the cross-modality semantic consistency (CMSC)encourages the synthesized images after translation to be segmented exactly thesame as before translation. Comprehensive experimental results oncross-modality hip joint bone segmentation show the effectiveness of ourproposed method, which achieves an average DICE of 81.61% on the acetabulum and88.16% on the proximal femur, outperforming other state-of-the-art methods. Itis worth to note that without UDA, a model trained on CT for hip joint bonesegmentation is non-transferable to MRI and has almost zero-DICE segmentation."}
{"id2": 1025, "id3": 275, "content": "Superpixel algorithms, which group pixels similar in color and otherlow-level properties, are increasingly used for pre-processing in imagesegmentation. Commonly important criteria for the computation of superpixelsare boundary adherence, speed, and regularity. Boundary adherence and regularity are typically contradictory goals. Mostrecent algorithms have focused on improving boundary adherence. Motivated byimproving superpixel regularity, we propose a diagram-based superpixelgeneration method called Power-SLIC. On the BSDS500 data set, Power-SLIC outperforms other state-of-the-artalgorithms in terms of compactness and boundary precision, and its boundaryadherence is the most robust against varying levels of Gaussian noise. In termsof speed, Power-SLIC is competitive with SLIC."}
{"id2": 1026, "id3": 299, "content": "Dilated convolutions are widely used in deep semantic segmentation models asthey can enlarge the filters receptive field without adding additional weightsnor sacrificing spatial resolution. However, as dilated convolutional filtersdo not possess positional knowledge about the pixels on semantically meaningfulcontours, they could lead to ambiguous predictions on object boundaries. Inaddition, although dilating the filter can expand its receptive field, thetotal number of sampled pixels remains unchanged, which usually comprises asmall fraction of the receptive fields total area. Inspired by the LateralInhibition (LI) mechanisms in human visual systems, we propose the dilatedconvolution with lateral inhibitions (LI-Convs) to overcome these limitations.Introducing LI mechanisms improves the convolutional filters sensitivity tosemantic object boundaries. Moreover, since LI-Convs also implicitly take thepixels from the laterally inhibited zones into consideration, they can alsoextract features at a denser scale. By integrating LI-Convs into the Deeplabv3+architecture, we propose the Lateral Inhibited Atrous Spatial Pyramid Pooling(LI-ASPP), the Lateral Inhibited MobileNet-V2 (LI-MNV2) and the LateralInhibited ResNet (LI-ResNet). Experimental results on three benchmark datasets(PASCAL VOC 2012, CelebAMask-HQ and ADE20K) show that our LI-based segmentationmodels outperform the baseline on all of them, thus verify the effectivenessand generality of the proposed LI-Convs."}
{"id2": 1027, "id3": 313, "content": "A key requirement for the success of supervised deep learning is a largelabeled dataset - a condition that is difficult to meet in medical imageanalysis. Self-supervised learning (SSL) can help in this regard by providing astrategy to pre-train a neural network with unlabeled data, followed byfine-tuning for a downstream task with limited annotations. Contrastivelearning, a particular variant of SSL, is a powerful technique for learningimage-level representations. In this work, we propose strategies for extendingthe contrastive learning framework for segmentation of volumetric medicalimages in the semi-supervised setting with limited annotations, by leveragingdomain-specific and problem-specific cues. Specifically, we propose (1) novelcontrasting strategies that leverage structural similarity across volumetricmedical images (domain-specific cue) and (2) a local version of the contrastiveloss to learn distinctive representations of local regions that are useful forper-pixel segmentation (problem-specific cue). We carry out an extensiveevaluation on three Magnetic Resonance Imaging (MRI) datasets. In the limitedannotation setting, the proposed method yields substantial improvementscompared to other self-supervision and semi-supervised learning techniques.When combined with a simple data augmentation technique, the proposed methodreaches within 8% of benchmark performance using only two labeled MRI volumesfor training, corresponding to only 4% (for ACDC) of the training data used totrain the benchmark. The code is made public athttps://github.com/krishnabits001/domain_specific_cl."}
{"id2": 1028, "id3": 323, "content": "Road network and building footprint extraction is essential for manyapplications such as updating maps, traffic regulations, city planning,ride-hailing, disaster response textit etc . Mapping road networks iscurrently both expensive and labor-intensive. Recently, improvements in imagesegmentation through the application of deep neural networks has shownpromising results in extracting road segments from large scale, high resolutionsatellite imagery. However, significant challenges remain due to lack of enoughlabeled training data needed to build models for industry grade applications.In this paper, we propose a two-stage transfer learning technique to improverobustness of semantic segmentation for satellite images that leverages noisypseudo ground truth masks obtained automatically (without human labor) fromcrowd-sourced OpenStreetMap (OSM) data. We further propose PyramidPooling-LinkNet (PP-LinkNet), an improved deep neural network for segmentationthat uses focal loss, poly learning rate, and context module. We demonstratethe strengths of our approach through evaluations done on three populardatasets over two tasks, namely, road extraction and building foot-printdetection. Specifically, we obtain 78.19 % meanIoU on SpaceNet buildingfootprint dataset, 67.03 % and 77.11 % on the road topology metric on SpaceNetand DeepGlobe road extraction dataset, respectively."}
{"id2": 1029, "id3": 324, "content": "Deep convolutional neural networks have significantly boosted the performanceof fundus image segmentation when test datasets have the same distribution asthe training datasets. However, in clinical practice, medical images oftenexhibit variations in appearance for various reasons, e.g., different scannervendors and image quality. These distribution discrepancies could lead the deepnetworks to over-fit on the training datasets and lack generalization abilityon the unseen test datasets. To alleviate this issue, we present a novelDomain-oriented Feature Embedding (DoFE) framework to improve thegeneralization ability of CNNs on unseen target domains by exploring theknowledge from multiple source domains. Our DoFE framework dynamically enrichesthe image features with additional domain prior knowledge learned frommulti-source domains to make the semantic features more discriminative.Specifically, we introduce a Domain Knowledge Pool to learn and memorize theprior information extracted from multi-source domains. Then the original imagefeatures are augmented with domain-oriented aggregated features, which areinduced from the knowledge pool based on the similarity between the input imageand multi-source domain images. We further design a novel domain codeprediction branch to infer this similarity and employ an attention-guidedmechanism to dynamically combine the aggregated features with the semanticfeatures. We comprehensively evaluate our DoFE framework on two fundus imagesegmentation tasks, including the optic cup and disc segmentation and vesselsegmentation. Our DoFE framework generates satisfying segmentation results onunseen datasets and surpasses other domain generalization and networkregularization methods."}
{"id2": 1030, "id3": 341, "content": "Deep Learning (DL) models are becoming larger, because the increase in modelsize might offer significant accuracy gain. To enable the training of largedeep networks, data parallelism and model parallelism are two well-knownapproaches for parallel training. However, data parallelism does not helpreduce memory footprint per device. In this work, we introduce Large deep 3DConvNets with Automated Model Parallelism (LAMP) and investigate the impact ofboth inputs and deep 3D ConvNets size on segmentation accuracy. Throughautomated model parallelism, it is feasible to train large deep 3D ConvNetswith a large input patch, even the whole image. Extensive experimentsdemonstrate that, facilitated by the automated model parallelism, thesegmentation accuracy can be improved through increasing model size and inputcontext size, and large input yields significant inference speedup comparedwith sliding window of small patches in the inference. Code isavailable footnote https://monai.io/research/lamp-automated-model-parallelism ."}
{"id2": 1031, "id3": 342, "content": "Games such as go, chess and checkers have multiple equivalent game states,i.e. multiple board positions where symmetrical and opposite moves should bemade. These equivalences are not exploited by current state of the art neuralagents which instead must relearn similar information, thereby wastingcomputing time. Group equivariant CNNs in existing work create networks whichcan exploit symmetries to improve learning, however, they lack theexpressiveness to correctly reflect the move embeddings necessary for games. Weintroduce Finite Group Neural Networks (FGNNs), a method for creating agentswith an innate understanding of these board positions. FGNNs are shown toimprove the performance of networks playing checkers (draughts), and can beeasily adapted to other games and learning problems. Additionally, FGNNs can becreated from existing network architectures. These include, for the first time,those with skip connections and arbitrary layer types. We demonstrate that anequivariant version of U-Net (FGNN-U-Net) outperforms the unmodified network inimage segmentation."}
{"id2": 1032, "id3": 346, "content": "Given a 3D surface defined by an elevation function on a 2D grid as well asnon-spatial features observed at each pixel, the problem of surfacesegmentation aims to classify pixels into contiguous classes based on bothnon-spatial features and surface topology. The problem has importantapplications in hydrology, planetary science, and biochemistry but is uniquelychallenging for several reasons. First, the spatial extent of class segmentsfollows surface contours in the topological space, regardless of their spatialshapes and directions. Second, the topological structure exists in multiplespatial scales based on different surface resolutions. Existing widelysuccessful deep learning models for image segmentation are often not applicabledue to their reliance on convolution and pooling operations to learn regularstructural patterns on a grid. In contrast, we propose to represent surfacetopological structure by a contour tree skeleton, which is a polytree capturingthe evolution of surface contours at different elevation levels. We furtherdesign a graph neural network based on the contour tree hierarchy to modelsurface topological structure at different spatial scales. Experimentalevaluations based on real-world hydrological datasets show that our modeloutperforms several baseline methods in classification accuracy."}
{"id2": 1033, "id3": 349, "content": "This work presents use of Fully Convolutional Network (FCN-8) for semanticsegmentation of high-resolution RGB earth surface satel-lite images into landuse land cover (LULC) categories. Specically, we propose a non-overlappinggrid-based approach to train a Fully Convo-lutional Network (FCN-8) with vgg-16weights to segment satellite im-ages into four (forest, built-up, farmland andwater) classes. The FCN-8 semantically projects the discriminating features inlower resolution learned by the encoder onto the pixel space in higherresolution to get a dense classi cation. We experimented the proposed systemwith Gaofen-2 image dataset, that contains 150 images of over 60 di erentcities in china. For comparison, we used available ground-truth along withimages segmented using a widely used commeriial GIS software calledeCogni-tion. With the proposed non-overlapping grid-based approach, FCN-8obtains signi cantly improved performance, than the eCognition soft-ware. Ourmodel achieves average accuracy of 91.0% and average Inter-section over Union(IoU) of 0.84. In contrast, eCognitions average accu-racy is 74.0% and IoU is0.60. This paper also reports a detail analysis of errors occurred at the LULCboundary."}
{"id2": 1034, "id3": 352, "content": "The 3D volumetric shape of the hearts left ventricle (LV) myocardium (MYO)wall provides important information for diagnosis of cardiac disease andinvasive procedure navigation. Many cardiac image segmentation methods haverelied on detection of region-of-interest as a pre-requisite for shapesegmentation and modeling. With segmentation results, a 3D surface mesh and acorresponding point cloud of the segmented cardiac volume can be reconstructedfor further analyses. Although state-of-the-art methods (e.g., U-Net) haveachieved decent performance on cardiac image segmentation in terms of accuracy,these segmentation results can still suffer from imaging artifacts and noise,which will lead to inaccurate shape modeling results. In this paper, we proposea PC-U net that jointly reconstructs the point cloud of the LV MYO walldirectly from volumes of 2D CT slices and generates its segmentation masks fromthe predicted 3D point cloud. Extensive experimental results show that byincorporating a shape prior from the point cloud, the segmentation masks aremore accurate than the state-of-the-art U-Net results in terms of Dicescoefficient and Hausdorff distance.The proposed joint learning framework of ourPC-U net is beneficial for automatic cardiac image analysis tasks because itcan obtain simultaneously the 3D shape and segmentation of the LV MYO walls."}
{"id2": 1035, "id3": 384, "content": "In this work, we propose a new unsupervised image segmentation approach basedon mutual information maximization between different constructed views of theinputs. Taking inspiration from autoregressive generative models that predictthe current pixel from past pixels in a raster-scan ordering created withmasked convolutions, we propose to use different orderings over the inputsusing various forms of masked convolutions to construct different views of thedata. For a given input, the model produces a pair of predictions with twovalid orderings, and is then trained to maximize the mutual information betweenthe two outputs. These outputs can either be low-dimensional features forrepresentation learning or output clusters corresponding to semantic labels forclustering. While masked convolutions are used during training, in inference,no masking is applied and we fall back to the standard convolution where themodel has access to the full input. The proposed method outperforms currentstate-of-the-art on unsupervised image segmentation. It is simple and easy toimplement, and can be extended to other visual tasks and integrated seamlesslyinto existing unsupervised learning methods requiring different views of thedata."}
{"id2": 1036, "id3": 387, "content": "The strict security requirements placed on medical records by various privacyregulations become major obstacles in the age of big data. To ensure efficientmachine learning as a service schemes while protecting data confidentiality, inthis work, we propose blind UNET (BUNET), a secure protocol that implementsprivacy-preserving medical image segmentation based on the UNET architecture.In BUNET, we efficiently utilize cryptographic primitives such as homomorphicencryption and garbled circuits (GC) to design a complete secure protocol forthe UNET neural architecture. In addition, we perform extensive architecturalsearch in reducing the computational bottleneck of GC-based secure activationprotocols with high-dimensional input data. In the experiment, we thoroughlyexamine the parameter space of our protocol, and show that we can achieve up to14x inference time reduction compared to the-state-of-the-art secure inferencetechnique on a baseline architecture with negligible accuracy degradation."}
{"id2": 1037, "id3": 389, "content": "Medical image annotations are prohibitively time-consuming and expensive toobtain. To alleviate annotation scarcity, many approaches have been developedto efficiently utilize extra information, e.g.,semi-supervised learning furtherexploring plentiful unlabeled data, domain adaptation including multi-modalitylearning and unsupervised domain adaptation resorting to the prior knowledgefrom additional modality. In this paper, we aim to investigate the feasibilityof simultaneously leveraging abundant unlabeled data and well-establishedcross-modality data for annotation-efficient medical image segmentation. Tothis end, we propose a novel semi-supervised domain adaptation approach, namelyDual-Teacher, where the student model not only learns from labeled target data(e.g., CT), but also explores unlabeled target data and labeled source data(e.g., MR) by two teacher models. Specifically, the student model learns theknowledge of unlabeled target data from intra-domain teacher by encouragingprediction consistency, as well as the shape priors embedded in labeled sourcedata from inter-domain teacher via knowledge distillation. Consequently, thestudent model can effectively exploit the information from all three dataresources and comprehensively integrate them to achieve improved performance.We conduct extensive experiments on MM-WHS 2017 dataset and demonstrate thatour approach is able to concurrently utilize unlabeled data and cross-modalitydata with superior performance, outperforming semi-supervised learning anddomain adaptation methods with a large margin."}
{"id2": 1038, "id3": 408, "content": "Image segmentation is a fundamental and challenging problem in computervision with applications spanning multiple areas, such as medical imaging,remote sensing, and autonomous vehicles. Recently, convolutional neuralnetworks (CNNs) have gained traction in the design of automated segmentationpipelines. Although CNN-based models are adept at learning abstract featuresfrom raw image data, their performance is dependent on the availability andsize of suitable training datasets. Additionally, these models are often unableto capture the details of object boundaries and generalize poorly to unseenclasses. In this thesis, we devise novel methodologies that address theseissues and establish robust representation learning frameworks forfully-automatic semantic segmentation in medical imaging and mainstreamcomputer vision. In particular, our contributions include (1) state-of-the-art2D and 3D image segmentation networks for computer vision and medical imageanalysis, (2) an end-to-end trainable image segmentation framework that unifiesCNNs and active contour models with learnable parameters for fast and robustobject delineation, (3) a novel approach for disentangling edge and textureprocessing in segmentation networks, and (4) a novel few-shot learning model inboth supervised settings and semi-supervised settings where synergies betweenlatent and image spaces are leveraged to learn to segment images given limitedtraining data."}
{"id2": 1039, "id3": 418, "content": "Most existing black-box optimization methods assume that all variables in thesystem being optimized have equal cost and can change freely at each iteration.However, in many real world systems, inputs are passed through a sequence ofdifferent operations or modules, making variables in earlier stages ofprocessing more costly to update. Such structure imposes a cost on switchingvariables in early parts of a data processing pipeline. In this work, wepropose a new algorithm for switch cost-aware optimization called Lazy ModularBayesian Optimization (LaMBO). This method efficiently identifies the globaloptimum while minimizing cost through a passive change of variables in earlymodules. The method is theoretical grounded and achieves vanishing regret whenaugmented with switching cost. We apply LaMBO to multiple synthetic functionsand a three-stage image segmentation pipeline used in a neuroscienceapplication, where we obtain promising improvements over prevailing cost-awareBayesian optimization algorithms. Our results demonstrate that LaMBO is aneffective strategy for black-box optimization that is capable of minimizingswitching costs in modular systems."}
{"id2": 1040, "id3": 430, "content": "Fully supervised deep neural networks for segmentation usually require amassive amount of pixel-level labels which are manually expensive to create. Inthis work, we develop a multi-task learning method to relax this constraint. Weregard the segmentation problem as a sequence of approximation subproblems thatare recursively defined and in increasing levels of approximation accuracy. Thesubproblems are handled by a framework that consists of 1) a segmentation taskthat learns from pixel-level ground truth segmentation masks of a smallfraction of the images, 2) a recursive approximation task that conducts partialobject regions learning and data-driven mask evolution starting from partialmasks of each object instance, and 3) other problem oriented auxiliary tasksthat are trained with sparse annotations and promote the learning of dedicatedfeatures. Most training images are only labeled by (rough) partial masks, whichdo not contain exact object boundaries, rather than by their full segmentationmasks. During the training phase, the approximation task learns the statisticsof these partial masks, and the partial regions are recursively increasedtowards object boundaries aided by the learned information from thesegmentation task in a fully data-driven fashion. The network is trained on anextremely small amount of precisely segmented images and a large set of coarselabels. Annotations can thus be obtained in a cheap way. We demonstrate theefficiency of our approach in three applications with microscopy images andultrasound images."}
{"id2": 1041, "id3": 441, "content": "The ability of neural networks to continuously learn and adapt to new taskswhile retaining prior knowledge is crucial for many applications. However,current neural networks tend to forget previously learned tasks when trained onnew ones, i.e., they suffer from Catastrophic Forgetting (CF). The objective ofContinual Learning (CL) is to alleviate this problem, which is particularlyrelevant for medical applications, where it may not be feasible to store andaccess previously used sensitive patient data. In this work, we propose aContinual Learning approach for brain segmentation, where a single network isconsecutively trained on samples from different domains. We build upon animportance driven approach and adapt it for medical image segmentation.Particularly, we introduce learning rate regularization to prevent the loss ofthe networks knowledge. Our results demonstrate that directly restricting theadaptation of important network parameters clearly reduces CatastrophicForgetting for segmentation across domains."}
{"id2": 1042, "id3": 445, "content": "3D convolution neural networks (CNN) have been proved very successful inparsing organs or tumours in 3D medical images, but it remains sophisticatedand time-consuming to choose or design proper 3D networks given different taskcontexts. Recently, Neural Architecture Search (NAS) is proposed to solve thisproblem by searching for the best network architecture automatically. However,the inconsistency between search stage and deployment stage often exists in NASalgorithms due to memory constraints and large search space, which could becomemore serious when applying NAS to some memory and time consuming tasks, such as3D medical image segmentation. In this paper, we propose coarse-to-fine neuralarchitecture search (C2FNAS) to automatically search a 3D segmentation networkfrom scratch without inconsistency on network size or input size. Specifically,we divide the search procedure into two stages: 1) the coarse stage, where wesearch the macro-level topology of the network, i.e. how each convolutionmodule is connected to other modules; 2) the fine stage, where we search atmicro-level for operations in each cell based on previous searched macro-leveltopology. The coarse-to-fine manner divides the search procedure into twoconsecutive stages and meanwhile resolves the inconsistency. We evaluate ourmethod on 10 public datasets from Medical Segmentation Decalthon (MSD)challenge, and achieve state-of-the-art performance with the network searchedusing one dataset, which demonstrates the effectiveness and generalization ofour searched models."}
{"id2": 1043, "id3": 447, "content": "Deep learning based image segmentation has achieved the state-of-the-artperformance in many medical applications such as lesion quantification, organdetection, etc. However, most of the methods rely on supervised learning, whichrequire a large set of high-quality labeled data. Data annotation is generallyan extremely time-consuming process. To address this problem, we propose ageneric semi-supervised learning framework for image segmentation based on adeep convolutional neural network (DCNN). An encoder-decoder based DCNN isinitially trained using a few annotated training samples. This initiallytrained model is then copied into sub-models and improved iteratively usingrandom subsets of unlabeled data with pseudo labels generated from modelstrained in the previous iteration. The number of sub-models is graduallydecreased to one in the final iteration. We evaluate the proposed method on apublic grand-challenge dataset for skin lesion segmentation. Our method is ableto significantly improve beyond fully supervised model learning byincorporating unlabeled data."}
{"id2": 1044, "id3": 460, "content": "We present BiLingUNet, a state-of-the-art model for image segmentation usingreferring expressions. BiLingUNet uses language to customize visual filters andoutperforms approaches that concatenate a linguistic representation to thevisual input. We find that using language to modulate both bottom-up andtop-down visual processing works better than just making the top-downprocessing language-conditional. We argue that common 1x1 language-conditionalfilters cannot represent relational concepts and experimentally demonstratethat wider filters work better. Our model achieves state-of-the-art performanceon four referring expression datasets."}
{"id2": 1045, "id3": 481, "content": "Deep learning has shown its great promise in various biomedical imagesegmentation tasks. Existing models are typically based on U-Net and rely on anencoder-decoder architecture with stacked local operators to aggregatelong-range information gradually. However, only using the local operatorslimits the efficiency and effectiveness. In this work, we propose the non-localU-Nets, which are equipped with flexible global aggregation blocks, forbiomedical image segmentation. These blocks can be inserted into U-Net assize-preserving processes, as well as down-sampling and up-sampling layers. Weperform thorough experiments on the 3D multimodality isointense infant brain MRimage segmentation task to evaluate the non-local U-Nets. Results show that ourproposed models achieve top performances with fewer parameters and fastercomputation."}
{"id2": 1046, "id3": 489, "content": "Accurate image segmentation of the liver is a challenging problem owing toits large shape variability and unclear boundaries. Although the applicationsof fully convolutional neural networks (CNNs) have shown groundbreakingresults, limited studies have focused on the performance of generalization. Inthis study, we introduce a CNN for liver segmentation on abdominal computedtomography (CT) images that shows high generalization performance and accuracy.To improve the generalization performance, we initially propose an auto-contextalgorithm in a single CNN. The proposed auto-context neural network exploits aneffective high-level residual estimation to obtain the shape prior. Identicaldual paths are effectively trained to represent mutual complementary featuresfor an accurate posterior analysis of a liver. Further, we extend our networkby employing a self-supervised contour scheme. We trained sparse contourfeatures by penalizing the ground-truth contour to focus more contourattentions on the failures. The experimental results show that the proposednetwork results in better accuracy when compared to the state-of-the-artnetworks by reducing 10.31% of the Hausdorff distance. We used 180 abdominal CTimages for training and validation. Two-fold cross-validation is presented fora comparison with the state-of-the-art neural networks. Novel multiple N-foldcross-validations are conducted to verify the performance of generalization.The proposed network showed the best generalization performance among thenetworks. Additionally, we present a series of ablation experiments thatcomprehensively support the importance of the underlying concepts."}
{"id2": 1047, "id3": 495, "content": "This paper presents an extension proposal of the semi-supervised learningmethod known as Particle Competition and Cooperation for carrying out tasks ofimage segmentation. Preliminary results show that this is a promising approach. Este artigo apresenta uma proposta de extens ~ao do modelo de aprendizadosemi-supervisionado conhecido como Competi c c ~ao e Coopera c c ~ao entrePart iculas para a realiza c c ~ao de tarefas de segmenta c c ~ao deimagens. Resultados preliminares mostram que esta e uma abordagem promissora."}
{"id2": 1048, "id3": 498, "content": "Image normalization is a critical step in medical imaging. This step is oftendone on a per-dataset basis, preventing current segmentation algorithms fromthe full potential of exploiting jointly normalized information across multipledatasets. To solve this problem, we propose an adversarial normalizationapproach for image segmentation which learns common normalizing functionsacross multiple datasets while retaining image realism. The adversarialtraining provides an optimal normalizer that improves both the segmentationaccuracy and the discrimination of unrealistic normalizing functions. Ourcontribution therefore leverages common imaging information from multipledomains. The optimality of our common normalizer is evaluated by combiningbrain images from both infants and adults. Results on the challenging iSEG andMRBrainS datasets reveal the potential of our adversarial normalizationapproach for segmentation, with Dice improvements of up to 59.6% over thebaseline."}
{"id2": 1049, "id3": 523, "content": "We present an image segmentation method that iteratively evolves a polygon.At each iteration, the vertices of the polygon are displaced based on the localvalue of a 2D shift map that is inferred from the input image via anencoder-decoder architecture. The main training loss that is used is thedifference between the polygon shape and the ground truth segmentation mask.The network employs a neural renderer to create the polygon from its vertices,making the process fully differentiable. We demonstrate that our methodoutperforms the state of the art segmentation networks and deep active contoursolutions in a variety of benchmarks, including medical imaging and aerialimages. Our code is available at https://github.com/shirgur/ACDRNet."}
{"id2": 1050, "id3": 526, "content": "The medical image is characterized by the inter-class indistinction, highvariability, and noise, where the recognition of pixels is challenging. Unlikeprevious self-attention based methods that capture context information from onelevel, we reformulate the self-attention mechanism from the view of thehigh-order graph and propose a novel method, namely Hierarchical AttentionNetwork (HANet), to address the problem of medical image segmentation.Concretely, an HA module embedded in the HANet captures context informationfrom neighbors of multiple levels, where these neighbors are extracted from thehigh-order graph. In the high-order graph, there will be an edge between twonodes only if the correlation between them is high enough, which naturallyreduces the noisy attention information caused by the inter-classindistinction. The proposed HA module is robust to the variance of input andcan be flexibly inserted into the existing convolution neural networks. Weconduct experiments on three medical image segmentation tasks including opticdisc/cup segmentation, blood vessel segmentation, and lung segmentation.Extensive results show our method is more effective and robust than theexisting state-of-the-art methods."}
{"id2": 1051, "id3": 534, "content": "Over the last decade, electron microscopy has improved up to a point thatgenerating high quality gigavoxel sized datasets only requires a few hours.Automated image analysis, particularly image segmentation, however, has notevolved at the same pace. Even though state-of-the-art methods such as U-Netand DeepLab have improved segmentation performance substantially, the requiredamount of labels remains too expensive. Active learning is the subfield inmachine learning that aims to mitigate this burden by selecting the samplesthat require labeling in a smart way. Many techniques have been proposed,particularly for image classification, to increase the steepness of learningcurves. In this work, we extend these techniques to deep CNN based imagesegmentation. Our experiments on three different electron microscopy datasetsshow that active learning can improve segmentation quality by 10 to 15% interms of Jaccard score compared to standard randomized sampling."}
{"id2": 1052, "id3": 535, "content": "Convolutional neural networks (CNNs) have been widely and successfully usedfor medical image segmentation. However, CNNs are typically considered torequire large numbers of dedicated expert-segmented training volumes, which maybe limiting in practice. This work investigates whether clinically obtainedsegmentations which are readily available in picture archiving andcommunication systems (PACS) could provide a possible source of data to train aCNN for segmentation of organs-at-risk (OARs) in radiotherapy treatmentplanning. In such data, delineations of structures deemed irrelevant to thetarget clinical use may be lacking. To overcome this issue, we use multi-labelinstead of multi-class segmentation. We empirically assess how many clinicaldelineations would be sufficient to train a CNN for the segmentation of OARsand find that increasing the training set size beyond a limited number ofimages leads to sharply diminishing returns. Moreover, we find that by usingmulti-label segmentation, missing structures in the reference standard do nothave a negative effect on overall segmentation accuracy. These results indicatethat segmentations obtained in a clinical workflow can be used to train anaccurate OAR segmentation model."}
{"id2": 1053, "id3": 540, "content": "The scarcity of labeled data often limits the application of supervised deeplearning techniques for medical image segmentation. This has motivated thedevelopment of semi-supervised techniques that learn from a mixture of labeledand unlabeled images. In this paper, we propose a novel semi-supervised methodthat, in addition to supervised learning on labeled training images, learns topredict segmentations consistent under a given class of transformations on bothlabeled and unlabeled images. More specifically, in this work we explorelearning equivariance to elastic deformations. We implement this through: 1) aSiamese architecture with two identical branches, each of which receives adifferently transformed image, and 2) a composite loss function with asupervised segmentation loss term and an unsupervised term that encouragessegmentation consistency between the predictions of the two branches. Weevaluate the method on a public dataset of chest radiographs with segmentationsof anatomical structures using 5-fold cross-validation. The proposed methodreaches significantly higher segmentation accuracy compared to supervisedlearning. This is due to learning transformation consistency on both labeledand unlabeled images, with the latter contributing the most. We achieve theperformance comparable to state-of-the-art chest X-ray segmentation methodswhile using substantially fewer labeled images."}
{"id2": 1054, "id3": 550, "content": "Recently, Fully Convolutional Network (FCN) seems to be the go-toarchitecture for image segmentation, including semantic scene parsing. However,it is difficult for a generic FCN to discriminate pixels around the objectboundaries, thus FCN based methods may output parsing results with inaccurateboundaries. Meanwhile, level set based active contours are superior to theboundary estimation due to the sub-pixel accuracy that they achieve. However,they are quite sensitive to initial settings. To address these limitations, inthis paper we propose a novel Deep Multiphase Level Set (DMLS) method forsemantic scene parsing, which efficiently incorporates multiphase level setsinto deep neural networks. The proposed method consists of three modules, i.e.,recurrent FCNs, adaptive multiphase level set, and deeply supervised learning.More specifically, recurrent FCNs learn multi-level representations of inputimages with different contexts. Adaptive multiphase level set drives thediscriminative contour for each semantic class, which makes use of theadvantages of both global and local information. In each time-step of therecurrent FCNs, deeply supervised learning is incorporated for model training.Extensive experiments on three public benchmarks have shown that our proposedmethod achieves new state-of-the-art performances."}
{"id2": 1055, "id3": 552, "content": "Automated fiber placement (AFP) is an advanced manufacturing technology thatincreases the rate of production of composite materials. At the same time, theneed for adaptable and fast inline control methods of such parts raises.Existing inspection systems make use of handcrafted filter chains and featuredetectors, tuned for a specific measurement methods by domain experts. Thesemethods hardly scale to new defects or different measurement devices. In thispaper, we propose to formulate AFP defect detection as an image segmentationproblem that can be solved in an end-to-end fashion using artificiallygenerated training data. We employ a probabilistic graphical model to generatetraining images and annotations. We then train a deep neural network based onrecent architectures designed for image segmentation. This leads to anappealing method that scales well with new defect types and measurement devicesand requires little real world data for training."}
{"id2": 1056, "id3": 575, "content": "In recent years, there has been remarkable progress in supervised imagesegmentation. Video segmentation is less explored, despite the temporaldimension being highly informative. Semantic labels, e.g. that cannot beaccurately detected in the current frame, may be inferred by incorporatinginformation from previous frames. However, video segmentation is challengingdue to the amount of data that needs to be processed and, more importantly, thecost involved in obtaining ground truth annotations for each frame. In thispaper, we tackle the issue of label scarcity by using consecutive frames of avideo, where only one frame is annotated. We propose a deep, end-to-endtrainable model which leverages temporal information in order to make use ofeasy to acquire unlabeled data. Our network architecture relies on a novelinterconnection of two components: a fully convolutional network to modelspatial information and temporal units that are employed at intermediate levelsof the convolutional network in order to propagate information through time.The main contribution of this work is the guidance of the temporal signalthrough the network. We show that only placing a temporal module between theencoder and decoder is suboptimal (baseline). Our extensive experiments on theCityScapes dataset indicate that the resulting model can leverage unlabeledtemporal frames and significantly outperform both the frame-by-frame imagesegmentation and the baseline approach."}
{"id2": 1057, "id3": 576, "content": "U-Net has been providing state-of-the-art performance in many medical imagesegmentation problems. Many modifications have been proposed for U-Net, such asattention U-Net, recurrent residual convolutional U-Net (R2-UNet), and U-Netwith residual blocks or blocks with dense connections. However, all thesemodifications have an encoder-decoder structure with skip connections, and thenumber of paths for information flow is limited. We propose LadderNet in thispaper, which can be viewed as a chain of multiple U-Nets. Instead of only onepair of encoder branch and decoder branch in U-Net, a LadderNet has multiplepairs of encoder-decoder branches, and has skip connections between every pairof adjacent decoder and decoder branches in each level. Inspired by the successof ResNet and R2-UNet, we use modified residual blocks where two convolutionallayers in one block share the same weights. A LadderNet has more paths forinformation flow because of skip connections and residual blocks, and can beviewed as an ensemble of Fully Convolutional Networks (FCN). The equivalence toan ensemble of FCNs improves segmentation accuracy, while the shared weightswithin each residual block reduce parameter number. Semantic segmentation isessential for retinal disease detection. We tested LadderNet on two benchmarkdatasets for blood vessel segmentation in retinal images, and achieved superiorperformance over methods in the literature. The implementation is provided url https://github.com/juntang-zhuang/LadderNet"}
{"id2": 1058, "id3": 579, "content": "Data for Image segmentation models can be costly to obtain due to theprecision required by human annotators. We run a series of experiments showingthe effect of different kinds of Dropout training on the DeepLabv3+ Imagesegmentation model when trained using a small dataset. We find that whenappropriate forms of Dropout are applied in the right place in the modelarchitecture that non-insignificant improvement in Mean Intersection over Union(mIoU) score can be observed. In our best case, we find that applying Dropoutscheduling in conjunction with SpatialDropout improves baseline mIoU from 0.49to 0.59. This result shows that even where a model architecture makes extensiveuse of Batch Normalization, Dropout can still be an effective way of improvingperformance in low data situations."}
{"id2": 1059, "id3": 588, "content": "For the task of medical image segmentation, fully convolutional network (FCN)based architectures have been extensively used with various modifications. Arising trend in these architectures is to employ joint-learning of the targetregion with an auxiliary task, a method commonly known as multi-task learning.These approaches help impose smoothness and shape priors, which vanilla FCNapproaches do not necessarily incorporate. In this paper, we propose a novelplug-and-play module, which we term as Conv-MCD, which exploits structuralinformation in two ways - i) using the contour map and ii) using the distancemap, both of which can be obtained from ground truth segmentation maps with noadditional annotation costs. The key benefit of our module is the ease of itsaddition to any state-of-the-art architecture, resulting in a significantimprovement in performance with a minimal increase in parameters. Tosubstantiate the above claim, we conduct extensive experiments using 4state-of-the-art architectures across various evaluation metrics, and report asignificant increase in performance in relation to the base networks. Inaddition to the aforementioned experiments, we also perform ablative studiesand visualization of feature maps to further elucidate our approach."}
{"id2": 1060, "id3": 611, "content": "In this paper, an stereo-based traversability analysis approach for allterrains in off-road mobile robotics, e.g. Unmanned Ground Vehicles (UGVs) isproposed. This approach reformulates the problem of terrain traversabilityanalysis into two main problems: (1) 3D terrain reconstruction and (2) terrainall surfaces detection and analysis. The proposed approach is using stereocamera for perception and 3D reconstruction of the terrain. In order to detectall the existing surfaces in the 3D reconstructed terrain as superpixelsurfaces (i.e. segments), an image segmentation technique is applied usinggeometry-based features (pixel-based surface normals). Having detected all thesurfaces, Superpixel Surface Traversability Analysis approach (SSTA) is appliedon all of the detected surfaces (superpixel segments) in order to classify thembased on their traversability index. The proposed SSTA approach is based on:(1) Superpixel surface normal and plane estimation, (2) Traversability analysisusing superpixel surface planes. Having analyzed all the superpixel surfacesbased on their traversability, these surfaces are finally classified into fivemain categories as following: traversable, semi-traversable, non-traversable,unknown and undecided."}
{"id2": 1061, "id3": 612, "content": "To improve the classification performance in the context of hyperspectralimage processing, many works have been developed based on two commonstrategies, namely the spatial-spectral information integration and theutilization of neural networks. However, both strategies typically require moretraining data than the classical algorithms, aggregating the shortage oflabeled samples. In this letter, we propose a novel framework that organicallycombines the spectrum-based deep metric learning model and the conditionalrandom field algorithm. The deep metric learning model is supervised by thecenter loss to produce spectrum-based features that gather more tightly inEuclidean space within classes. The conditional random field with Gaussian edgepotentials, which is firstly proposed for image segmentation tasks, isintroduced to give the pixel-wise classification over the hyperspectral imageby utilizing both the geographical distances between pixels and the Euclideandistances between the features produced by the deep metric learning model. Theproposed framework is trained by spectral pixels at the deep metric learningstage and utilizes the half handcrafted spatial features at the conditionalrandom field stage. This settlement alleviates the shortage of training data tosome extent. Experiments on two real hyperspectral images demonstrate theadvantages of the proposed method in terms of both classification accuracy andcomputation cost."}
{"id2": 1062, "id3": 624, "content": "Advances in the image-based diagnostics of complex biological andmanufacturing processes have brought unsupervised image segmentation to theforefront of enabling automated, on the fly decision making. However, mostexisting unsupervised segmentation approaches are either computationallycomplex or require manual parameter selection (e.g., flow capacities inmax-flow/min-cut segmentation). In this work, we present a fully unsupervisedsegmentation approach using a continuous max-flow formulation over the imagedomain while optimally estimating the flow parameters from the imagecharacteristics. More specifically, we show that the maximum a posterioriestimate of the image labels can be formulated as a continuous max-flow problemgiven the flow capacities are known. The flow capacities are then iterativelyobtained by employing a novel Markov random field prior over the image domain.We present theoretical results to establish the posterior consistency of theflow capacities. We compare the performance of our approach on two real-worldcase studies including brain tumor image segmentation and defect identificationin additively manufactured components using electron microscopic images.Comparative results with several state-of-the-art supervised as well asunsupervised methods suggest that the present method performs statisticallysimilar to the supervised methods, but results in more than 90% improvement inthe Dice score when compared to the state-of-the-art unsupervised methods."}
{"id2": 1063, "id3": 654, "content": "Recent progress in biomedical image segmentation based on deep convolutionalneural networks (CNNs) has drawn much attention. However, its vulnerabilitytowards adversarial samples cannot be overlooked. This paper is the first onethat discovers that all the CNN-based state-of-the-art biomedical imagesegmentation models are sensitive to adversarial perturbations. This limits thedeployment of these methods in safety-critical biomedical fields. In thispaper, we discover that global spatial dependencies and global contextualinformation in a biomedical image can be exploited to defend againstadversarial attacks. To this end, non-local context encoder (NLCE) is proposedto model short- and long range spatial dependencies and encode global contextsfor strengthening feature activations by channel-wise attention. The NLCEmodules enhance the robustness and accuracy of the non-local context encodingnetwork (NLCEN), which learns robust enhanced pyramid feature representationswith NLCE modules, and then integrates the information across different levels.Experiments on both lung and skin lesion segmentation datasets havedemonstrated that NLCEN outperforms any other state-of-the-art biomedical imagesegmentation methods against adversarial attacks. In addition, NLCE modules canbe applied to improve the robustness of other CNN-based biomedical imagesegmentation methods."}
{"id2": 1064, "id3": 655, "content": "It is important to find the target as soon as possible for search and rescueoperations. Surveillance camera systems and unmanned aerial vehicles (UAVs) areused to support search and rescue. Automatic object detection is importantbecause a person cannot monitor multiple surveillance screens simultaneouslyfor 24 hours. Also, the object is often too small to be recognized by the humaneye on the surveillance screen. This study used UAVs around the Port of Houstonand fixed surveillance cameras to build an automatic target detection systemthat supports the US Coast Guard (USCG) to help find targets (e.g., personoverboard). We combined image segmentation, enhancement, and convolution neuralnetworks to reduce detection time to detect small targets. We compared theperformance between the auto-detection system and the human eye. Our systemdetected the target within 8 seconds, but the human eye detected the targetwithin 25 seconds. Our systems also used synthetic data generation and dataaugmentation techniques to improve target detection accuracy. This solution mayhelp the search and rescue operations of the first responders in a timelymanner."}
{"id2": 1065, "id3": 661, "content": "Segmenting objects in images and separating sound sources in audio arechallenging tasks, in part because traditional approaches require large amountsof labeled data. In this paper we develop a neural network model for visualobject segmentation and sound source separation that learns from natural videosthrough self-supervision. The model is an extension of recently proposed workthat maps image pixels to sounds. Here, we introduce a learning approach todisentangle concepts in the neural networks, and assign semantic categories tonetwork feature channels to enable independent image segmentation and soundsource separation after audio-visual training on videos. Our evaluations showthat the disentangled model outperforms several baselines in semanticsegmentation and sound source separation."}
{"id2": 1066, "id3": 662, "content": "Cardiac image segmentation is a critical process for generating personalizedmodels of the heart and for quantifying cardiac performance parameters. Severalconvolutional neural network (CNN) architectures have been proposed to segmentthe heart chambers from cardiac cine MR images. Here we propose a multi-tasklearning (MTL)-based regularization framework for cardiac MR imagesegmentation. The network is trained to perform the main task of semanticsegmentation, along with a simultaneous, auxiliary task of pixel-wise distancemap regression. The proposed distance map regularizer is a decoder networkadded to the bottleneck layer of an existing CNN architecture, facilitating thenetwork to learn robust global features. The regularizer block is removed aftertraining, so that the original number of network parameters does not change. Weshow that the proposed regularization method improves both binary andmulti-class segmentation performance over the corresponding state-of-the-artCNN architectures on two publicly available cardiac cine MRI datasets,obtaining average dice coefficient of 0.84$ pm$0.03 and 0.91$ pm$0.04,respectively. Furthermore, we also demonstrate improved generalizationperformance of the distance map regularized network on cross-datasetsegmentation, showing as much as 42% improvement in myocardium Dice coefficientfrom 0.56$ pm$0.28 to 0.80$ pm$0.14."}
{"id2": 1067, "id3": 683, "content": "We propose a method to classify cardiac pathology based on a novel approachto extract image derived features to characterize the shape and motion of theheart. An original semi-supervised learning procedure, which makes efficientuse of a large amount of non-segmented images and a small amount of imagessegmented manually by experts, is developed to generate pixel-wise apparentflow between two time points of a 2D+t cine MRI image sequence. Combining theapparent flow maps and cardiac segmentation masks, we obtain a local apparentflow corresponding to the 2D motion of myocardium and ventricular cavities.This leads to the generation of time series of the radius and thickness ofmyocardial segments to represent cardiac motion. These time series of motionfeatures are reliable and explainable characteristics of pathological cardiacmotion. Furthermore, they are combined with shape-related features to classifycardiac pathologies. Using only nine feature values as input, we propose anexplainable, simple and flexible model for pathology classification. On ACDCtraining set and testing set, the model achieves 95% and 94% respectively asclassification accuracy. Its performance is hence comparable to that of thestate-of-the-art. Comparison with various other models is performed to outlinesome advantages of our model."}
{"id2": 1068, "id3": 684, "content": "This paper presents a new approach for relatively accurate brain region ofinterest (ROI) detection from dynamic susceptibility contrast (DSC) perfusionmagnetic resonance (MR) images of a human head with abnormal brain anatomy.Such images produce problems for automatic brain segmentation algorithms, andas a result, poor perfusion ROI detection affects both quantitativemeasurements and visual assessment of perfusion data. In the proposed approachimage segmentation is based on CUSUM filter usage that was adapted to beapplicable to process DSC perfusion MR images. The result of segmentation is abinary mask of brain ROI that is generated via usage of brain boundarylocation. Each point of the boundary between the brain and surrounding tissuesis detected as a change-point by CUSUM filter. Proposed adopted CUSUM filteroperates by accumulating the deviations between the observed and expectedintensities of image points at the time of moving on a trajectory. Motiontrajectory is created by the iterative change of movement direction inside thebackground region in order to reach brain region, and vice versa after boundarycrossing. Proposed segmentation approach was evaluated with Dice indexcomparing obtained results to the reference standard. Manually marked brainregion pixels (reference standard), as well as visual inspection of detectedwith CUSUM filter usage brain ROI, were provided by experienced radiologists.The results showed that proposed approach is suitable to be used for brain ROIdetection from DSC perfusion MR images of a human head with abnormal brainanatomy and can, therefore, be applied in the DSC perfusion data analysis."}
{"id2": 1069, "id3": 685, "content": "Over the past few years, deep learning techniques have achieved tremendoussuccess in many visual understanding tasks such as object detection, imagesegmentation, and caption generation. Despite this thriving in computer visionand natural language processing, deep learning has not yet shown significantimpact in robotics. Due to the gap between theory and application, there aremany challenges when applying the results of deep learning to the real roboticsystems. In this study, our long-term goal is to bridge the gap betweencomputer vision and robotics by developing visual methods that can be used inreal robots. In particular, this work tackles two fundamental visual problemsfor autonomous robotic manipulation: affordance detection and fine-grainedaction understanding. Theoretically, we propose different deep architectures tofurther improves the state of the art in each problem. Empirically, we showthat the outcomes of our proposed methods can be applied in real robots andallow them to perform useful manipulation tasks."}
{"id2": 1070, "id3": 702, "content": "Human motion capture data has been widely used in data-driven characteranimation. In order to generate realistic, natural-looking motions, mostdata-driven approaches require considerable efforts of pre-processing,including motion segmentation and annotation. Existing (semi-) automaticsolutions either require hand-crafted features for motion segmentation or donot produce the semantic annotations required for motion synthesis and buildinglarge-scale motion databases. In addition, human labeled annotation datasuffers from inter- and intra-labeler inconsistencies by design. We propose asemi-automatic framework for semantic segmentation of motion capture data basedon supervised machine learning techniques. It first transforms a motion capturesequence into a motion image and applies a convolutional neural network forimage segmentation. Dilated temporal convolutions enable the extraction oftemporal information from a large receptive field. Our model outperforms twostate-of-the-art models for action segmentation, as well as a popular networkfor sequence modeling. Most of all, our method is very robust under noisy andinaccurate training labels and thus can handle human errors during the labelingprocess."}
{"id2": 1071, "id3": 719, "content": "Despite the state-of-the-art performance for medical image segmentation, deepconvolutional neural networks (CNNs) have rarely provided uncertaintyestimations regarding their segmentation outputs, e.g., model (epistemic) andimage-based (aleatoric) uncertainties. In this work, we analyze these differenttypes of uncertainties for CNN-based 2D and 3D medical image segmentationtasks. We additionally propose a test-time augmentation-based aleatoricuncertainty to analyze the effect of different transformations of the inputimage on the segmentation output. Test-time augmentation has been previouslyused to improve segmentation accuracy, yet not been formulated in a consistentmathematical framework. Hence, we also propose a theoretical formulation oftest-time augmentation, where a distribution of the prediction is estimated byMonte Carlo simulation with prior distributions of parameters in an imageacquisition model that involves image transformations and noise. We compare andcombine our proposed aleatoric uncertainty with model uncertainty. Experimentswith segmentation of fetal brains and brain tumors from 2D and 3D MagneticResonance Images (MRI) showed that 1) the test-time augmentation-basedaleatoric uncertainty provides a better uncertainty estimation than calculatingthe test-time dropout-based model uncertainty alone and helps to reduceoverconfident incorrect predictions, and 2) our test-time augmentationoutperforms a single-prediction baseline and dropout-based multiplepredictions."}
{"id2": 1072, "id3": 744, "content": "Interactive image segmentation algorithms rely on the user to provideannotations as the guidance. When the task of interactive segmentation isperformed on a small touchscreen device, the requirement of providing preciseannotations could be cumbersome to the user. We design an efficient seedproposal method that actively proposes annotation seeds for the user to label.The user only needs to check which ones of the query seeds are inside theregion of interest (ROI). We enforce the sparsity and diversity criteria on theselection of the query seeds. At each round of interaction the user is onlypresented with a small number of informative query seeds that are far apartfrom each other. As a result, we are able to derive a user friendly interactionmechanism for annotation on small touchscreen devices. The user merely has toswipe through on the ROI-relevant query seeds, which should be easy since thosegestures are commonly used on a touchscreen. The performance of our algorithmis evaluated on six publicly available datasets. The evaluation results showthat our algorithm achieves high segmentation accuracy, with short responsetime and less user feedback."}
{"id2": 1073, "id3": 750, "content": "Partial differential equations (PDEs) are indispensable for modeling manyphysical phenomena and also commonly used for solving image processing tasks.In the latter area, PDE-based approaches interpret image data asdiscretizations of multivariate functions and the output of image processingalgorithms as solutions to certain PDEs. Posing image processing problems inthe infinite dimensional setting provides powerful tools for their analysis andsolution. Over the last few decades, the reinterpretation of classical imageprocessing problems through the PDE lens has been creating multiple celebratedapproaches that benefit a vast area of tasks including image segmentation,denoising, registration, and reconstruction. In this paper, we establish a new PDE-interpretation of a class of deepconvolutional neural networks (CNN) that are commonly used to learn fromspeech, image, and video data. Our interpretation includes convolution residualneural networks (ResNet), which are among the most promising approaches fortasks such as image classification having improved the state-of-the-artperformance in prestigious benchmark challenges. Despite their recentsuccesses, deep ResNets still face some critical challenges associated withtheir design, immense computational costs and memory requirements, and lack ofunderstanding of their reasoning. Guided by well-established PDE theory, we derive three new ResNetarchitectures that fall into two new classes: parabolic and hyperbolic CNNs. Wedemonstrate how PDE theory can provide new insights and algorithms for deeplearning and demonstrate the competitiveness of three new CNN architecturesusing numerical experiments."}
{"id2": 1074, "id3": 773, "content": "Collecting training data from the physical world is usually time-consumingand even dangerous for fragile robots, and thus, recent advances in robotlearning advocate the use of simulators as the training platform.Unfortunately, the reality gap between synthetic and real visual data prohibitsdirect migration of the models trained in virtual worlds to the real world.This paper proposes a modular architecture for tackling the virtual-to-realproblem. The proposed architecture separates the learning model into aperception module and a control policy module, and uses semantic imagesegmentation as the meta representation for relating these two modules. Theperception module translates the perceived RGB image to semantic imagesegmentation. The control policy module is implemented as a deep reinforcementlearning agent, which performs actions based on the translated imagesegmentation. Our architecture is evaluated in an obstacle avoidance task and atarget following task. Experimental results show that our architecturesignificantly outperforms all of the baseline methods in both virtual and realenvironments, and demonstrates a faster learning curve than them. We alsopresent a detailed analysis for a variety of variant configurations, andvalidate the transferability of our modular architecture."}
{"id2": 1075, "id3": 788, "content": "Left atrium shape has been shown to be an independent predictor of recurrenceafter atrial fibrillation (AF) ablation. Shape-based representation isimperative to such an estimation process, where correspondence-basedrepresentation offers the most flexibility and ease-of-computation forpopulation-level shape statistics. Nonetheless, population-level shaperepresentations in the form of image segmentation and correspondence modelsderived from cardiac MRI require significant human resources with sufficientanatomy-specific expertise. In this paper, we propose a machine learningapproach that uses deep networks to estimate AF recurrence by predicting shapedescriptors directly from MRI images, with NO image pre-processing involved. Wealso propose a novel data augmentation scheme to effectively train a deepnetwork in a limited training data setting. We compare this new method ofestimating shape descriptors from images with the state-of-the-artcorrespondence-based shape modeling that requires image segmentation andcorrespondence optimization. Results show that the proposed method and thecurrent state-of-the-art produce statistically similar outcomes on AFrecurrence, eliminating the need for expensive pre-processing pipelines andassociated human labor."}
{"id2": 1076, "id3": 793, "content": "Over the past years, computer vision community has contributed to enormousprogress in semantic image segmentation, a per-pixel classification task,crucial for dense scene understanding and rapidly becoming vital in lots ofreal-world applications, including driverless cars and medical imaging. Mostrecent models are now reaching previously unthinkable numbers (e.g., 89% meaniou on PASCAL VOC, 83% on CityScapes), and, while intersection-over-union and arange of other metrics provide the general picture of model performance, inthis paper we aim to extend them into other meaningful and important forapplications characteristics, answering such questions as how accurate themodel segmentation is on small objects in the general scene?, or what are thesources of uncertainty that cause the model to make an erroneous prediction?.Besides establishing a methodology that covers the performance of a singlemodel from different perspectives, we also showcase several extensions that canbe worth pursuing in order to further improve current results in semanticsegmentation."}
{"id2": 1077, "id3": 810, "content": "This paper presents the experimental study revealing weaker performance ofthe automatic iris recognition methods for cataract-affected eyes when comparedto healthy eyes. There is little research on the topic, mostly incorporatingscarce databases that are often deficient in images representing more than oneillness. We built our own database, acquiring 1288 eye images of 37 patients ofthe Medical University of Warsaw. Those images represent several common oculardiseases, such as cataract, along with less ordinary conditions, such as irispattern alterations derived from illness or eye trauma. Images were captured innear-infrared light (used in biometrics) and for selected cases also in visiblelight (used in ophthalmological diagnosis). Since cataract is a disorder thatis most populated by samples in the database, in this paper we focus solely onthis illness. To assess the extent of the performance deterioration we usethree iris recognition methodologies (commercial and academic solutions) tocalculate genuine match scores for healthy eyes and those influenced bycataract. Results show a significant degradation in iris recognitionreliability manifesting by worsening the genuine scores in all three matchersused in this study (12% of genuine score increase for an academic matcher, upto 175% of genuine score increase obtained for an example commercial matcher).This increase in genuine scores affected the final false non-match rate in twomatchers. To our best knowledge this is the only study of such kind thatemploys more than one iris matcher, and analyzes the iris image segmentation asa potential source of decreased reliability."}
{"id2": 1078, "id3": 816, "content": "Spatial pyramid pooling module or encode-decoder structure are used in deepneural networks for semantic segmentation task. The former networks are able toencode multi-scale contextual information by probing the incoming features withfilters or pooling operations at multiple rates and multiple effectivefields-of-view, while the latter networks can capture sharper object boundariesby gradually recovering the spatial information. In this work, we propose tocombine the advantages from both methods. Specifically, our proposed model,DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder moduleto refine the segmentation results especially along object boundaries. Wefurther explore the Xception model and apply the depthwise separableconvolution to both Atrous Spatial Pyramid Pooling and decoder modules,resulting in a faster and stronger encoder-decoder network. We demonstrate theeffectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets,achieving the test set performance of 89.0 % and 82.1 % without anypost-processing. Our paper is accompanied with a publicly available referenceimplementation of the proposed models in Tensorflow at url https://github.com/tensorflow/models/tree/master/research/deeplab ."}
{"id2": 1079, "id3": 821, "content": "Unsupervised learning poses one of the most difficult challenges in computervision today. The task has an immense practical value with many applications inartificial intelligence and emerging technologies, as large quantities ofunlabeled videos can be collected at relatively low cost. In this paper, weaddress the unsupervised learning problem in the context of detecting the mainforeground objects in single images. We train a student deep network to predictthe output of a teacher pathway that performs unsupervised object discovery invideos or large image collections. Our approach is different from publishedmethods on unsupervised object discovery. We move the unsupervised learningphase during training time, then at test time we apply the standardfeed-forward processing along the student pathway. This strategy has thebenefit of allowing increased generalization possibilities during training,while remaining fast at testing. Our unsupervised learning algorithm can runover several generations of student-teacher training. Thus, a group of studentnetworks trained in the first generation collectively create the teacher at thenext generation. In experiments our method achieves top results on threecurrent datasets for object discovery in video, unsupervised image segmentationand saliency detection. At test time the proposed system is fast, being one totwo orders of magnitude faster than published unsupervised methods."}
{"id2": 1080, "id3": 829, "content": "Mesh labeling is the key problem of classifying the facets of a 3D mesh witha label among a set of possible ones. State-of-the-art methods model meshlabeling as a Markov Random Field over the facets. These algorithms map imagesegmentations to the mesh by minimizing an energy function that comprises adata term, a smoothness terms, and class-specific priors. The latter favor alabeling with respect to another depending on the orientation of the facetnormals. In this paper we propose a novel energy term that acts as a prior, butdoes not require any prior knowledge about the scene nor scene-specificrelationship among classes. It bootstraps from a coarse mapping of the 2Dsegmentations on the mesh, and it favors the facets to be labeled according tothe statistics of the mesh normals in their neighborhood. We tested ourapproach against five different datasets and, even if we do not inject priorknowledge, our method adapts to the data and overcomes the state-of-the-art."}
{"id2": 1081, "id3": 840, "content": "Semantic segmentation of medical images is a crucial step for thequantification of healthy anatomy and diseases alike. The majority of thecurrent state-of-the-art segmentation algorithms are based on deep neuralnetworks and rely on large datasets with full pixel-wise annotations. Producingsuch annotations can often only be done by medical professionals and requireslarge amounts of valuable time. Training a medical image segmentation networkwith weak annotations remains a relatively unexplored topic. In this work weinvestigate training strategies to learn the parameters of a pixel-wisesegmentation network from scribble annotations alone. We evaluate thetechniques on public cardiac (ACDC) and prostate (NCI-ISBI) segmentationdatasets. We find that the networks trained on scribbles suffer from aremarkably small degradation in Dice of only 2.9% (cardiac) and 4.5% (prostate)with respect to a network trained on full annotations."}
{"id2": 1082, "id3": 841, "content": "Unpaired image-to-image translation is the problem of mapping an image in thesource domain to one in the target domain, without requiring correspondingimage pairs. To ensure the translated images are realistically plausible,recent works, such as Cycle-GAN, demands this mapping to be invertible. While,this requirement demonstrates promising results when the domains are unimodal,its performance is unpredictable in a multi-modal scenario such as in an imagesegmentation task. This is because, invertibility does not necessarily enforcesemantic correctness. To this end, we present a semantically-consistent GANframework, dubbed Sem-GAN, in which the semantics are defined by the classidentities of image segments in the source domain as produced by a semanticsegmentation algorithm. Our proposed framework includes consistency constraintson the translation task that, together with the GAN loss and thecycle-constraints, enforces that the images when translated will inherit theappearances of the target domain, while (approximately) maintaining theiridentities from the source domain. We present experiments on severalimage-to-image translation tasks and demonstrate that Sem-GAN improves thequality of the translated images significantly, sometimes by more than 20% onthe FCN score. Further, we show that semantic segmentation models, trained withsynthetic images translated via Sem-GAN, leads to significantly bettersegmentation results than other variants."}
{"id2": 1083, "id3": 854, "content": "Mucous glands lesions analysis and assessing of malignant potential of colonpolyps are very important tasks of surgical pathology. However, differentialdiagnosis of colon polyps often seems impossible by classical methods and it isnecessary to involve computer methods capable of assessing minimal differencesto extend the capabilities of the classical pathology examination. Accuratesegmentation of mucous glands from histology images is a crucial step to obtainreliable morphometric criteria for quantitative diagnostic methods. We reviewmajor trends in histological images segmentation and design a new convolutionalneural network for mucous gland segmentation."}
{"id2": 1084, "id3": 858, "content": "Semantic image segmentation plays an important role in modelingpatient-specific anatomy. We propose a convolution neural network, calledKid-Net, along with a training schema to segment kidney vessels: artery, veinand collecting system. Such segmentation is vital during the surgical planningphase in which medical decisions are made before surgical incision. Our maincontribution is developing a training schema that handles unbalanced data,reduces false positives and enables high-resolution segmentation with a limitedmemory budget. These objectives are attained using dynamic weighting, randomsampling and 3D patch segmentation. Manual medical image annotation is bothtime-consuming and expensive. Kid-Net reduces kidney vessels segmentation timefrom matter of hours to minutes. It is trained end-to-end using 3D patches fromvolumetric CT-images. A complete segmentation for a 512x512x512 CT-volume isobtained within a few minutes (1-2 mins) by stitching the output 3D patchestogether. Feature down-sampling and up-sampling are utilized to achieve higherclassification and localization accuracies. Quantitative and qualitativeevaluation results on a challenging testing dataset show Kid-Net competence."}
{"id2": 1085, "id3": 859, "content": "Brain extraction is a fundamental step for most brain imaging studies. Inthis paper, we investigate the problem of skull stripping and proposecomplementary segmentation networks (CompNets) to accurately extract the brainfrom T1-weighted MRI scans, for both normal and pathological brain images. Theproposed networks are designed in the framework of encoder-decoder networks andhave two pathways to learn features from both the brain tissue and itscomplementary part located outside of the brain. The complementary pathwayextracts the features in the non-brain region and leads to a robust solution tobrain extraction from MRIs with pathologies, which do not exist in our trainingdataset. We demonstrate the effectiveness of our networks by evaluating them onthe OASIS dataset, resulting in the state of the art performance under thetwo-fold cross-validation setting. Moreover, the robustness of our networks isverified by testing on images with introduced pathologies and by showing itsinvariance to unseen brain pathologies. In addition, our complementary networkdesign is general and can be extended to address other image segmentationproblems with better generalization."}
{"id2": 1086, "id3": 868, "content": "Uncertainty estimation methods are expected to improve the understanding andquality of computer-assisted methods used in medical applications (e.g.,neurosurgical interventions, radiotherapy planning), where automated medicalimage segmentation is crucial. In supervised machine learning, a commonpractice to generate ground truth label data is to merge observer annotations.However, as many medical image tasks show a high inter-observer variabilityresulting from factors such as image quality, different levels of userexpertise and domain knowledge, little is known as to how inter-observervariability and commonly used fusion methods affect the estimation ofuncertainty of automated image segmentation. In this paper we analyze theeffect of common image label fusion techniques on uncertainty estimation, andpropose to learn the uncertainty among observers. The results highlight thenegative effect of fusion methods applied in deep learning, to obtain reliableestimates of segmentation uncertainty. Additionally, we show that the learnedobservers uncertainty can be combined with current standard Monte Carlodropout Bayesian neural networks to characterize uncertainty of modelsparameters."}
{"id2": 1087, "id3": 873, "content": "Deep learning (DL) based semantic segmentation methods have been providingstate-of-the-art performance in the last few years. More specifically, thesetechniques have been successfully applied to medical image classification,segmentation, and detection tasks. One deep learning technique, U-Net, hasbecome one of the most popular for these applications. In this paper, wepropose a Recurrent Convolutional Neural Network (RCNN) based on U-Net as wellas a Recurrent Residual Convolutional Neural Network (RRCNN) based on U-Netmodels, which are named RU-Net and R2U-Net respectively. The proposed modelsutilize the power of U-Net, Residual Network, as well as RCNN. There areseveral advantages of these proposed architectures for segmentation tasks.First, a residual unit helps when training deep architecture. Second, featureaccumulation with recurrent residual convolutional layers ensures betterfeature representation for segmentation tasks. Third, it allows us to designbetter U-Net architecture with same number of network parameters with betterperformance for medical image segmentation. The proposed models are tested onthree benchmark datasets such as blood vessel segmentation in retina images,skin cancer segmentation, and lung lesion segmentation. The experimentalresults show superior performance on segmentation tasks compared to equivalentmodels including U-Net and residual U-Net (ResU-Net)."}
{"id2": 1088, "id3": 896, "content": "We present a multilinear statistical model of the human tongue that capturesanatomical and tongue pose related shape variations separately. The model isderived from 3D magnetic resonance imaging data of 11 speakers sustainingspeech related vocal tract configurations. The extraction is performed by usinga minimally supervised method that uses as basis an image segmentation approachand a template fitting technique. Furthermore, it uses image denoising to dealwith possibly corrupt data, palate surface information reconstruction to handlepalatal tongue contacts, and a bootstrap strategy to refine the obtainedshapes. Our evaluation concludes that limiting the degrees of freedom for theanatomical and speech related variations to 5 and 4, respectively, produces amodel that can reliably register unknown data while avoiding overfittingeffects. Furthermore, we show that it can be used to generate a plausibletongue animation by tracking sparse motion capture data."}
{"id2": 1089, "id3": 900, "content": "This paper presents a novel method for unsupervised segmentation of pathologyimages. Staging of lung cancer is a major factor of prognosis. Measuring themaximum dimensions of the invasive component in a pathology images is anessential task. Therefore, image segmentation methods for visualizing theextent of invasive and noninvasive components on pathology images could supportpathological examination. However, it is challenging for most of the recentsegmentation methods that rely on supervised learning to cope with unlabeledpathology images. In this paper, we propose a unified approach to unsupervisedrepresentation learning and clustering for pathology image segmentation. Ourmethod consists of two phases. In the first phase, we learn featurerepresentations of training patches from a target image using the sphericalk-means. The purpose of this phase is to obtain cluster centroids which couldbe used as filters for feature extraction. In the second phase, we applyconventional k-means to the representations extracted by the centroids and thenproject cluster labels to the target images. We evaluated our methods onpathology images of lung cancer specimen. Our experiments showed that theproposed method outperforms traditional k-means segmentation and themultithreshold Otsu method both quantitatively and qualitatively with animproved normalized mutual information (NMI) score of 0.626 compared to 0.168and 0.167, respectively. Furthermore, we found that the centroids can beapplied to the segmentation of other slices from the same sample."}
{"id2": 1090, "id3": 912, "content": "Recent advances in 3D fully convolutional networks (FCN) have made itfeasible to produce dense voxel-wise predictions of volumetric images. In thiswork, we show that a multi-class 3D FCN trained on manually labeled CT scans ofseveral anatomical structures (ranging from the large organs to thin vessels)can achieve competitive segmentation results, while avoiding the need forhandcrafting features or training class-specific models. To this end, we propose a two-stage, coarse-to-fine approach that will firstuse a 3D FCN to roughly define a candidate region, which will then be used asinput to a second 3D FCN. This reduces the number of voxels the second FCN hasto classify to ~10% and allows it to focus on more detailed segmentation of theorgans and vessels. We utilize training and validation sets consisting of 331 clinical CT imagesand test our models on a completely unseen data collection acquired at adifferent hospital that includes 150 CT scans, targeting three anatomicalorgans (liver, spleen, and pancreas). In challenging organs such as thepancreas, our cascaded approach improves the mean Dice score from 68.5 to82.2%, achieving the highest reported average score on this dataset. We comparewith a 2D FCN method on a separate dataset of 240 CT scans with 18 classes andachieve a significantly higher performance in small organs and vessels.Furthermore, we explore fine-tuning our models to different datasets. Our experiments illustrate the promise and robustness of current 3D FCN basedsemantic segmentation of medical images, achieving state-of-the-art results.Our code and trained models are available for download:https://github.com/holgerroth/3Dunet_abdomen_cascade."}
{"id2": 1091, "id3": 915, "content": "Modern deep learning algorithms have triggered various image segmentationapproaches. However most of them deal with pixel based segmentation. However,superpixels provide a certain degree of contextual information while reducingcomputation cost. In our approach, we have performed superpixel level semanticsegmentation considering 3 various levels as neighbours for semantic contexts.Furthermore, we have enlisted a number of ensemble approaches like max-votingand weighted-average. We have also used the Dempster-Shafer theory ofuncertainty to analyze confusion among various classes. Our method has provedto be superior to a number of different modern approaches on the same dataset."}
{"id2": 1092, "id3": 923, "content": "Image segmentation is the process of partitioning an image into a set ofmeaningful regions according to some criteria. Hierarchical segmentation hasemerged as a major trend in this regard as it favors the emergence of importantregions at different scales. On the other hand, many methods allow us to haveprior information on the position of structures of interest in the images. Inthis paper, we present a versatile hierarchical segmentation method that takesinto account any prior spatial information and outputs a hierarchicalsegmentation that emphasizes the contours or regions of interest whilepreserving the important structures in the image. An application of this methodto the weakly-supervised segmentation problem is presented."}
{"id2": 1093, "id3": 928, "content": "Image segmentation is an important component of many image understandingsystems. It aims to group pixels in a spatially and perceptually coherentmanner. Typically, these algorithms have a collection of parameters thatcontrol the degree of over-segmentation produced. It still remains a challengeto properly select such parameters for human-like perceptual grouping. In thiswork, we exploit the diversity of segments produced by different choices ofparameters. We scan the segmentation parameter space and generate a collectionof image segmentation hypotheses (from highly over-segmented tounder-segmented). These are fed into a cost minimization framework thatproduces the final segmentation by selecting segments that: (1) better describethe natural contours of the image, and (2) are more stable and persistent amongall the segmentation hypotheses. We compare our algorithms performance withstate-of-the-art algorithms, showing that we can achieve improved results. Wealso show that our framework is robust to the choice of segmentation kernelthat produces the initial set of hypotheses."}
{"id2": 1094, "id3": 929, "content": "Superpixel segmentation has become an important research problem in imageprocessing. In this paper, we propose an Iterative Spanning Forest (ISF)framework, based on sequences of Image Foresting Transforms, where one canchoose i) a seed sampling strategy, ii) a connectivity function, iii) anadjacency relation, and iv) a seed pixel recomputation procedure to generateimproved sets of connected superpixels (supervoxels in 3D) per iteration. Thesuperpixels in ISF structurally correspond to spanning trees rooted at thoseseeds. We present five ISF methods to illustrate different choices of itscomponents. These methods are compared with approaches from thestate-of-the-art in effectiveness and efficiency. The experiments involve 2Dand 3D datasets with distinct characteristics, and a high level application,named sky image segmentation. The theoretical properties of ISF aredemonstrated in the supplementary material and the results show that some ofits methods are competitive with or superior to the best baselines ineffectiveness and efficiency."}
{"id2": 1095, "id3": 934, "content": "Deep fully convolutional neural network (FCN) based architectures have showngreat potential in medical image segmentation. However, such architecturesusually have millions of parameters and inadequate number of training samplesleading to over-fitting and poor generalization. In this paper, we present anovel highly parameter and memory efficient FCN based architecture for medicalimage analysis. We propose a novel up-sampling path which incorporates longskip and short-cut connections to overcome the feature map explosion in FCNlike architectures. In order to processes the input images at multiple scalesand view points simultaneously, we propose to incorporate Inception modulesparallel structures. We also propose a novel dual loss function whose weightingscheme allows to combine advantages of cross-entropy and dice loss. We havevalidated our proposed network architecture on two publicly available datasets,namely: (i) Automated Cardiac Disease Diagnosis Challenge (ACDC-2017), (ii)Left Ventricular Segmentation Challenge (LV-2011). Our approach in ACDC-2017challenge stands second place for segmentation and first place in automatedcardiac disease diagnosis tasks with an accuracy of 100%. In the LV-2011challenge our approach attained 0.74 Jaccard index, which is so far the highestpublished result in fully automated algorithms. From the segmentation weextracted clinically relevant cardiac parameters and hand-crafted featureswhich reflected the clinical diagnostic analysis to train an ensemble systemfor cardiac disease classification. Our approach combined both cardiacsegmentation and disease diagnosis into a fully automated framework which iscomputational efficient and hence has the potential to be incorporated incomputer-aided diagnosis (CAD) tools for clinical application."}
{"id2": 1096, "id3": 942, "content": "In this paper we present our system for human-in-the-loop video objectsegmentation. The backbone of our system is a method for one-shot video objectsegmentation. While fast, this method requires an accurate pixel-levelsegmentation of one (or several) frames as input. As manually annotating such asegmentation is impractical, we propose a deep interactive image segmentationmethod, that can accurately segment objects with only a handful of clicks. Onthe GrabCut dataset, our method obtains 90% IOU with just 3.8 clicks onaverage, setting the new state of the art. Furthermore, as our methoditeratively refines an initial segmentation, it can effectively correct frameswhere the video object segmentation fails, thus allowing users to quicklyobtain high quality results even on challenging sequences. Finally, weinvestigate usage patterns and give insights in how many steps users take toannotate frames, what kind of corrections they provide, etc., thus givingimportant insights for further improving interactive video segmentation."}
{"id2": 1097, "id3": 973, "content": "Accurate and reliable brain tumor segmentation is a critical component incancer diagnosis, treatment planning, and treatment outcome evaluation. Buildupon successful deep learning techniques, a novel brain tumor segmentationmethod is developed by integrating fully convolutional neural networks (FCNNs)and Conditional Random Fields (CRFs) in a unified framework to obtainsegmentation results with appearance and spatial consistency. We train a deeplearning based segmentation model using 2D image patches and image slices infollowing steps: 1) training FCNNs using image patches; 2) training CRFs asRecurrent Neural Networks (CRF-RNN) using image slices with parameters of FCNNsfixed; and 3) fine-tuning the FCNNs and the CRF-RNN using image slices.Particularly, we train 3 segmentation models using 2D image patches and slicesobtained in axial, coronal and sagittal views respectively, and combine them tosegment brain tumors using a voting based fusion strategy. Our method couldsegment brain images slice-by-slice, much faster than those based on imagepatches. We have evaluated our method based on imaging data provided by theMultimodal Brain Tumor Image Segmentation Challenge (BRATS) 2013, BRATS 2015and BRATS 2016. The experimental results have demonstrated that our methodcould build a segmentation model with Flair, T1c, and T2 scans and achievecompetitive performance as those built with Flair, T1, T1c, and T2 scans."}
{"id2": 1098, "id3": 997, "content": "In the isointense stage, the accurate volumetric image segmentation is achallenging task due to the low contrast between tissues. In this paper, wepropose a novel very deep network architecture based on a densely convolutionalnetwork for volumetric brain segmentation. The proposed network architectureprovides a dense connection between layers that aims to improve the informationflow in the network. By concatenating features map of fine and coarse denseblocks, it allows capturing multi-scale contextual information. Experimentalresults demonstrate significant advantages of the proposed method over existingmethods, in terms of both segmentation accuracy and parameter efficiency inMICCAI grand challenge on 6-month infant brain MRI segmentation."}
{"id2": 1099, "id3": 998, "content": "For complex segmentation tasks, fully automatic systems are inherentlylimited in their achievable accuracy for extracting relevant objects.Especially in cases where only few data sets need to be processed for a highlyaccurate result, semi-automatic segmentation techniques exhibit a clear benefitfor the user. One area of application is medical image processing during anintervention for a single patient. We propose a learning-based cooperativesegmentation approach which includes the computing entity as well as the userinto the task. Our system builds upon a state-of-the-art fully convolutionalartificial neural network (FCN) as well as an active user model for training.During the segmentation process, a user of the trained system can iterativelyadd additional hints in form of pictorial scribbles as seed points into the FCNsystem to achieve an interactive and precise segmentation result. Thesegmentation quality of interactive FCNs is evaluated. Iterative FCN approachescan yield superior results compared to networks without the user input channelcomponent, due to a consistent improvement in segmentation quality after eachinteraction."}
{"id2": 1100, "id3": 1000, "content": "Semantic image segmentation is an important computer vision task that isdifficult because it consists of both recognition and segmentation. The task isoften cast as a structured output problem on an exponentially largeoutput-space, which is typically modeled by a discrete probabilistic model. Thebest segmentation is found by inferring the Maximum a-Posteriori (MAP) solutionover the output distribution defined by the model. Due to limitations inoptimization, the model cannot be arbitrarily complex. This leads to atrade-off: devise a more accurate model that incorporates rich high-orderinteractions between image elements at the cost of inaccurate and possiblyintractable optimization OR leverage a tractable model which produces lessaccurate MAP solutions but may contain high quality solutions as other modes ofits output distribution. This thesis investigates the latter and presents a two stage approach tosemantic segmentation. In the first stage a tractable segmentation modeloutputs a set of high probability segmentations from the underlyingdistribution that are not just minor perturbations of each other. Criticallythe output of this stage is a diverse set of plausible solutions and not just asingle one. In the second stage, a discriminatively trained re-ranking modelselects the best segmentation from this set. The re-ranking stage can use muchmore complex features than what could be tractably used in the segmentationmodel, allowing a better exploration of the solution space than simplyreturning the MAP solution. The formulation is agnostic to the underlyingsegmentation model (e.g. CRF, CNN, etc.) and optimization algorithm, whichmakes it applicable to a wide range of models and inference methods. Evaluationof the approach on a number of semantic image segmentation benchmark datasetshighlight its superiority over inferring the MAP solution."}
