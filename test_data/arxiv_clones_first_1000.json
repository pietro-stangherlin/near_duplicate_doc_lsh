{"id": "1", "content": "Stereo matching is one of the widely used techniques for inferring depth fromstereo images owing to its robustness and speed. It has become one of the majortopics of research since it finds its applications in autonomous driving,robotic navigation, 3D reconstruction, and many other fields. Finding pixelcorrespondences in non-textured, occluded and reflective areas is the majorchallenge in stereo matching. Recent developments have shown that semantic cuesfrom image segmentation can be used to improve the results of stereo matching.Many deep neural network architectures have been proposed to leverage theadvantages of semantic segmentation in stereo matching. This paper aims to givea comparison among the state of art networks both in terms of accuracy and interms of speed which are of higher importance in real-time applications. ", "id2": "0", "id3": "None"}
{"id": "2", "content": "The recent advancements in artificial intelligence (AI) combined with theextensive amount of data generated by todays clinical systems, has led to thedevelopment of imaging AI solutions across the whole value chain of medicalimaging, including image reconstruction, medical image segmentation,image-based diagnosis and treatment planning. Notwithstanding the successes andfuture potential of AI in medical imaging, many stakeholders are concerned ofthe potential risks and ethical implications of imaging AI solutions, which areperceived as complex, opaque, and difficult to comprehend, utilise, and trustin critical clinical applications. Despite these concerns and risks, there arecurrently no concrete guidelines and best practices for guiding future AIdevelopments in medical imaging towards increased trust, safety and adoption.To bridge this gap, this paper introduces a careful selection of guidingprinciples drawn from the accumulated experiences, consensus, and bestpractices from five large European projects on AI in Health Imaging. Theseguiding principles are named FUTURE-AI and its building blocks consist of (i)Fairness, (ii) Universality, (iii) Traceability, (iv) Usability, (v) Robustnessand (vi) Explainability. In a step-by-step approach, these guidelines arefurther translated into a framework of concrete recommendations for specifying,developing, evaluating, and deploying technically, clinically and ethicallytrustworthy AI solutions into clinical practice. ", "id2": "1", "id3": "None"}
{"id2": 1001, "id3": "1", "content": "The recent advancements in artificial intelligence (AI) combined with theextensive amount of data generated by todays clinical systems, has led to thedevelopment of imaging AI solutions across the whole value chain of medicalimaging, including image reconstruction, medical image segmentation,image-based diagnosis and treatment planning. Notwithstanding the successes andfuture potential of AI in medical imaging, many stakeholders are concerned ofthe potential risks and ethical implications of imaging AI solutions, which areperceived as complex, opaque, and difficult to comprehend, utilise, and trustin critical clinical applications. Despite these concerns and risks, there arecurrently no concrete guidelines and best practices for guiding future AIdevelopments in medical imaging towards increased trust, safety and adoption.To bridge this gap, this paper introduces a careful selection of guidingprinciples drawn from the accumulated experiences, consensus, and bestpractices from five large European projects on AI in Health Imaging. Theseguiding principles are named FUTURE-AI and its building blocks consist of (i)Fairness, (ii) Universality, (iii) Traceability, (iv) Usability, (v) Robustnessand (vi) Explainability. In a step-by-step approach, these guidelines arefurther translated into a framework of concrete recommendations for specifying,developing, evaluating, and deploying technically, clinically and ethicallytrustworthy AI solutions into clinical practice."}
{"id": "3", "content": "In this paper, we proposed a novel mutual consistency network (MC-Net+) toeffectively exploit the unlabeled hard regions for semi-supervised medicalimage segmentation. The MC-Net+ model is motivated by the observation that deepmodels trained with limited annotations are prone to output highly uncertainand easily mis-classified predictions in the ambiguous regions (e.g. adhesiveedges or thin branches) for the image segmentation task. Leveraging theseregion-level challenging samples can make the semi-supervised segmentationmodel training more effective. Therefore, our proposed MC-Net+ model consistsof two new designs. First, the model contains one shared encoder and multiplesightly different decoders (i.e. using different up-sampling strategies). Thestatistical discrepancy of multiple decoders outputs is computed to denote themodels uncertainty, which indicates the unlabeled hard regions. Second, a newmutual consistency constraint is enforced between one decoders probabilityoutput and other decoders soft pseudo labels. In this way, we minimize themodels uncertainty during training and force the model to generate invariantand low-entropy results in such challenging areas of unlabeled data, in orderto learn a generalized feature representation. We compared the segmentationresults of the MC-Net+ with five state-of-the-art semi-supervised approaches onthree public medical datasets. Extension experiments with two commonsemi-supervised settings demonstrate the superior performance of our model overother existing methods, which sets a new state of the art for semi-supervisedmedical image segmentation. ", "id2": "2", "id3": "None"}
{"id": "4", "content": "Consistency training has proven to be an advanced semi-supervised frameworkand achieved promising results in medical image segmentation tasks throughenforcing an invariance of the predictions over different views of the inputs.However, with the iterative updating of model parameters, the models would tendto reach a coupled state and eventually lose the ability to exploit unlabeleddata. To address the issue, we present a novel semi-supervised segmentationmodel based on parameter decoupling strategy to encourage consistentpredictions from diverse views. Specifically, we first adopt a two-branchnetwork to simultaneously produce predictions for each image. During thetraining process, we decouple the two prediction branch parameters by quadraticcosine distance to construct different views in latent space. Based on this,the feature extractor is constrained to encourage the consistency ofprobability maps generated by classifiers under diversified features. In theoverall training process, the parameters of feature extractor and classifiersare updated alternately by consistency regularization operation and decouplingoperation to gradually improve the generalization performance of the model. Ourmethod has achieved a competitive result over the state-of-the-artsemi-supervised methods on the Atrial Segmentation Challenge dataset,demonstrating the effectiveness of our framework. Code is available athttps://github.com/BX0903/PDC. ", "id2": "3", "id3": "None"}
{"id": "5", "content": "To ensure safety in automated driving, the correct perception of thesituation inside the car is as important as its environment. Thus, seatoccupancy detection and classification of detected instances play an importantrole in interior sensing. By the knowledge of the seat occupancy status, it ispossible to, e.g., automate the airbag deployment control. Furthermore, thepresence of a driver, which is necessary for partially automated driving carsat the automation levels two to four can be verified. In this work, we comparedifferent statistical methods from the field of image segmentation to approachthe problem of background-foreground segmentation in camera based interiorsensing. In the recent years, several methods based on different techniqueshave been developed and applied to images or videos from differentapplications. The peculiarity of the given scenarios of interior sensing is,that the foreground instances and the background both contain static as well asdynamic elements. In data considered in this work, even the camera position isnot completely fixed. We review and benchmark three different methods ranging,i.e., Gaussian Mixture Models (GMM), Morphological Snakes and a deep neuralnetwork, namely a Mask R-CNN. In particular, the limitations of the classicalmethods, GMM and Morphological Snakes, for interior sensing are shown.Furthermore, it turns, that it is possible to overcome these limitations bydeep learning, e.g.  using a Mask R-CNN. Although only a small amount of groundtruth data was available for training, we enabled the Mask R-CNN to producehigh quality background-foreground masks via transfer learning. Moreover, wedemonstrate that certain augmentation as well as pre- and post-processingmethods further enhance the performance of the investigated methods. ", "id2": "4", "id3": "None"}
{"id": "6", "content": "High-quality training data play a key role in image segmentation tasks.Usually, pixel-level annotations are expensive, laborious and time-consumingfor the large volume of training data. To reduce labelling cost and improvesegmentation quality, interactive segmentation methods have been proposed,which provide the result with just a few clicks. However, their performancedoes not meet the requirements of practical segmentation tasks in terms ofspeed and accuracy. In this work, we propose EdgeFlow, a novel architecturethat fully utilizes interactive information of user clicks with edge-guidedflow. Our method achieves state-of-the-art performance without anypost-processing or iterative optimization scheme. Comprehensive experiments onbenchmarks also demonstrate the superiority of our method. In addition, withthe proposed method, we develop an efficient interactive segmentation tool forpractical data annotation tasks. The source code and tool is avaliable athttps://github.com/PaddlePaddle/PaddleSeg. ", "id2": "5", "id3": "None"}
{"id": "7", "content": "Semantic segmentation of fine-resolution urban scene images plays a vitalrole in extensive practical applications, such as land cover mapping, urbanchange detection, environmental protection and economic assessment. Driven byrapid developments in deep learning technologies, convolutional neural networks(CNNs) have dominated the semantic segmentation task for many years.Convolutional neural networks adopt hierarchical feature representation andhave strong local context extraction. However, the local property of theconvolution layer limits the network from capturing global information that iscrucial for improving fine-resolution image segmentation. Recently, Transformercomprise a hot topic in the computer vision domain. Vision Transformerdemonstrates the great capability of global information modelling, boostingmany vision tasks, such as image classification, object detection andespecially semantic segmentation. In this paper, we propose an efficient hybridTransformer (EHT) for semantic segmentation of urban scene images. EHT takesadvantage of CNNs and Transformer, learning global-local context to strengthenthe feature representation. Extensive experiments demonstrate that EHT hashigher efficiency with competitive accuracy compared with state-of-the-artbenchmark methods. Specifically, the proposed EHT achieves a 67.0% mIoU on theUAVid test set and outperforms other lightweight models significantly. The codewill be available soon. ", "id2": "6", "id3": "None"}
{"id2": 1002, "id3": "6", "content": "Semantic segmentation of fine-resolution urban scene images plays a vitalrole in extensive practical applications, such as land cover mapping, urbanchange detection, environmental protection and economic assessment. Driven byrapid developments in deep learning technologies, convolutional neural networks(CNNs) have dominated the semantic segmentation task for many years.Convolutional neural networks adopt hierarchical feature representation andhave strong local context extraction. However, the local property of theconvolution layer limits the network from capturing global information that iscrucial for improving fine-resolution image segmentation. Recently, Transformercomprise a hot topic in the computer vision domain. Vision Transformerdemonstrates the great capability of global information modelling, boostingmany vision tasks, such as image classification, object detection andespecially semantic segmentation. In this paper, we propose an efficient hybridTransformer (EHT) for semantic segmentation of urban scene images. EHT takesadvantage of CNNs and Transformer, learning global-local context to strengthenthe feature representation. Extensive experiments demonstrate that EHT hashigher efficiency with competitive accuracy compared with state-of-the-artbenchmark methods. Specifically, the proposed EHT achieves a 67.0% mIoU on theUAVid test set and outperforms other lightweight models significantly. The codewill be available soon."}
{"id": "8", "content": "To mitigate the radiologists workload, computer-aided diagnosis with thecapability to review and analyze medical images is gradually deployed. Deeplearning-based region of interest segmentation is among the most exciting usecases. However, this paradigm is restricted in real-world clinical applicationsdue to poor robustness and generalization. The issue is more sinister with alack of training data. In this paper, we address the challenge from therepresentation learning point of view. We investigate that the collapsedrepresentations, as one of the main reasons which caused poor robustness andgeneralization, could be avoided through transfer learning. Therefore, wepropose a novel two-stage framework for robust generalized segmentation. Inparticular, an unsupervised Tile-wise AutoEncoder (T-AE) pretrainingarchitecture is coined to learn meaningful representation for improving thegeneralization and robustness of the downstream tasks. Furthermore, the learnedknowledge is transferred to the segmentation benchmark. Coupled with an imagereconstruction network, the representation keeps to be decoded, encouraging themodel to capture more semantic features. Experiments of lung segmentation onmulti chest X-ray datasets are conducted. Empirically, the related experimentalresults demonstrate the superior generalization capability of the proposedframework on unseen domains in terms of high performance and robustness tocorruption, especially under the scenario of the limited training data. ", "id2": "7", "id3": "None"}
{"id": "9", "content": "Generalising deep models to new data from new centres (termed here domains)remains a challenge. This is largely attributed to shifts in data statistics(domain shifts) between source and unseen domains. Recently, gradient-basedmeta-learning approaches where the training data are split into meta-train andmeta-test sets to simulate and handle the domain shifts during training haveshown improved generalisation performance. However, the current fullysupervised meta-learning approaches are not scalable for medical imagesegmentation, where large effort is required to create pixel-wise annotations.Meanwhile, in a low data regime, the simulated domain shifts may notapproximate the true domain shifts well across source and unseen domains. Toaddress this problem, we propose a novel semi-supervised meta-learningframework with disentanglement. We explicitly model the representations relatedto domain shifts. Disentangling the representations and combining them toreconstruct the input image allows unlabeled data to be used to betterapproximate the true domain shifts for meta-learning. Hence, the model canachieve better generalisation performance, especially when there is a limitedamount of labeled data. Experiments show that the proposed method is robust ondifferent segmentation tasks and achieves state-of-the-art generalisationperformance on two public benchmarks. ", "id2": "8", "id3": "None"}
{"id": "10", "content": "The success of deep learning methods in medical image segmentation tasksheavily depends on a large amount of labeled data to supervise the training. Onthe other hand, the annotation of biomedical images requires domain knowledgeand can be laborious. Recently, contrastive learning has demonstrated greatpotential in learning latent representation of images even without any label.Existing works have explored its application to biomedical image segmentationwhere only a small portion of data is labeled, through a pre-training phasebased on self-supervised contrastive learning without using any labels followedby a supervised fine-tuning phase on the labeled portion of data only. In thispaper, we establish that by including the limited label in formation in thepre-training phase, it is possible to boost the performance of contrastivelearning. We propose a supervised local contrastive loss that leverages limitedpixel-wise annotation to force pixels with the same label to gather around inthe embedding space. Such loss needs pixel-wise computation which can beexpensive for large images, and we further propose two strategies, downsamplingand block division, to address the issue. We evaluate our methods on two publicbiomedical image datasets of different modalities. With different amounts oflabeled data, our methods consistently outperform the state-of-the-artcontrast-based methods and other semi-supervised learning techniques. ", "id2": "9", "id3": "None"}
{"id": "11", "content": "Image segmentation algorithms often depend on appearance models thatcharacterize the distribution of pixel values in different image regions. Wedescribe a new approach for estimating appearance models directly from animage, without explicit consideration of the pixels that make up each region.Our approach is based on novel algebraic expressions that relate local imagestatistics to the appearance of spatially coherent regions. We describe twoalgorithms that can use the aforementioned algebraic expressions to estimateappearance models directly from an image. The first algorithm solves a systemof linear and quadratic equations using a least squares formulation. The secondalgorithm is a spectral method based on an eigenvector computation. We presentexperimental results that demonstrate the proposed methods work well inpractice and lead to effective image segmentation algorithms. ", "id2": "10", "id3": "None"}
{"id": "12", "content": "The CNN-based methods have achieved impressive results in medical imagesegmentation, but it failed to capture the long-range dependencies due to theinherent locality of convolution operation. Transformer-based methods arepopular in vision tasks recently because of its capacity of long-rangedependencies and get a promising performance. However, it lacks in modelinglocal context, although some works attempted to embed convolutional layer toovercome this problem and achieved some improvement, but it makes the featureinconsistent and fails to leverage the natural multi-scale features ofhierarchical transformer, which limit the performance of models. In this paper,taking medical image segmentation as an example, we present MISSFormer, aneffective and powerful Medical Image Segmentation tranSFormer. MISSFormer is ahierarchical encoder-decoder network and has two appealing designs: 1) A feedforward network is redesigned with the proposed Enhanced Transformer Block,which makes features aligned adaptively and enhances the long-rangedependencies and local context. 2) We proposed Enhanced Transformer ContextBridge, a context bridge with the enhanced transformer block to model thelong-range dependencies and local context of multi-scale features generated byour hierarchical transformer encoder. Driven by these two designs, theMISSFormer shows strong capacity to capture more valuable dependencies andcontext in medical image segmentation. The experiments on multi-organ andcardiac segmentation tasks demonstrate the superiority, effectiveness androbustness of our MISSFormer, the exprimental results of MISSFormer trainedfrom scratch even outperforms state-of-the-art methods pretrained on ImageNet,and the core designs can be generalized to other visual segmentation tasks. Thecode will be released in Github. ", "id2": "11", "id3": "None"}
{"id": "13", "content": "Deep learning has become in recent years a cornerstone tool fueling keyinnovations in the industry, such as autonomous driving. To attain goodperformances, the neural network architecture used for a given application mustbe chosen with care. These architectures are often handcrafted and thereforeprone to human biases and sub-optimal selection. Neural Architecture Search(NAS) is a framework introduced to mitigate such risks by jointly optimizingthe network architectures and its weights. Albeit its novelty, it was appliedon complex tasks with significant results - e.g. semantic image segmentation.In this technical paper, we aim to evaluate its ability to tackle a challengingoperational task: semantic segmentation of objects of interest in satelliteimagery. Designing a NAS framework is not trivial and has strong dependenciesto hardware constraints. We therefore motivate our NAS approach selection andprovide corresponding implementation details. We also present novel ideas tocarry out other such use-case studies. ", "id2": "12", "id3": "None"}
{"id2": 1003, "id3": "12", "content": "Deep learning has become in recent years a cornerstone tool fueling keyinnovations in the industry, such as autonomous driving. To attain goodperformances, the neural network architecture used for a given application mustbe chosen with care. These architectures are often handcrafted and thereforeprone to human biases and sub-optimal selection. Neural Architecture Search(NAS) is a framework introduced to mitigate such risks by jointly optimizingthe network architectures and its weights. Albeit its novelty, it was appliedon complex tasks with significant results - e.g. semantic image segmentation.In this technical paper, we aim to evaluate its ability to tackle a challengingoperational task: semantic segmentation of objects of interest in satelliteimagery. Designing a NAS framework is not trivial and has strong dependenciesto hardware constraints. We therefore motivate our NAS approach selection andprovide corresponding implementation details. We also present novel ideas tocarry out other such use-case studies."}
{"id": "14", "content": "Tensor networks are efficient factorisations of high dimensional tensors intoa network of lower order tensors. They have been most commonly used to modelentanglement in quantum many-body systems and more recently are witnessingincreased applications in supervised machine learning. In this work, weformulate image segmentation in a supervised setting with tensor networks. Thekey idea is to first lift the pixels in image patches to exponentially highdimensional feature spaces and using a linear decision hyper-plane to classifythe input pixels into foreground and background classes. The high dimensionallinear model itself is approximated using the matrix product state (MPS) tensornetwork. The MPS is weight-shared between the non-overlapping image patchesresulting in our strided tensor network model. The performance of the proposedmodel is evaluated on three 2D- and one 3D- biomedical imaging datasets. Theperformance of the proposed tensor network segmentation model is compared withrelevant baseline methods. In the 2D experiments, the tensor network modelyeilds competitive performance compared to the baseline methods while beingmore resource efficient. ", "id2": "13", "id3": "None"}
{"id": "15", "content": "Simultaneous segmentation of multiple organs from different medical imagingmodalities is a crucial task as it can be utilized for computer-aideddiagnosis, computer-assisted surgery, and therapy planning. Thanks to therecent advances in deep learning, several deep neural networks for medicalimage segmentation have been introduced successfully for this purpose. In thispaper, we focus on learning a deep multi-organ segmentation network that labelsvoxels. In particular, we examine the critical choice of a loss function inorder to handle the notorious imbalance problem that plagues both the input andoutput of a learning model. The input imbalance refers to the class-imbalancein the input training samples (i.e., small foreground objects embedded in anabundance of background voxels, as well as organs of varying sizes). The outputimbalance refers to the imbalance between the false positives and falsenegatives of the inference model. In order to tackle both types of imbalanceduring training and inference, we introduce a new curriculum learning basedloss function. Specifically, we leverage Dice similarity coefficient to determodel parameters from being held at bad local minima and at the same timegradually learn better model parameters by penalizing for falsepositives/negatives using a cross entropy term. We evaluated the proposed lossfunction on three datasets: whole body positron emission tomography (PET) scanswith 5 target organs, magnetic resonance imaging (MRI) prostate scans, andultrasound echocardigraphy images with a single target organ i.e., leftventricular. We show that a simple network architecture with the proposedintegrative loss function can outperform state-of-the-art methods and resultsof the competing methods can be improved when our proposed loss is used. ", "id2": "14", "id3": "None"}
{"id": "16", "content": "Semi-supervised learning (SSL) uses unlabeled data to compensate for thescarcity of annotated images and the lack of method generalization to unseendomains, two usual problems in medical segmentation tasks. In this work, wepropose POPCORN, a novel method combining consistency regularization andpseudo-labeling designed for image segmentation. The proposed framework useshigh-level regularization to constrain our segmentation model to use similarlatent features for images with similar segmentations. POPCORN estimates aproximity graph to select data from easiest ones to more difficult ones, inorder to ensure accurate pseudo-labeling and to limit confirmation bias.Applied to multiple sclerosis lesion segmentation, our method demonstratescompetitive results compared to other state-of-the-art SSL strategies. ", "id2": "15", "id3": "None"}
{"id": "17", "content": "Modern deep neural networks struggle to transfer knowledge and generalizeacross domains when deploying to real-world applications. Domain generalization(DG) aims to learn a universal representation from multiple source domains toimprove the network generalization ability on unseen target domains. PreviousDG methods mostly focus on the data-level consistency scheme to advance thegeneralization capability of deep networks, without considering the synergisticregularization of different consistency schemes. In this paper, we present anovel Hierarchical Consistency framework for Domain Generalization (HCDG) byensembling Extrinsic Consistency and Intrinsic Consistency. Particularly, forExtrinsic Consistency, we leverage the knowledge across multiple source domainsto enforce data-level consistency. Also, we design a novel AmplitudeGaussian-mixing strategy for Fourier-based data augmentation to enhance suchconsistency. For Intrinsic Consistency, we perform task-level consistency forthe same instance under the dual-task form. We evaluate the proposed HCDGframework on two medical image segmentation tasks, i.e., optic cup/discsegmentation on fundus images and prostate MRI segmentation. Extensiveexperimental results manifest the effectiveness and versatility of our HCDGframework. Code will be available once accept. ", "id2": "16", "id3": "None"}
{"id": "18", "content": "Deep neural networks have been a prevailing technique in the field of medicalimage processing. However, the most popular convolutional neural networks(CNNs) based methods for medical image segmentation are imperfect because theymodel long-range dependencies by stacking layers or enlarging filters.Transformers and the self-attention mechanism are recently proposed toeffectively learn long-range dependencies by modeling all pairs of word-to-wordattention regardless of their positions. The idea has also been extended to thecomputer vision field by creating and treating image patches as embeddings.Considering the computation complexity for whole image self-attention, currenttransformer-based models settle for a rigid partitioning scheme thatpotentially loses informative relations. Besides, current medical transformersmodel global context on full resolution images, leading to unnecessarycomputation costs. To address these issues, we developed a novel method tointegrate multi-scale attention and CNN feature extraction using a pyramidalnetwork architecture, namely Pyramid Medical Transformer (PMTrans). The PMTranscaptured multi-range relations by working on multi-resolution images. Anadaptive partitioning scheme was implemented to retain informative relationsand to access different receptive fields efficiently. Experimental results onthree medical image datasets (gland segmentation, MoNuSeg, and HECKTORdatasets) showed that PMTrans outperformed the latest CNN-based andtransformer-based models for medical image segmentation. ", "id2": "17", "id3": "None"}
{"id": "19", "content": "We propose a novel neural-network-based method to perform matting of videosdepicting people that does not require additional user input such as trimaps.Our architecture achieves temporal stability of the resulting alpha mattes byusing motion-estimation-based smoothing of image-segmentation algorithmoutputs, combined with convolutional-LSTM modules on U-Net skip connections.  We also propose a fake-motion algorithm that generates training clips for thevideo-matting network given photos with ground-truth alpha mattes andbackground videos. We apply random motion to photos and their mattes tosimulate movement one would find in real videos and composite the result withthe background clips. It lets us train a deep neural network operating onvideos in an absence of a large annotated video dataset and providesground-truth training-clip foreground optical flow for use in loss functions. ", "id2": "18", "id3": "None"}
{"id": "20", "content": "Image segmentation is a common and challenging task in autonomous driving.Availability of sufficient pixel-level annotations for the training data is ahurdle. Active learning helps learning from small amounts of data by suggestingthe most promising samples for labeling. In this work, we propose a newpool-based method for active learning, which proposes promising patchesextracted from full image, in each acquisition step. The problem is framed inan exploration-exploitation framework by combining an embedding based onUniform Manifold Approximation to model representativeness with entropy asuncertainty measure to model informativeness. We applied our proposed method tothe autonomous driving datasets CamVid and Cityscapes and performed aquantitative comparison with state-of-the-art baselines. We find that ouractive learning method achieves better performance compared to previousmethods. ", "id2": "19", "id3": "None"}
{"id": "21", "content": "Most recent semantic segmentation methods adopt a U-Net framework with anencoder-decoder architecture. It is still challenging for U-Net with a simpleskip connection scheme to model the global multi-scale context: 1) Not eachskip connection setting is effective due to the issue of incompatible featuresets of encoder and decoder stage, even some skip connection negativelyinfluence the segmentation performance; 2) The original U-Net is worse than theone without any skip connection on some datasets. Based on our findings, wepropose a new segmentation framework, named UCTransNet (with a proposed CTransmodule in U-Net), from the channel perspective with attention mechanism.Specifically, the CTrans module is an alternate of the U-Net skip connections,which consists of a sub-module to conduct the multi-scale Channel Cross fusionwith Transformer (named CCT) and a sub-module Channel-wise Cross-Attention(named CCA) to guide the fused multi-scale channel-wise information toeffectively connect to the decoder features for eliminating the ambiguity.Hence, the proposed connection consisting of the CCT and CCA is able to replacethe original skip connection to solve the semantic gaps for an accurateautomatic medical image segmentation. The experimental results suggest that ourUCTransNet produces more precise segmentation performance and achievesconsistent improvements over the state-of-the-art for semantic segmentationacross different datasets and conventional architectures involving transformeror U-shaped framework. Code: https://github.com/McGregorWwww/UCTransNet. ", "id2": "20", "id3": "None"}
{"id": "22", "content": "Detection faults in seismic data is a crucial step for seismic structuralinterpretation, reservoir characterization and well placement. Some recentworks regard it as an image segmentation task. The task of image segmentationrequires huge labels, especially 3D seismic data, which has a complex structureand lots of noise. Therefore, its annotation requires expert experience and ahuge workload. In this study, we present lambda-BCE and lambda-smooth L1loss toeffectively train 3D-CNN by some slices from 3D seismic data, so that the modelcan learn the segmentation of 3D seismic data from a few 2D slices. In order tofully extract information from limited data and suppress seismic noise, wepropose an attention module that can be used for active supervision trainingand embedded in the network. The attention heatmap label is generated by theoriginal label, and letting it supervise the attention module using thelambda-smooth L1loss. The experiment demonstrates the effectiveness of our lossfunction, the method can extract 3D seismic features from a few 2D slicelabels. And it also shows the advanced performance of the attention module,which can significantly suppress the noise in the seismic data while increasingthe models sensitivity to the foreground. Finally, on the public test set, weonly use the 2D slice labels training that accounts for 3.3% of the 3D volumelabel, and achieve similar performance to the 3D volume label training. ", "id2": "21", "id3": "None"}
{"id2": 1004, "id3": "21", "content": "Detection faults in seismic data is a crucial step for seismic structuralinterpretation, reservoir characterization and well placement. Some recentworks regard it as an image segmentation task. The task of image segmentationrequires huge labels, especially 3D seismic data, which has a complex structureand lots of noise. Therefore, its annotation requires expert experience and ahuge workload. In this study, we present lambda-BCE and lambda-smooth L1loss toeffectively train 3D-CNN by some slices from 3D seismic data, so that the modelcan learn the segmentation of 3D seismic data from a few 2D slices. In order tofully extract information from limited data and suppress seismic noise, wepropose an attention module that can be used for active supervision trainingand embedded in the network. The attention heatmap label is generated by theoriginal label, and letting it supervise the attention module using thelambda-smooth L1loss. The experiment demonstrates the effectiveness of our lossfunction, the method can extract 3D seismic features from a few 2D slicelabels. And it also shows the advanced performance of the attention module,which can significantly suppress the noise in the seismic data while increasingthe models sensitivity to the foreground. Finally, on the public test set, weonly use the 2D slice labels training that accounts for 3.3% of the 3D volumelabel, and achieve similar performance to the 3D volume label training."}
{"id": "23", "content": "Machine learning has been utilized to perform tasks in many different domainssuch as classification, object detection, image segmentation and naturallanguage analysis. Data labeling has always been one of the most importanttasks in machine learning. However, labeling large amounts of data increasesthe monetary cost in machine learning. As a result, researchers started tofocus on reducing data annotation and labeling costs. Transfer learning wasdesigned and widely used as an efficient approach that can reasonably reducethe negative impact of limited data, which in turn, reduces the datapreparation cost. Even transferring previous knowledge from a source domainreduces the amount of data needed in a target domain. However, large amounts ofannotated data are still demanded to build robust models and improve theprediction accuracy of the model. Therefore, researchers started to pay moreattention on auto annotation and labeling. In this survey paper, we provide areview of previous techniques that focuses on optimized data annotation andlabeling for video, audio, and text data. ", "id2": "22", "id3": "None"}
{"id": "24", "content": "Ultra-high resolution image segmentation has raised increasing interests inrecent years due to its realistic applications. In this paper, we innovate thewidely used high-resolution image segmentation pipeline, in which an ultra-highresolution image is partitioned into regular patches for local segmentation andthen the local results are merged into a high-resolution semantic mask. Inparticular, we introduce a novel locality-aware contextual correlation basedsegmentation model to process local patches, where the relevance between localpatch and its various contexts are jointly and complementarily utilized tohandle the semantic regions with large variations. Additionally, we present acontextual semantics refinement network that associates the local segmentationresult with its contextual semantics, and thus is endowed with the ability ofreducing boundary artifacts and refining mask contours during the generation offinal high-resolution mask. Furthermore, in comprehensive experiments, wedemonstrate that our model outperforms other state-of-the-art methods in publicbenchmarks. Our released codes are available athttps://github.com/liqiokkk/FCtL. ", "id2": "23", "id3": "None"}
{"id": "25", "content": "Motivated by a $2$-dimensional (unsupervised) image segmentation task wherebylocal regions of pixels are clustered via edge detection methods, a moregeneral probabilistic mathematical framework is devised. Critical thresholdsare calculated that indicate strong correlation between randomly-generated,high dimensional data points that have been projected into structures in apartition of a bounded, $2$-dimensional area, of which, an image is a specialcase. A neighbor concept for structures in the partition is defined and acritical radius is uncovered. Measured from a central structure in localizedregions of the partition, the radius indicates strong, long and short rangecorrelation in the count of occupied structures. The size of a short intervalof radii is estimated upon which the transition from short-to-long rangecorrelation is virtually assured, which defines a demarcation of when an imageceases to be interesting. ", "id2": "24", "id3": "None"}
{"id": "26", "content": "Semantic segmentation models trained on public datasets have achieved greatsuccess in recent years. However, these models didnt consider thepersonalization issue of segmentation though it is important in practice. Inthis paper, we address the problem of personalized image segmentation. Theobjective is to generate more accurate segmentation results on unlabeledpersonalized images by investigating the datas personalized traits. To open upfuture research in this area, we collect a large dataset containing varioususers personalized images called PIS (Personalized Image SemanticSegmentation). We also survey some recent researches related to this problemand report their performance on our dataset. Furthermore, by observing thecorrelation among a users personalized images, we propose a baseline methodthat incorporates the inter-image context when segmenting certain images.Extensive experiments show that our method outperforms the existing methods onthe proposed dataset. The code and the PIS dataset will be made publiclyavailable. ", "id2": "25", "id3": "None"}
{"id": "27", "content": "Image segmentation is often ambiguous at the level of individual imagepatches and requires contextual information to reach label consensus. In thispaper we introduce Segmenter, a transformer model for semantic segmentation. Incontrast to convolution-based methods, our approach allows to model globalcontext already at the first layer and throughout the network. We build on therecent Vision Transformer (ViT) and extend it to semantic segmentation. To doso, we rely on the output embeddings corresponding to image patches and obtainclass labels from these embeddings with a point-wise linear decoder or a masktransformer decoder. We leverage models pre-trained for image classificationand show that we can fine-tune them on moderate sized datasets available forsemantic segmentation. The linear decoder allows to obtain excellent resultsalready, but the performance can be further improved by a mask transformergenerating class masks. We conduct an extensive ablation study to show theimpact of the different parameters, in particular the performance is better forlarge models and small patch sizes. Segmenter attains excellent results forsemantic segmentation. It outperforms the state of the art on both ADE20K andPascal Context datasets and is competitive on Cityscapes. ", "id2": "26", "id3": "None"}
{"id": "28", "content": "Elevator button recognition is a critical function to realize the autonomousoperation of elevators. However, challenging image conditions and various imagedistortions make it difficult to recognize buttons accurately. To fill thisgap, we propose a novel deep learning-based approach, which aims toautonomously correct perspective distortions of elevator button images based onbutton corner detection results. First, we leverage a novel image segmentationmodel and the Hough Transform method to obtain button segmentation and buttoncorner detection results. Then, pixel coordinates of standard button cornersare used as reference features to estimate camera motions for correctingperspective distortions. Fifteen elevator button images are captured fromdifferent angles of view as the dataset. The experimental results demonstratethat our proposed approach is capable of estimating camera motions and removingperspective distortions of elevator button images with high accuracy. ", "id2": "27", "id3": "None"}
{"id": "29", "content": "Deep learning has achieved remarkable success in medicalimage segmentation,but it usually requires a large numberof images labeled with fine-grainedsegmentation masks, andthe annotation of these masks can be very expensiveandtime-consuming. Therefore, recent methods try to use un-supervised domainadaptation (UDA) methods to borrow in-formation from labeled data from otherdatasets (source do-mains) to a new dataset (target domain). However, due totheabsence of labels in the target domain, the performance ofUDA methods is muchworse than that of the fully supervisedmethod. In this paper, we propose aweakly supervised do-main adaptation setting, in which we can partially labelnewdatasets with bounding boxes, which are easier and cheaperto obtain thansegmentation masks. Accordingly, we proposea new weakly-supervised domainadaptation method calledBox-Adapt, which fully explores the fine-grainedsegmenta-tion mask in the source domain and the weak bounding boxin the targetdomain. Our Box-Adapt is a two-stage methodthat first performs joint trainingon the source and target do-mains, and then conducts self-training with thepseudo-labelsof the target domain. We demonstrate the effectiveness ofourmethod in the liver segmentation task. Weakly supervised do-main adaptation ", "id2": "28", "id3": "None"}
{"id": "30", "content": "Co-occurrent visual pattern makes aggregating contextual information a commonparadigm to enhance the pixel representation for semantic image segmentation.The existing approaches focus on modeling the context from the perspective ofthe whole image, i.e., aggregating the image-level contextual information.Despite impressive, these methods weaken the significance of the pixelrepresentations of the same category, i.e., the semantic-level contextualinformation. To address this, this paper proposes to augment the pixelrepresentations by aggregating the image-level and semantic-level contextualinformation, respectively. First, an image-level context module is designed tocapture the contextual information for each pixel in the whole image. Second,we aggregate the representations of the same category for each pixel where thecategory regions are learned under the supervision of the ground-truthsegmentation. Third, we compute the similarities between each pixelrepresentation and the image-level contextual information, the semantic-levelcontextual information, respectively. At last, a pixel representation isaugmented by weighted aggregating both the image-level contextual informationand the semantic-level contextual information with the similarities as theweights. Integrating the image-level and semantic-level context allows thispaper to report state-of-the-art accuracy on four benchmarks, i.e., ADE20K,LIP, COCOStuff and Cityscapes. ", "id2": "29", "id3": "None"}
{"id": "31", "content": "Thanks to their ability to learn flexible data-driven losses, GenerativeAdversarial Networks (GANs) are an integral part of many semi- andweakly-supervised methods for medical image segmentation. GANs jointly optimisea generator and an adversarial discriminator on a set of training data. Aftertraining has completed, the discriminator is usually discarded and only thegenerator is used for inference. But should we discard discriminators? In thiswork, we argue that training stable discriminators produces expressive lossfunctions that we can re-use at inference to detect and correct segmentationmistakes. First, we identify key challenges and suggest possible solutions tomake discriminators re-usable at inference. Then, we show that we can combinediscriminators with image reconstruction costs (via decoders) to furtherimprove the model. Our method is simple and improves the test-time performanceof pre-trained GANs. Moreover, we show that it is compatible with standardpost-processing techniques and it has potentials to be used for OnlineContinual Learning. With our work, we open new research avenues for re-usingadversarial discriminators at inference. ", "id2": "30", "id3": "None"}
{"id": "32", "content": "This paper studies the context aggregation problem in semantic imagesegmentation. The existing researches focus on improving the pixelrepresentations by aggregating the contextual information within individualimages. Though impressive, these methods neglect the significance of therepresentations of the pixels of the corresponding class beyond the inputimage. To address this, this paper proposes to mine the contextual informationbeyond individual images to further augment the pixel representations. We firstset up a feature memory module, which is updated dynamically during training,to store the dataset-level representations of various categories. Then, welearn class probability distribution of each pixel representation under thesupervision of the ground-truth segmentation. At last, the representation ofeach pixel is augmented by aggregating the dataset-level representations basedon the corresponding class probability distribution. Furthermore, by utilizingthe stored dataset-level representations, we also propose a representationconsistent learning strategy to make the classification head better addressintra-class compactness and inter-class dispersion. The proposed method couldbe effortlessly incorporated into existing segmentation frameworks (e.g., FCN,PSPNet, OCRNet and DeepLabV3) and brings consistent performance improvements.Mining contextual information beyond image allows us to report state-of-the-artperformance on various benchmarks: ADE20K, LIP, Cityscapes and COCO-Stuff. ", "id2": "31", "id3": "None"}
{"id": "33", "content": "Transformers have shown impressive performance in various natural languageprocessing and computer vision tasks, due to the capability of modelinglong-range dependencies. Recent progress has demonstrated to combine suchtransformers with CNN-based semantic image segmentation models is verypromising. However, it is not well studied yet on how well a pure transformerbased approach can achieve for image segmentation. In this work, we explore anovel framework for semantic image segmentation, which is encoder-decoder basedFully Transformer Networks (FTN). Specifically, we first propose a PyramidGroup Transformer (PGT) as the encoder for progressively learning hierarchicalfeatures, while reducing the computation complexity of the standard visualtransformer(ViT). Then, we propose a Feature Pyramid Transformer (FPT) to fusesemantic-level and spatial-level information from multiple levels of the PGTencoder for semantic image segmentation. Surprisingly, this simple baseline canachieve new state-of-the-art results on multiple challenging semanticsegmentation benchmarks, including PASCAL Context, ADE20K and COCO-Stuff. Thesource code will be released upon the publication of this work. ", "id2": "32", "id3": "None"}
{"id2": 1005, "id3": "32", "content": "Transformers have shown impressive performance in various natural languageprocessing and computer vision tasks, due to the capability of modelinglong-range dependencies. Recent progress has demonstrated to combine suchtransformers with CNN-based semantic image segmentation models is verypromising. However, it is not well studied yet on how well a pure transformerbased approach can achieve for image segmentation. In this work, we explore anovel framework for semantic image segmentation, which is encoder-decoder basedFully Transformer Networks (FTN). Specifically, we first propose a PyramidGroup Transformer (PGT) as the encoder for progressively learning hierarchicalfeatures, while reducing the computation complexity of the standard visualtransformer(ViT). Then, we propose a Feature Pyramid Transformer (FPT) to fusesemantic-level and spatial-level information from multiple levels of the PGTencoder for semantic image segmentation. Surprisingly, this simple baseline canachieve new state-of-the-art results on multiple challenging semanticsegmentation benchmarks, including PASCAL Context, ADE20K and COCO-Stuff. Thesource code will be released upon the publication of this work."}
{"id": "34", "content": "The application of deep learning to medical image segmentation has beenhampered due to the lack of abundant pixel-level annotated data. Few-shotSemantic Segmentation (FSS) is a promising strategy for breaking the deadlock.However, a high-performing FSS model still requires sufficient pixel-levelannotated classes for training to avoid overfitting, which leads to itsperformance bottleneck in medical image segmentation due to the unmet need forannotations. Thus, semi-supervised FSS for medical images is accordinglyproposed to utilize unlabeled data for further performance improvement.Nevertheless, existing semi-supervised FSS methods has two obvious defects: (1)neglecting the relationship between the labeled and unlabeled data; (2) usingunlabeled data directly for end-to-end training leads to degeneratedrepresentation learning. To address these problems, we propose a novelsemi-supervised FSS framework for medical image segmentation. The proposedframework employs Poisson learning for modeling data relationship andpropagating supervision signals, and Spatial Consistency Calibration forencouraging the model to learn more coherent representations. In this process,unlabeled samples do not involve in end-to-end training, but providesupervisory information for query image segmentation through graph-basedlearning. We conduct extensive experiments on three medical image segmentationdatasets (i.e. ISIC skin lesion segmentation, abdominal organs segmentation forMRI and abdominal organs segmentation for CT) to demonstrate thestate-of-the-art performance and broad applicability of the proposed framework. ", "id2": "33", "id3": "None"}
{"id": "35", "content": "Deep reinforcement learning augments the reinforcement learning framework andutilizes the powerful representation of deep neural networks. Recent works havedemonstrated the remarkable successes of deep reinforcement learning in variousdomains including finance, medicine, healthcare, video games, robotics, andcomputer vision. In this work, we provide a detailed review of recent andstate-of-the-art research advances of deep reinforcement learning in computervision. We start with comprehending the theories of deep learning,reinforcement learning, and deep reinforcement learning. We then propose acategorization of deep reinforcement learning methodologies and discuss theiradvantages and limitations. In particular, we divide deep reinforcementlearning into seven main categories according to their applications in computervision, i.e. (i)landmark localization (ii) object detection; (iii) objecttracking; (iv) registration on both 2D image and 3D image volumetric data (v)image segmentation; (vi) videos analysis; and (vii) other applications. Each ofthese categories is further analyzed with reinforcement learning techniques,network design, and performance. Moreover, we provide a comprehensive analysisof the existing publicly available datasets and examine source codeavailability. Finally, we present some open issues and discuss future researchdirections on deep reinforcement learning in computer vision ", "id2": "34", "id3": "None"}
{"id": "36", "content": "Segmentation of images is a long-standing challenge in medical AI. This ismainly due to the fact that training a neural network to perform imagesegmentation requires a significant number of pixel-level annotated data, whichis often unavailable. To address this issue, we propose a semi-supervised imagesegmentation technique based on the concept of multi-view learning. In contrastto the previous art, we introduce an adversarial form of dual-view training andemploy a critic to formulate the learning problem in multi-view training as amin-max problem. Thorough quantitative and qualitative evaluations on severaldatasets indicate that our proposed method outperforms state-of-the-art medicalimage segmentation algorithms consistently and comfortably. The code ispublicly available at https://github.com/himashi92/Duo-SegNet ", "id2": "35", "id3": "None"}
{"id": "37", "content": "We investigate Referring Image Segmentation (RIS), which outputs asegmentation map corresponding to the given natural language description. Tosolve RIS efficiently, we need to understand each words relationship withother words, each region in the image to other regions, and cross-modalalignment between linguistic and visual domains. We argue that one of thelimiting factors in the recent methods is that they do not handle theseinteractions simultaneously. To this end, we propose a novel architecturecalled JRNet, which uses a Joint Reasoning Module(JRM) to concurrently capturethe inter-modal and intra-modal interactions. The output of JRM is passedthrough a novel Cross-Modal Multi-Level Fusion (CMMLF) module which furtherrefines the segmentation masks by exchanging contextual information acrossvisual hierarchy through linguistic features acting as a bridge. We presentthorough ablation studies and validate our approachs performance on fourbenchmark datasets, showing considerable performance gains over the existingstate-of-the-art methods. ", "id2": "36", "id3": "None"}
{"id": "38", "content": "Pre-training a recognition model with contrastive learning on a large datasetof unlabeled data has shown great potential to boost the performance of adownstream task, e.g., image classification. However, in domains such asmedical imaging, collecting unlabeled data can be challenging and expensive. Inthis work, we propose to adapt contrastive learning to work with meta-labelannotations, for improving the models performance in medical imagesegmentation even when no additional unlabeled data is available. Meta-labelssuch as the location of a 2D slice in a 3D MRI scan or the type of device used,often come for free during the acquisition process. We use the meta-labels forpre-training the image encoder as well as to regularize a semi-supervisedtraining, in which a reduced set of annotated data is used for training.Finally, to fully exploit the weak annotations, a self-paced learning approachis used to help the learning and discriminate useful labels from noise. Resultson three different medical image segmentation datasets show that our approach:i) highly boosts the performance of a model trained on a few scans, ii)outperforms previous contrastive and semi-supervised approaches, and iii)reaches close to the performance of a model trained on the full data. ", "id2": "37", "id3": "None"}
{"id2": 1006, "id3": "37", "content": "Pre-training a recognition model with contrastive learning on a large datasetof unlabeled data has shown great potential to boost the performance of adownstream task, e.g., image classification. However, in domains such asmedical imaging, collecting unlabeled data can be challenging and expensive. Inthis work, we propose to adapt contrastive learning to work with meta-labelannotations, for improving the models performance in medical imagesegmentation even when no additional unlabeled data is available. Meta-labelssuch as the location of a 2D slice in a 3D MRI scan or the type of device used,often come for free during the acquisition process. We use the meta-labels forpre-training the image encoder as well as to regularize a semi-supervisedtraining, in which a reduced set of annotated data is used for training.Finally, to fully exploit the weak annotations, a self-paced learning approachis used to help the learning and discriminate useful labels from noise. Resultson three different medical image segmentation datasets show that our approach:i) highly boosts the performance of a model trained on a few scans, ii)outperforms previous contrastive and semi-supervised approaches, and iii)reaches close to the performance of a model trained on the full data."}
{"id": "39", "content": "Federated learning (FL) for medical image segmentation becomes morechallenging in multi-task settings where clients might have differentcategories of labels represented in their data. For example, one client mighthave patient data with healthy pancreases only while datasets from otherclients may contain cases with pancreatic tumors. The vanilla federatedaveraging algorithm makes it possible to obtain more generalizable deeplearning-based segmentation models representing the training data from multipleinstitutions without centralizing datasets. However, it might be sub-optimalfor the aforementioned multi-task scenarios. In this paper, we investigateheterogeneous optimization methods that show improvements for the automatedsegmentation of pancreas and pancreatic tumors in abdominal CT images with FLsettings. ", "id2": "38", "id3": "None"}
{"id": "40", "content": "Membership inference attacks (MIA) try to detect if data samples were used totrain a neural network model, e.g. to detect copyright abuses. We show thatmodels with higher dimensional input and output are more vulnerable to MIA, andaddress in more detail models for image translation and semantic segmentation,including medical image segmentation. We show that reconstruction-errors canlead to very effective MIA attacks as they are indicative of memorization.Unfortunately, reconstruction error alone is less effective at discriminatingbetween non-predictable images used in training and easy to predict images thatwere never seen before. To overcome this, we propose using a novelpredictability error that can be computed for each sample, and its computationdoes not require a training set. Our membership error, obtained by subtractingthe predictability error from the reconstruction error, is shown to achievehigh MIA accuracy on an extensive number of benchmarks. ", "id2": "39", "id3": "None"}
{"id2": 1007, "id3": "39", "content": "Membership inference attacks (MIA) try to detect if data samples were used totrain a neural network model, e.g. to detect copyright abuses. We show thatmodels with higher dimensional input and output are more vulnerable to MIA, andaddress in more detail models for image translation and semantic segmentation,including medical image segmentation. We show that reconstruction-errors canlead to very effective MIA attacks as they are indicative of memorization.Unfortunately, reconstruction error alone is less effective at discriminatingbetween non-predictable images used in training and easy to predict images thatwere never seen before. To overcome this, we propose using a novelpredictability error that can be computed for each sample, and its computationdoes not require a training set. Our membership error, obtained by subtractingthe predictability error from the reconstruction error, is shown to achievehigh MIA accuracy on an extensive number of benchmarks."}
{"id": "41", "content": "Weakly supervised image segmentation trained with image-level labels usuallysuffers from inaccurate coverage of object areas during the generation of thepseudo groundtruth. This is because the object activation maps are trained withthe classification objective and lack the ability to generalize. To improve thegenerality of the objective activation maps, we propose a region prototypicalnetwork RPNet to explore the cross-image object diversity of the training set.Similar object parts across images are identified via region featurecomparison. Object confidence is propagated between regions to discover newobject areas while background regions are suppressed. Experiments show that theproposed method generates more complete and accurate pseudo object masks, whileachieving state-of-the-art performance on PASCAL VOC 2012 and MS COCO. Inaddition, we investigate the robustness of the proposed method on reducedtraining sets. ", "id2": "40", "id3": "None"}
{"id": "42", "content": "Automated segmentation in medical image analysis is a challenging task thatrequires a large amount of manually labeled data. However, most existinglearning-based approaches usually suffer from limited manually annotatedmedical data, which poses a major practical problem for accurate and robustmedical image segmentation. In addition, most existing semi-supervisedapproaches are usually not robust compared with the supervised counterparts,and also lack explicit modeling of geometric structure and semanticinformation, both of which limit the segmentation accuracy. In this work, wepresent SimCVD, a simple contrastive distillation framework that significantlyadvances state-of-the-art voxel-wise representation learning. We first describean unsupervised training strategy, which takes two views of an input volume andpredicts their signed distance maps of object boundaries in a contrastiveobjective, with only two independent dropout as mask. This simple approachworks surprisingly well, performing on the same level as previous fullysupervised methods with much less labeled data. We hypothesize that dropout canbe viewed as a minimal form of data augmentation and makes the network robustto representation collapse. Then, we propose to perform structural distillationby distilling pair-wise similarities. We evaluate SimCVD on two populardatasets: the Left Atrial Segmentation Challenge (LA) and the NIH pancreas CTdataset. The results on the LA dataset demonstrate that, in two types oflabeled ratios (i.e., 20% and 10%), SimCVD achieves an average Dice score of90.85% and 89.03% respectively, a 0.91% and 2.22% improvement compared toprevious best results. Our method can be trained in an end-to-end fashion,showing the promise of utilizing SimCVD as a general framework for downstreamtasks, such as medical image synthesis and registration. ", "id2": "41", "id3": "None"}
{"id": "43", "content": "Accurate automatic liver and tumor segmentation plays a vital role intreatment planning and disease monitoring. Recently, deep convolutional neuralnetwork (DCNNs) has obtained tremendous success in 2D and 3D medical imagesegmentation. However, 2D DCNNs cannot fully leverage the inter-sliceinformation, while 3D DCNNs are computationally expensive and memory intensive.To address these issues, we first propose a novel dense-sparse training flowfrom a data perspective, in which, densely adjacent slices and sparselyadjacent slices are extracted as inputs for regularizing DCNNs, therebyimproving the model performance. Moreover, we design a 2.5D light-weightnnU-Net from a network perspective, in which, depthwise separable convolutionsare adopted to improve the efficiency. Extensive experiments on the LiTSdataset have demonstrated the superiority of the proposed method. ", "id2": "42", "id3": "None"}
{"id": "44", "content": "Unmanned aerial vehicles (UAVs) equipped with multiple complementary sensorshave tremendous potential for fast autonomous or remote-controlled semanticscene analysis, e.g., for disaster examination. In this work, we propose a UAVsystem for real-time semantic inference and fusion of multiple sensormodalities. Semantic segmentation of LiDAR scans and RGB images, as well asobject detection on RGB and thermal images, run online onboard the UAV computerusing lightweight CNN architectures and embedded inference accelerators. Wefollow a late fusion approach where semantic information from multiplemodalities augments 3D point clouds and image segmentation masks while alsogenerating an allocentric semantic map. Our system provides augmented semanticimages and point clouds with $ approx ,$9$ ,$Hz. We evaluate the integratedsystem in real-world experiments in an urban environment. ", "id2": "43", "id3": "None"}
{"id": "45", "content": "The random walker method for image segmentation is a popular tool forsemi-automatic image segmentation, especially in the biomedical field. However,its linear asymptotic run time and memory requirements make application to 3Ddatasets of increasing sizes impractical. We propose a hierarchical frameworkthat, to the best of our knowledge, is the first attempt to overcome theserestrictions for the random walker algorithm and achieves sublinear run timeand constant memory complexity. The goal of this framework is -- rather thanimproving the segmentation quality compared to the baseline method -- to makeinteractive segmentation on out-of-core datasets possible. The method isevaluated quantitavely on synthetic data and the CT-ORG dataset where theexpected improvements in algorithm run time while maintaining high segmentationquality are confirmed. The incremental (i.e., interaction update) run time isdemonstrated to be in seconds on a standard PC even for volumes of hundreds ofGigabytes in size. In a small case study the applicability to large real worldfrom current biomedical research is demonstrated. An implementation of thepresented method is publicly available in version 5.2 of the widely used volumerendering and processing software Voreen (https://www.uni-muenster.de/Voreen/). ", "id2": "44", "id3": "None"}
{"id2": 1008, "id3": "44", "content": "The random walker method for image segmentation is a popular tool forsemi-automatic image segmentation, especially in the biomedical field. However,its linear asymptotic run time and memory requirements make application to 3Ddatasets of increasing sizes impractical. We propose a hierarchical frameworkthat, to the best of our knowledge, is the first attempt to overcome theserestrictions for the random walker algorithm and achieves sublinear run timeand constant memory complexity. The goal of this framework is -- rather thanimproving the segmentation quality compared to the baseline method -- to makeinteractive segmentation on out-of-core datasets possible. The method isevaluated quantitavely on synthetic data and the CT-ORG dataset where theexpected improvements in algorithm run time while maintaining high segmentationquality are confirmed. The incremental (i.e., interaction update) run time isdemonstrated to be in seconds on a standard PC even for volumes of hundreds ofGigabytes in size. In a small case study the applicability to large real worldfrom current biomedical research is demonstrated. An implementation of thepresented method is publicly available in version 5.2 of the widely used volumerendering and processing software Voreen (https://www.uni-muenster.de/Voreen/)."}
{"id": "46", "content": "In this work, we address the challenging task of few-shot segmentation.Previous few-shot segmentation methods mainly employ the information of supportimages as guidance for query image segmentation. Although some works propose tobuild cross-reference between support and query images, their extraction ofquery information still depends on the support images. We here propose toextract the information from the query itself independently to benefit thefew-shot segmentation task. To this end, we first propose a prior extractor tolearn the query information from the unlabeled images with our proposedglobal-local contrastive learning. Then, we extract a set of predeterminedpriors via this prior extractor. With the obtained priors, we generate theprior region maps for query images, which locate the objects, as guidance toperform cross interaction with support features. In such a way, the extractionof query information is detached from the support branch, overcoming thelimitation by support, and could obtain more informative query clues to achievebetter interaction. Without bells and whistles, the proposed approach achievesnew state-of-the-art performance for the few-shot segmentation task onPASCAL-5$^ i $ and COCO datasets. ", "id2": "45", "id3": "None"}
{"id": "47", "content": "In this paper, we present a new network named Attention Aware Network (AASeg)for real time semantic image segmentation. Our network incorporates spatial andchannel information using Spatial Attention (SA) and Channel Attention (CA)modules respectively. It also uses dense local multi-scale context informationusing Multi Scale Context (MSC) module. The feature maps are concatenatedindividually to produce the final segmentation map. We demonstrate theeffectiveness of our method using a comprehensive analysis, quantitativeexperimental results and ablation study using Cityscapes, ADE20K and Camviddatasets. Our network performs better than most previous architectures with a74.4 % Mean IOU on Cityscapes test dataset while running at 202.7 FPS. ", "id2": "46", "id3": "None"}
{"id": "48", "content": "The development of high quality medical image segmentation algorithms dependson the availability of large datasets with pixel-level labels. The challengesof collecting such datasets, especially in case of 3D volumes, motivate todevelop approaches that can learn from other types of labels that are cheap toobtain, e.g. bounding boxes. We focus on 3D medical images with theircorresponding 3D bounding boxes which are considered as series of per-slicenon-tight 2D bounding boxes. While current weakly-supervised approaches thatuse 2D bounding boxes as weak labels can be applied to medical imagesegmentation, we show that their success is limited in cases when theassumption about the tightness of the bounding boxes breaks. We propose a newbounding box correction framework which is trained on a small set ofpixel-level annotations to improve the tightness of a larger set of non-tightbounding box annotations. The effectiveness of our solution is demonstrated byevaluating a known weakly-supervised segmentation approach with and without theproposed bounding box correction algorithm. When the tightness is improved byour solution, the results of the weakly-supervised segmentation become muchcloser to those of the fully-supervised one. ", "id2": "47", "id3": "None"}
{"id": "49", "content": "Contrastive Learning (CL) is a recent representation learning approach, whichencourages inter-class separability and intra-class compactness in learnedimage representations. Since medical images often contain multiple semanticclasses in an image, using CL to learn representations of local features (asopposed to global) is important. In this work, we present a novelsemi-supervised 2D medical segmentation solution that applies CL on imagepatches, instead of full images. These patches are meaningfully constructedusing the semantic information of different classes obtained via pseudolabeling. We also propose a novel consistency regularization (CR) scheme, whichworks in synergy with CL. It addresses the problem of confirmation bias, andencourages better clustering in the feature space. We evaluate our method onfour public medical segmentation datasets and a novel histopathology datasetthat we introduce. Our method obtains consistent improvements overstate-of-the-art semi-supervised segmentation approaches for all datasets. ", "id2": "48", "id3": "None"}
{"id": "50", "content": "Domain adaptation (DA) has drawn high interest for its capacity to adapt amodel trained on labeled source data to perform well on unlabeled or weaklylabeled target data from a different domain. Most common DA techniques requireconcurrent access to the input images of both the source and target domains.However, in practice, privacy concerns often impede the availability of sourceimages in the adaptation phase. This is a very frequent DA scenario in medicalimaging, where, for instance, the source and target images could come fromdifferent clinical sites. We introduce a source-free domain adaptation forimage segmentation. Our formulation is based on minimizing a label-free entropyloss defined over target-domain data, which we further guide with adomain-invariant prior on the segmentation regions. Many priors can be derivedfrom anatomical information. Here, a class ratio prior is estimated fromanatomical knowledge and integrated in the form of a Kullback Leibler (KL)divergence in our overall loss function. Furthermore, we motivate our overallloss with an interesting link to maximizing the mutual information between thetarget images and their label predictions. We show the effectiveness of ourprior aware entropy minimization in a variety of domain-adaptation scenarios,with different modalities and applications, including spine, prostate, andcardiac segmentation. Our method yields comparable results to several state ofthe art adaptation techniques, despite having access to much less information,as the source images are entirely absent in our adaptation phase. Ourstraightforward adaptation strategy uses only one network, contrary to popularadversarial techniques, which are not applicable to a source-free DA setting.Our framework can be readily used in a breadth of segmentation problems, andour code is publicly available: https://github.com/mathilde-b/SFDA ", "id2": "49", "id3": "None"}
{"id": "51", "content": "Semantic segmentation of medical images is an essential first step incomputer-aided diagnosis systems for many applications. However, given manydisparate imaging modalities and inherent variations in the patient data, it isdifficult to consistently achieve high accuracy using modern deep neuralnetworks (DNNs). This has led researchers to propose interactive imagesegmentation techniques where a medical expert can interactively correct theoutput of a DNN to the desired accuracy. However, these techniques often needseparate training data with the associated human interactions, and do notgeneralize to various diseases, and types of medical images. In this paper, wesuggest a novel conditional inference technique for DNNs which takes theintervention by a medical expert as test time constraints and performsinference conditioned upon these constraints. Our technique is generic can beused for medical images from any modality. Unlike other methods, our approachcan correct multiple structures simultaneously and add structures missed atinitial segmentation. We report an improvement of 13.3, 12.5, 17.8, 10.2, and12.4 times in user annotation time than full human annotation for the nucleus,multiple cells, liver and tumor, organ, and brain segmentation respectively. Wereport a time saving of 2.8, 3.0, 1.9, 4.4, and 8.6 fold compared to otherinteractive segmentation techniques. Our method can be useful to clinicians fordiagnosis and post-surgical follow-up with minimal intervention from themedical expert. The source-code and the detailed results are available here[1]. ", "id2": "50", "id3": "None"}
{"id": "52", "content": "Recent advances in brain clearing and imaging have made it possible to imageentire mammalian brains at sub-micron resolution. These images offer thepotential to assemble brain-wide atlases of projection neuron morphology, butmanual neuron reconstruction remains a bottleneck. In this paper we present aprobabilistic method which combines a hidden Markov state process that encodesneuron geometric properties with a random field appearance model of theflourescence process. Our method utilizes dynamic programming to efficientlycompute the global maximizers of what we call the most probable neuron path.We applied our algorithm to the output of image segmentation models where falsenegatives severed neuronal processes, and showed that it can follow axons inthe presence of noise or nearby neurons. Our method has the potential to beintegrated into a semi or fully automated reconstruction pipeline.Additionally, it creates a framework for conditioning the probability to fixedstart and endpoints through which users can intervene with hard constraints to,for example, rule out certain reconstructions, or assign axons to particularcell bodies. ", "id2": "51", "id3": "None"}
{"id": "53", "content": "Quantifying uncertainty in medical image segmentation applications isessential, as it is often connected to vital decision-making. Compellingattempts have been made in quantifying the uncertainty in image segmentationarchitectures, e.g. to learn a density segmentation model conditioned on theinput image. Typical work in this field restricts these learnt densities to bestrictly Gaussian. In this paper, we propose to use a more flexible approach byintroducing Normalizing Flows (NFs), which enables the learnt densities to bemore complex and facilitate more accurate modeling for uncertainty. We provethis hypothesis by adopting the Probabilistic U-Net and augmenting theposterior density with an NF, allowing it to be more expressive. Ourqualitative as well as quantitative (GED and IoU) evaluations on themulti-annotated and single-annotated LIDC-IDRI and Kvasir-SEG segmentationdatasets, respectively, show a clear improvement. This is mostly apparent inthe quantification of aleatoric uncertainty and the increased predictiveperformance of up to 14 percent. This result strongly indicates that a moreflexible density model should be seriously considered in architectures thatattempt to capture segmentation ambiguity through density modeling. The benefitof this improved modeling will increase human confidence in annotation andsegmentation, and enable eager adoption of the technology in practice. ", "id2": "52", "id3": "None"}
{"id": "54", "content": "Shape modelling (with methods that output shapes) is a new and important taskin Bayesian nonparametrics and bioinformatics. In this work, we focus onBayesian nonparametric methods for capturing shapes by partitioning a spaceusing curves. In related work, the classical Mondrian process is used topartition spaces recursively with axis-aligned cuts, and is widely applied inmulti-dimensional and relational data. The Mondrian process outputshyper-rectangles. Recently, the random tessellation process was introduced as ageneralization of the Mondrian process, partitioning a domain with non-axisaligned cuts in an arbitrary dimensional space, and outputting polytopes.Motivated by these processes, in this work, we propose a novel parallelizedBayesian nonparametric approach to partition a domain with curves, enablingcomplex data-shapes to be acquired. We apply our method to HIV-1-infected humanmacrophage image dataset, and also simulated datasets sets to illustrate ourapproach. We compare to support vector machines, random forests andstate-of-the-art computer vision methods such as simple linear iterativeclustering super pixel image segmentation. We develop an R package that isavailable at url https://github.com/ShufeiGe/Shape-Modeling-with-Spline-Partitions . ", "id2": "53", "id3": "None"}
{"id2": 1009, "id3": "53", "content": "Shape modelling (with methods that output shapes) is a new and important taskin Bayesian nonparametrics and bioinformatics. In this work, we focus onBayesian nonparametric methods for capturing shapes by partitioning a spaceusing curves. In related work, the classical Mondrian process is used topartition spaces recursively with axis-aligned cuts, and is widely applied inmulti-dimensional and relational data. The Mondrian process outputshyper-rectangles. Recently, the random tessellation process was introduced as ageneralization of the Mondrian process, partitioning a domain with non-axisaligned cuts in an arbitrary dimensional space, and outputting polytopes.Motivated by these processes, in this work, we propose a novel parallelizedBayesian nonparametric approach to partition a domain with curves, enablingcomplex data-shapes to be acquired. We apply our method to HIV-1-infected humanmacrophage image dataset, and also simulated datasets sets to illustrate ourapproach. We compare to support vector machines, random forests andstate-of-the-art computer vision methods such as simple linear iterativeclustering super pixel image segmentation. We develop an R package that isavailable at url https://github.com/ShufeiGe/Shape-Modeling-with-Spline-Partitions ."}
{"id": "55", "content": "While improving prediction accuracy has been the focus of machine learning inrecent years, this alone does not suffice for reliable decision-making.Deploying learning systems in consequential settings also requires calibratingand communicating the uncertainty of predictions. To convey instance-wiseuncertainty for prediction tasks, we show how to generate set-valuedpredictions from a black-box predictor that control the expected loss on futuretest points at a user-specified level. Our approach provides explicitfinite-sample guarantees for any dataset by using a holdout set to calibratethe size of the prediction sets. This framework enables simple,distribution-free, rigorous error control for many tasks, and we demonstrate itin five large-scale machine learning problems: (1) classification problemswhere some mistakes are more costly than others; (2) multi-labelclassification, where each observation has multiple associated labels; (3)classification problems where the labels have a hierarchical structure; (4)image segmentation, where we wish to predict a set of pixels containing anobject of interest; and (5) protein structure prediction. Lastly, we discussextensions to uncertainty quantification for ranking, metric learning anddistributionally robust learning. ", "id2": "54", "id3": "None"}
{"id": "56", "content": "Although having achieved great success in medical image segmentation, deepconvolutional neural networks usually require a large dataset with manualannotations for training and are difficult to generalize to unseen classes.Few-shot learning has the potential to address these challenges by learning newclasses from only a few labeled examples. In this work, we propose a newframework for few-shot medical image segmentation based on prototypicalnetworks. Our innovation lies in the design of two key modules: 1) a contextrelation encoder (CRE) that uses correlation to capture local relation featuresbetween foreground and background regions; and 2) a recurrent mask refinementmodule that repeatedly uses the CRE and a prototypical network to recapture thechange of context relationship and refine the segmentation mask iteratively.Experiments on two abdomen CT datasets and an abdomen MRI dataset show theproposed method obtains substantial improvement over the state-of-the-artmethods by an average of 16.32%, 8.45% and 6.24% in terms of DSC, respectively.Code is publicly available. ", "id2": "55", "id3": "None"}
{"id": "57", "content": "Semi-supervised learning (SSL) uses unlabeled data during training to learnbetter models. Previous studies on SSL for medical image segmentation focusedmostly on improving model generalization to unseen data. In some applications,however, our primary interest is not generalization but to obtain optimalpredictions on a specific unlabeled database that is fully available duringmodel development. Examples include population studies for extracting imagingphenotypes. This work investigates an often overlooked aspect of SSL,transduction. It focuses on the quality of predictions made on the unlabeleddata of interest when they are included for optimization during training,rather than improving generalization. We focus on the self-training frameworkand explore its potential for transduction. We analyze it through the lens ofInformation Gain and reveal that learning benefits from the use of calibratedor under-confident models. Our extensive experiments on a large MRI databasefor multi-class segmentation of traumatic brain lesions shows promising resultswhen comparing transductive with inductive predictions. We believe this studywill inspire further research on transductive learning, a well-suited paradigmfor medical image analysis. ", "id2": "56", "id3": "None"}
{"id": "58", "content": "When confronted with objects of unknown types in an image, humans caneffortlessly and precisely tell their visual boundaries. This recognitionmechanism and underlying generalization capability seem to contrast tostate-of-the-art image segmentation networks that rely on large-scalecategory-aware annotated training samples. In this paper, we make an attempttowards building models that explicitly account for visual boundary knowledge,in hope to reduce the training effort on segmenting unseen categories.Specifically, we investigate a new task termed as Boundary KnowledgeTranslation (BKT). Given a set of fully labeled categories, BKT aims totranslate the visual boundary knowledge learned from the labeled categories, toa set of novel categories, each of which is provided only a few labeledsamples. To this end, we propose a Translation Segmentation Network(Trans-Net), which comprises a segmentation network and two boundarydiscriminators. The segmentation network, combined with a boundary-awareself-supervised mechanism, is devised to conduct foreground segmentation, whilethe two discriminators work together in an adversarial manner to ensure anaccurate segmentation of the novel categories under light supervision.Exhaustive experiments demonstrate that, with only tens of labeled samples asguidance, Trans-Net achieves close results on par with fully supervisedmethods. ", "id2": "57", "id3": "None"}
{"id": "59", "content": "Machine learning techniques have been paramount throughout the last years,being applied in a wide range of tasks, such as classification, objectrecognition, person identification, and image segmentation. Nevertheless,conventional classification algorithms, e.g., Logistic Regression, DecisionTrees, and Bayesian classifiers, might lack complexity and diversity, notsuitable when dealing with real-world data. A recent graph-inspired classifier,known as the Optimum-Path Forest, has proven to be a state-of-the-arttechnique, comparable to Support Vector Machines and even surpassing it in sometasks. This paper proposes a Python-based Optimum-Path Forest framework,denoted as OPFython, where all of its functions and classes are based upon theoriginal C language implementation. Additionally, as OPFython is a Python-basedlibrary, it provides a more friendly environment and a faster prototypingworkspace than the C language. ", "id2": "58", "id3": "None"}
{"id": "60", "content": "Deep learning has not been routinely employed for semantic segmentation ofseabed environment for synthetic aperture sonar (SAS) imagery due to theimplicit need of abundant training data such methods necessitate. Abundanttraining data, specifically pixel-level labels for all images, is usually notavailable for SAS imagery due to the complex logistics (e.g., diver survey,chase boat, precision position information) needed for obtaining accurateground-truth. Many hand-crafted feature based algorithms have been proposed tosegment SAS in an unsupervised fashion. However, there is still room forimprovement as the feature extraction step of these methods is fixed. In thiswork, we present a new iterative unsupervised algorithm for learning deepfeatures for SAS image segmentation. Our proposed algorithm alternates betweenclustering superpixels and updating the parameters of a convolutional neuralnetwork (CNN) so that the feature extraction for image segmentation can beoptimized. We demonstrate the efficacy of our method on a realistic benchmarkdataset. Our results show that the performance of our proposed method isconsiderably better than current state-of-the-art methods in SAS imagesegmentation. ", "id2": "59", "id3": "None"}
{"id": "61", "content": "We introduce a new image segmentation task, termed Entity Segmentation (ES)with the aim to segment all visual entities in an image without consideringsemantic category labels. It has many practical applications in imagemanipulation/editing where the segmentation mask quality is typically crucialbut category labels are less important. In this setting, allsemantically-meaningful segments are equally treated as categoryless entitiesand there is no thing-stuff distinction. Based on our unified entityrepresentation, we propose a center-based entity segmentation framework withtwo novel modules to improve mask quality. Experimentally, both our new taskand framework demonstrate superior advantages as against existing work. Inparticular, ES enables the following: (1) merging multiple datasets to form alarge training set without the need to resolve label conflicts; (2) any modeltrained on one dataset can generalize exceptionally well to other datasets withunseen domains. Our code is made publicly available athttps://github.com/dvlab-research/Entity. ", "id2": "60", "id3": "None"}
{"id": "62", "content": "Humanitarian actions require accurate information to efficiently delegatesupport operations. Such information can be maps of building footprints,building functions, and population densities. While the access to thisinformation is comparably easy in industrialized countries thanks to reliablecensus data and national geo-data infrastructures, this is not the case fordeveloping countries, where that data is often incomplete or outdated. Buildingmaps derived from remote sensing images may partially remedy this challenge insuch countries, but are not always accurate due to different landscapeconfigurations and lack of validation data. Even when they exist, buildingfootprint layers usually do not reveal more fine-grained building properties,such as the number of stories or the buildings function (e.g., office,residential, school, etc.). In this project we aim to automate buildingfootprint and function mapping using heterogeneous data sources. In a firststep, we intend to delineate buildings from satellite data, using deep learningmodels for semantic image segmentation. Building functions shall be retrievedby parsing social media data like for instance tweets, as well as ground-basedimagery, to automatically identify different buildings functions and retrievefurther information such as the number of building stories. Building mapsaugmented with those additional attributes make it possible to derive moreaccurate population density maps, needed to support the targeted provision ofhumanitarian aid. ", "id2": "61", "id3": "None"}
{"id": "63", "content": "The latest advances in computer-assisted precision medicine are making itfeasible to move from population-wide models that are useful to discoveraggregate patterns that hold for group-based analysis to patient-specificmodels that can drive patient-specific decisions with regard to treatmentchoices, and predictions of outcomes of treatment. Body Composition isrecognized as an important driver and risk factor for a wide variety ofdiseases, as well as a predictor of individual patient-specific clinicaloutcomes to treatment choices or surgical interventions. 3D CT images areroutinely acquired in the oncological worklows and deliver accurate renderingof internal anatomy and therefore can be used opportunistically to assess theamount of skeletal muscle and adipose tissue compartments. Powerful tools ofartificial intelligence such as deep learning are making it feasible now tosegment the entire 3D image and generate accurate measurements of all internalanatomy. These will enable the overcoming of the severe bottleneck that existedpreviously, namely, the need for manual segmentation, which was prohibitive toscale to the hundreds of 2D axial slices that made up a 3D volumetric image.Automated tools such as presented here will now enable harvesting whole-bodymeasurements from 3D CT or MRI images, leading to a new era of discovery of thedrivers of various diseases based on individual tissue, organ volume, shape,and functional status. These measurements were hitherto unavailable therebylimiting the field to a very small and limited subset. These discoveries andthe potential to perform individual image segmentation with high speed andaccuracy are likely to lead to the incorporation of these 3D measures intoindividual specific treatment planning models related to nutrition, aging,chemotoxicity, surgery and survival after the onset of a major disease such ascancer. ", "id2": "62", "id3": "None"}
{"id": "64", "content": "We introduce a method that allows to automatically segment images intosemantically meaningful regions without human supervision. Derived regions areconsistent across different images and coincide with human-defined semanticclasses on some datasets. In cases where semantic regions might be hard forhuman to define and consistently label, our method is still able to findmeaningful and consistent semantic classes. In our work, we use pretrainedStyleGAN2~ cite karras2020analyzing  generative model: clustering in thefeature space of the generative model allows to discover semantic classes. Onceclasses are discovered, a synthetic dataset with generated images andcorresponding segmentation masks can be created. After that a segmentationmodel is trained on the synthetic dataset and is able to generalize to realimages. Additionally, by using CLIP~ cite radford2021learning  we are able touse prompts defined in a natural language to discover some desired semanticclasses. We test our method on publicly available datasets and showstate-of-the-art results. ", "id2": "63", "id3": "None"}
{"id": "65", "content": "Accurate image segmentation plays a crucial role in medical image analysis,yet it faces great challenges of various shapes, diverse sizes, and blurryboundaries. To address these difficulties, square kernel-based encoder-decoderarchitecture has been proposed and widely used, but its performance remainsstill unsatisfactory. To further cope with these challenges, we present a noveldouble-branch encoder architecture. Our architecture is inspired by twoobservations: 1) Since the discrimination of features learned via squareconvolutional kernels needs to be further improved, we propose to utilizenon-square vertical and horizontal convolutional kernels in the double-branchencoder, so features learned by the two branches can be expected to complementeach other. 2) Considering that spatial attention can help models to betterfocus on the target region in a large-sized image, we develop an attention lossto further emphasize the segmentation on small-sized targets. Together, theabove two schemes give rise to a novel double-branch encoder segmentationframework for medical image segmentation, namely Crosslink-Net. The experimentsvalidate the effectiveness of our model on four datasets. The code is releasedat https://github.com/Qianyu1226/Crosslink-Net. ", "id2": "64", "id3": "None"}
{"id": "66", "content": "The paper proposes a novel approach for gray scale images segmentation. It isbased on multiple features extraction from single feature per image pixel,namely its intensity value, using Echo state network. The newly extractedfeatures -- reservoir equilibrium states -- reveal hidden image characteristicsthat improve its segmentation via a clustering algorithm. Moreover, it wasdemonstrated that the intrinsic plasticity tuning of reservoir fits itsequilibrium states to the original image intensity distribution thus allowingfor its better segmentation. The proposed approach is tested on the benchmarkimage Lena. ", "id2": "65", "id3": "None"}
{"id": "67", "content": "Learning segmentation from noisy labels is an important task for medicalimage analysis due to the difficulty in acquiring highquality annotations. Mostexisting methods neglect the pixel correlation and structural prior insegmentation, often producing noisy predictions around object boundaries. Toaddress this, we adopt a superpixel representation and develop a robustiterative learning strategy that combines noise-aware training of segmentationnetwork and noisy label refinement, both guided by the superpixels. This designenables us to exploit the structural constraints in segmentation labels andeffectively mitigate the impact of label noise in learning. Experiments on twobenchmarks show that our method outperforms recent state-of-the-art approaches,and achieves superior robustness in a wide range of label noises. Code isavailable at https://github.com/gaozhitong/SP_guided_Noisy_Label_Seg. ", "id2": "66", "id3": "None"}
{"id": "68", "content": "In this paper, we propose a novel evaluation metric for performanceevaluation of semantic segmentation. In recent years, many studies have triedto train pixel-level classifiers on large-scale image datasets to performaccurate semantic segmentation. The goal of semantic segmentation is to assigna class label of each pixel in the scene. It has various potential applicationsin computer vision fields e.g., object detection, classification, sceneunderstanding and Etc. To validate the proposed wIoU evaluation metric, wetested state-of-the art methods on public benchmark datasets (e.g., KITTI)based on the proposed wIoU metric and compared with other conventionalevaluation metrics. ", "id2": "67", "id3": "None"}
{"id": "69", "content": "Deep learning techniques for 3D brain vessel image segmentation have not beenas successful as in the segmentation of other organs and tissues. This can beexplained by two factors. First, deep learning techniques tend to show poorperformances at the segmentation of relatively small objects compared to thesize of the full image. Second, due to the complexity of vascular trees and thesmall size of vessels, it is challenging to obtain the amount of annotatedtraining data typically needed by deep learning methods. To address theseproblems, we propose a novel annotation-efficient deep learning vesselsegmentation framework. The framework avoids pixel-wise annotations, onlyrequiring weak patch-level labels to discriminate between vessel and non-vessel2D patches in the training set, in a setup similar to the CAPTCHAs used todifferentiate humans from bots in web applications. The user-provided weakannotations are used for two tasks: 1) to synthesize pixel-wise pseudo-labelsfor vessels and background in each patch, which are used to train asegmentation network, and 2) to train a classifier network. The classifiernetwork allows to generate additional weak patch labels, further reducing theannotation burden, and it acts as a noise filter for poor quality images. Weuse this framework for the segmentation of the cerebrovascular tree inTime-of-Flight angiography (TOF) and Susceptibility-Weighted Images (SWI). Theresults show that the framework achieves state-of-the-art accuracy, whilereducing the annotation time by ~77% w.r.t. learning-based segmentation methodsusing pixel-wise labels for training. ", "id2": "68", "id3": "None"}
{"id": "70", "content": "Medical image segmentation plays an essential role in developingcomputer-assisted diagnosis and therapy systems, yet still faces manychallenges. In the past few years, the popular encoder-decoder architecturesbased on CNNs (e.g., U-Net) have been successfully applied in the task ofmedical image segmentation. However, due to the locality of convolutionoperations, they demonstrate limitations in learning global context andlong-range spatial relations. Recently, several researchers try to introducetransformers to both the encoder and decoder components with promising results,but the efficiency requires further improvement due to the high computationalcomplexity of transformers. In this paper, we propose LeViT-UNet, whichintegrates a LeViT Transformer module into the U-Net architecture, for fast andaccurate medical image segmentation. Specifically, we use LeViT as the encoderof the LeViT-UNet, which better trades off the accuracy and efficiency of theTransformer block. Moreover, multi-scale feature maps from transformer blocksand convolutional blocks of LeViT are passed into the decoder viaskip-connection, which can effectively reuse the spatial information of thefeature maps. Our experiments indicate that the proposed LeViT-UNet achievesbetter performance comparing to various competing methods on severalchallenging medical image segmentation benchmarks including Synapse and ACDC.Code and models will be publicly available athttps://github.com/apple1986/LeViT_UNet. ", "id2": "69", "id3": "None"}
{"id": "71", "content": "The balance between high accuracy and high speed has always been achallenging task in semantic image segmentation. Compact segmentation networksare more widely used in the case of limited resources, while their performancesare constrained. In this paper, motivated by the residual learning and globalaggregation, we propose a simple yet general and effective knowledgedistillation framework called double similarity distillation (DSD) to improvethe classification accuracy of all existing compact networks by capturing thesimilarity knowledge in pixel and category dimensions, respectively.Specifically, we propose a pixel-wise similarity distillation (PSD) module thatutilizes residual attention maps to capture more detailed spatial dependenciesacross multiple layers. Compared with exiting methods, the PSD module greatlyreduces the amount of calculation and is easy to expand. Furthermore,considering the differences in characteristics between semantic segmentationtask and other computer vision tasks, we propose a category-wise similaritydistillation (CSD) module, which can help the compact segmentation networkstrengthen the global category correlation by constructing the correlationmatrix. Combining these two modules, DSD framework has no extra parameters andonly a minimal increase in FLOPs. Extensive experiments on four challengingdatasets, including Cityscapes, CamVid, ADE20K, and Pascal VOC 2012, show thatDSD outperforms current state-of-the-art methods, proving its effectiveness andgenerality. The code and models will be publicly available. ", "id2": "70", "id3": "None"}
{"id": "72", "content": "In a class of piecewise-constant image segmentation models, we propose toincorporate a weighted difference of anisotropic and isotropic total variation(AITV) to regularize the partition boundaries in an image. In particular, wereplace the total variation regularization in the Chan-Vese segmentation modeland a fuzzy region competition model by the proposed AITV. To deal with thenonconvex nature of AITV, we apply the difference-of-convex algorithm (DCA), inwhich the subproblems can be minimized by the primal-dual hybrid gradientmethod with linesearch. The convergence of the DCA scheme is analyzed. Inaddition, a generalization to color image segmentation is discussed. In thenumerical experiments, we compare the proposed models with the classic convexapproaches and the two-stage segmentation methods (smoothing and thenthresholding) on various images, showing that our models are effective in imagesegmentation and robust with respect to impulsive noises. ", "id2": "71", "id3": "None"}
{"id2": 1010, "id3": "71", "content": "In a class of piecewise-constant image segmentation models, we propose toincorporate a weighted difference of anisotropic and isotropic total variation(AITV) to regularize the partition boundaries in an image. In particular, wereplace the total variation regularization in the Chan-Vese segmentation modeland a fuzzy region competition model by the proposed AITV. To deal with thenonconvex nature of AITV, we apply the difference-of-convex algorithm (DCA), inwhich the subproblems can be minimized by the primal-dual hybrid gradientmethod with linesearch. The convergence of the DCA scheme is analyzed. Inaddition, a generalization to color image segmentation is discussed. In thenumerical experiments, we compare the proposed models with the classic convexapproaches and the two-stage segmentation methods (smoothing and thenthresholding) on various images, showing that our models are effective in imagesegmentation and robust with respect to impulsive noises."}
{"id": "73", "content": "The attractiveness of a property is one of the most interesting, yetchallenging, categories to model. Image characteristics are used to describecertain attributes, and to examine the influence of visual factors on the priceor timeframe of the listing. In this paper, we propose a set of techniques forthe extraction of visual features for efficient numerical inclusion inmodern-day predictive algorithms. We discuss techniques such as Shannonsentropy, calculating the center of gravity, employing image segmentation, andusing Convolutional Neural Networks. After comparing these techniques asapplied to a set of property-related images (indoor, outdoor, and satellite),we conclude the following: (i) the entropy is the most efficient single-digitvisual measure for housing price prediction; (ii) image segmentation is themost important visual feature for the prediction of housing lifespan; and (iii)deep image features can be used to quantify interior characteristics andcontribute to captivation modeling. The set of 40 image features selected herecarries a significant amount of predictive power and outperforms some of thestrongest metadata predictors. Without any need to replace a human expert in areal-estate appraisal process, we conclude that the techniques presented inthis paper can efficiently describe visible characteristics, thus introducingperceived attractiveness as a quantitative measure into the predictive modelingof housing. ", "id2": "72", "id3": "None"}
{"id": "74", "content": "Scarcity of high quality annotated images remains a limiting factor fortraining accurate image segmentation models. While more and more annotateddatasets become publicly available, the number of samples in each individualdatabase is often small. Combining different databases to create larger amountsof training data is appealing yet challenging due to the heterogeneity as aresult of differences in data acquisition and annotation processes, oftenyielding incompatible or even conflicting information. In this paper, weinvestigate and propose several strategies for learning from partiallyoverlapping labels in the context of abdominal organ segmentation. We find thatcombining a semi-supervised approach with an adaptive cross entropy loss cansuccessfully exploit heterogeneously annotated data and substantially improvesegmentation accuracy compared to baseline and alternative approaches. ", "id2": "73", "id3": "None"}
{"id": "75", "content": "In recent years, computer-aided diagnosis has become an increasingly populartopic. Methods based on convolutional neural networks have achieved goodperformance in medical image segmentation and classification. Due to thelimitations of the convolution operation, the long-term spatial features areoften not accurately obtained. Hence, we propose a TransClaw U-Net networkstructure, which combines the convolution operation with the transformeroperation in the encoding part. The convolution part is applied for extractingthe shallow spatial features to facilitate the recovery of the image resolutionafter upsampling. The transformer part is used to encode the patches, and theself-attention mechanism is used to obtain global information betweensequences. The decoding part retains the bottom upsampling structure for betterdetail segmentation performance. The experimental results on SynapseMulti-organ Segmentation Datasets show that the performance of TransClaw U-Netis better than other network structures. The ablation experiments also provethe generalization performance of TransClaw U-Net. ", "id2": "74", "id3": "None"}
{"id": "76", "content": "Convolutional neural networks (CNNs) have led to significant improvements intasks involving semantic segmentation of images. CNNs are vulnerable in thearea of biomedical image segmentation because of distributional gap between twosource and target domains with different data modalities which leads to domainshift. Domain shift makes data annotations in new modalities necessary becausemodels must be retrained from scratch. Unsupervised domain adaptation (UDA) isproposed to adapt a model to new modalities using solely unlabeled targetdomain data. Common UDA algorithms require access to data points in the sourcedomain which may not be feasible in medical imaging due to privacy concerns. Inthis work, we develop an algorithm for UDA in a privacy-constrained setting,where the source domain data is inaccessible. Our idea is based on encoding theinformation from the source samples into a prototypical distribution that isused as an intermediate distribution for aligning the target domaindistribution with the source domain distribution. We demonstrate theeffectiveness of our algorithm by comparing it to state-of-the-art medicalimage semantic segmentation approaches on two medical image semanticsegmentation datasets. ", "id2": "75", "id3": "None"}
{"id": "77", "content": "The segmentation of medical images is a fundamental step in automatedclinical decision support systems. Existing medical image segmentation methodsbased on supervised deep learning, however, remain problematic because of theirreliance on large amounts of labelled training data. Although medical imagingdata repositories continue to expand, there has not been a commensurateincrease in the amount of annotated data. Hence, we propose a new spatialguided self-supervised clustering network (SGSCN) for medical imagesegmentation, where we introduce multiple loss functions designed to aid ingrouping image pixels that are spatially connected and have similar featurerepresentations. It iteratively learns feature representations and clusteringassignment of each pixel in an end-to-end fashion from a single image. We alsopropose a context-based consistency loss that better delineates the shape andboundaries of image regions. It enforces all the pixels belonging to a clusterto be spatially close to the cluster centre. We evaluated our method on 2public medical image datasets and compared it to existing conventional andself-supervised clustering methods. Experimental results show that our methodwas most accurate for medical image segmentation. ", "id2": "76", "id3": "None"}
{"id": "78", "content": "Domain Adaptation (DA) methods are widely used in medical image segmentationtasks to tackle the problem of differently distributed train (source) and test(target) data. We consider the supervised DA task with a limited number ofannotated samples from the target domain. It corresponds to one of the mostrelevant clinical setups: building a sufficiently accurate model on the minimumpossible amount of annotated data. Existing methods mostly fine-tune specificlayers of the pretrained Convolutional Neural Network (CNN). However, there isno consensus on which layers are better to fine-tune, e.g. the first layers forimages with low-level domain shift or the deeper layers for images withhigh-level domain shift. To this end, we propose SpotTUnet - a CNN architecturethat automatically chooses the layers which should be optimally fine-tuned.More specifically, on the target domain, our method additionally learns thepolicy that indicates whether a specific layer should be fine-tuned or reusedfrom the pretrained network. We show that our method performs at the same levelas the best of the nonflexible fine-tuning methods even under the extremescarcity of annotated data. Secondly, we show that SpotTUnet policy provides alayer-wise visualization of the domain shift impact on the network, which couldbe further used to develop robust domain generalization methods. In order toextensively evaluate SpotTUnet performance, we use a publicly available datasetof brain MR images (CC359), characterized by explicit domain shift. We releasea reproducible experimental pipeline. ", "id2": "77", "id3": "None"}
{"id": "79", "content": "A large labeled dataset is a key to the success of supervised deep learning,but for medical image segmentation, it is highly challenging to obtainsufficient annotated images for model training. In many scenarios, unannotatedimages are abundant and easy to acquire. Self-supervised learning (SSL) hasshown great potentials in exploiting raw data information and representationlearning. In this paper, we propose Hierarchical Self-Supervised Learning(HSSL), a new self-supervised framework that boosts medical image segmentationby making good use of unannotated data. Unlike the current literature ontask-specific self-supervised pretraining followed by supervised fine-tuning,we utilize SSL to learn task-agnostic knowledge from heterogeneous data forvarious medical image segmentation tasks. Specifically, we first aggregate adataset from several medical challenges, then pre-train the network in aself-supervised manner, and finally fine-tune on labeled data. We develop a newloss function by combining contrastive loss and classification loss andpretrain an encoder-decoder architecture for segmentation tasks. Our extensiveexperiments show that multi-domain joint pre-training benefits downstreamsegmentation tasks and outperforms single-domain pre-training significantly.Compared to learning from scratch, our new method yields better performance onvarious tasks (e.g., +0.69% to +18.60% in Dice scores with 5% of annotateddata). With limited amounts of training data, our method can substantiallybridge the performance gap w.r.t. denser annotations (e.g., 10% vs.~100% ofannotated data). ", "id2": "78", "id3": "None"}
{"id": "80", "content": "Medical image segmentation - the prerequisite of numerous clinical needs -has been significantly prospered by recent advances in convolutional neuralnetworks (CNNs). However, it exhibits general limitations on modeling explicitlong-range relation, and existing cures, resorting to building deep encodersalong with aggressive downsampling operations, leads to redundant deepenednetworks and loss of localized details. Hence, the segmentation task awaits abetter solution to improve the efficiency of modeling global contexts whilemaintaining a strong grasp of low-level details. In this paper, we propose anovel parallel-in-branch architecture, TransFuse, to address this challenge.TransFuse combines Transformers and CNNs in a parallel style, where both globaldependency and low-level spatial details can be efficiently captured in a muchshallower manner. Besides, a novel fusion technique - BiFusion module iscreated to efficiently fuse the multi-level features from both branches.Extensive experiments demonstrate that TransFuse achieves the neweststate-of-the-art results on both 2D and 3D medical image sets including polyp,skin lesion, hip, and prostate segmentation, with significant parameterdecrease and inference speed improvement. ", "id2": "79", "id3": "None"}
{"id": "81", "content": "The reliability of Deep Learning systems depends on their accuracy but alsoon their robustness against adversarial perturbations to the input data.Several attacks and defenses have been proposed to improve the performance ofDeep Neural Networks under the presence of adversarial noise in the naturalimage domain. However, robustness in computer-aided diagnosis for volumetricdata has only been explored for specific tasks and with limited attacks. Wepropose a new framework to assess the robustness of general medical imagesegmentation systems. Our contributions are two-fold: (i) we propose a newbenchmark to evaluate robustness in the context of the Medical SegmentationDecathlon (MSD) by extending the recent AutoAttack natural image classificationframework to the domain of volumetric data segmentation, and (ii) we present anovel lattice architecture for RObust Generic medical image segmentation (ROG).Our results show that ROG is capable of generalizing across different tasks ofthe MSD and largely surpasses the state-of-the-art under sophisticatedadversarial attacks. ", "id2": "80", "id3": "None"}
{"id": "82", "content": "In medical image segmentation, it is difficult to mark ambiguous areasaccurately with binary masks, especially when dealing with small lesions.Therefore, it is a challenge for radiologists to reach a consensus by usingbinary masks under the condition of multiple annotations. However, these areasmay contain anatomical structures that are conducive to diagnosis. Uncertaintyis introduced to study these situations. Nevertheless, the uncertainty isusually measured by the variances between predictions in a multiple trial way.It is not intuitive, and there is no exact correspondence in the image.Inspired by image matting, we introduce matting as a soft segmentation methodand a new perspective to deal with and represent uncertain regions into medicalscenes, namely medical matting. More specifically, because there is noavailable medical matting dataset, we first labeled two medical datasets withalpha matte. Secondly, the matting method applied to the natural image is notsuitable for the medical scene, so we propose a new architecture to generatebinary masks and alpha matte in a row. Thirdly, the uncertainty map isintroduced to highlight the ambiguous regions from the binary results andimprove the matting performance. Evaluated on these datasets, the proposedmodel outperformed state-of-the-art matting algorithms by a large margin, andalpha matte is proved to be a more efficient labeling form than a binary mask. ", "id2": "81", "id3": "None"}
{"id": "83", "content": "Probabilistic finite mixture models are widely used for unsupervisedclustering. These models can often be improved by adapting them to the topologyof the data. For instance, in order to classify spatially adjacent data pointssimilarly, it is common to introduce a Laplacian constraint on the posteriorprobability that each data point belongs to a class. Alternatively, the mixingprobabilities can be treated as free parameters, while assuming Gauss-Markov ormore complex priors to regularize those mixing probabilities. However, theseapproaches are constrained by the shape of the prior and often lead tocomplicated or intractable inference. Here, we propose a new parametrization ofthe Dirichlet distribution to flexibly regularize the mixing probabilities ofover-parametrized mixture distributions. Using the Expectation-Maximizationalgorithm, we show that our approach allows us to define any linear update rulefor the mixing probabilities, including spatial smoothing regularization as aspecial case. We then show that this flexible design can be extended to shareclass information between multiple mixture models. We apply our algorithm toartificial and natural image segmentation tasks, and we provide quantitativeand qualitative comparison of the performance of Gaussian and Student-tmixtures on the Berkeley Segmentation Dataset. We also demonstrate how topropagate class information across the layers of deep convolutional neuralnetworks in a probabilistically optimal way, suggesting a new interpretationfor feedback signals in biological visual systems. Our flexible approach can beeasily generalized to adapt probabilistic mixture models to arbitrary datatopologies. ", "id2": "82", "id3": "None"}
{"id": "84", "content": "Assigning meaning to parts of image data is the goal of semantic imagesegmentation. Machine learning methods, specifically supervised learning iscommonly used in a variety of tasks formulated as semantic segmentation. One ofthe major challenges in the supervised learning approaches is expressing andcollecting the rich knowledge that experts have with respect to the meaningpresent in the image data. Towards this, typically a fixed set of labels isspecified and experts are tasked with annotating the pixels, patches orsegments in the images with the given labels. In general, however, the set ofclasses does not fully capture the rich semantic information present in theimages. For example, in medical imaging such as histology images, the differentparts of cells could be grouped and sub-grouped based on the expertise of thepathologist.  To achieve such a precise semantic representation of the concepts in theimage, we need access to the full depth of knowledge of the annotator. In thiswork, we develop a novel approach to collect segmentation annotations fromexperts based on psychometric testing. Our method consists of the psychometrictesting procedure, active query selection, query enhancement, and a deep metriclearning model to achieve a patch-level image embedding that allows forsemantic segmentation of images. We show the merits of our method withevaluation on the synthetically generated image, aerial image and histologyimage. ", "id2": "83", "id3": "None"}
{"id": "85", "content": "Semi-supervised learning has attracted great attention in the field ofmachine learning, especially for medical image segmentation tasks, since italleviates the heavy burden of collecting abundant densely annotated data fortraining. However, most of existing methods underestimate the importance ofchallenging regions (e.g. small branches or blurred edges) during training. Webelieve that these unlabeled regions may contain more crucial information tominimize the uncertainty prediction for the model and should be emphasized inthe training process. Therefore, in this paper, we propose a novel MutualConsistency Network (MC-Net) for semi-supervised left atrium segmentation from3D MR images. Particularly, our MC-Net consists of one encoder and two slightlydifferent decoders, and the prediction discrepancies of two decoders aretransformed as an unsupervised loss by our designed cycled pseudo label schemeto encourage mutual consistency. Such mutual consistency encourages the twodecoders to have consistent and low-entropy predictions and enables the modelto gradually capture generalized features from these unlabeled challengingregions. We evaluate our MC-Net on the public Left Atrium (LA) database and itobtains impressive performance gains by exploiting the unlabeled dataeffectively. Our MC-Net outperforms six recent semi-supervised methods for leftatrium segmentation, and sets the new state-of-the-art performance on the LAdatabase. ", "id2": "84", "id3": "None"}
{"id": "86", "content": "Over the past decade, Deep Convolutional Neural Networks have been widelyadopted for medical image segmentation and shown to achieve adequateperformance. However, due to the inherent inductive biases present in theconvolutional architectures, they lack understanding of long-range dependenciesin the image. Recently proposed Transformer-based architectures that leverageself-attention mechanism encode long-range dependencies and learnrepresentations that are highly expressive. This motivates us to exploreTransformer-based solutions and study the feasibility of usingTransformer-based network architectures for medical image segmentation tasks.Majority of existing Transformer-based network architectures proposed forvision applications require large-scale datasets to train properly. However,compared to the datasets for vision applications, for medical imaging thenumber of data samples is relatively low, making it difficult to efficientlytrain transformers for medical applications. To this end, we propose a GatedAxial-Attention model which extends the existing architectures by introducingan additional control mechanism in the self-attention module. Furthermore, totrain the model effectively on medical images, we propose a Local-Globaltraining strategy (LoGo) which further improves the performance. Specifically,we operate on the whole image and patches to learn global and local features,respectively. The proposed Medical Transformer (MedT) is evaluated on threedifferent medical image segmentation datasets and it is shown that it achievesbetter performance than the convolutional and other related transformer-basedarchitectures. Code: https://github.com/jeya-maria-jose/Medical-Transformer ", "id2": "85", "id3": "None"}
{"id": "87", "content": "Imperfect labels limit the quality of predictions learned by deep neuralnetworks. This is particularly relevant in medical image segmentation, wherereference annotations are difficult to collect and vary significantly evenacross expert annotators. Prior work on mitigating label noise focused onsimple models of mostly uniform noise. In this work, we explore biased andunbiased errors artificially introduced to brain tumour annotations on MRIdata. We found that supervised and semi-supervised segmentation methods arerobust or fairly robust to unbiased errors but sensitive to biased errors. Itis therefore important to identify the sorts of errors expected in medicalimage labels and especially mitigate the biased errors. ", "id2": "86", "id3": "None"}
{"id": "88", "content": "Despite recent progress of automatic medical image segmentation techniques,fully automatic results usually fail to meet the clinical use and typicallyrequire further refinement. In this work, we propose a quality-aware memorynetwork for interactive segmentation of 3D medical images. Provided by userguidance on an arbitrary slice, an interaction network is firstly employed toobtain an initial 2D segmentation. The quality-aware memory networksubsequently propagates the initial segmentation estimation bidirectionallyover the entire volume. Subsequent refinement based on additional user guidanceon other slices can be incorporated in the same manner. To further facilitateinteractive segmentation, a quality assessment module is introduced to suggestthe next slice to segment based on the current segmentation quality of eachslice. The proposed network has two appealing characteristics: 1) Thememory-augmented network offers the ability to quickly encode past segmentationinformation, which will be retrieved for the segmentation of other slices; 2)The quality assessment module enables the model to directly estimate thequalities of segmentation predictions, which allows an active learning paradigmwhere users preferentially label the lowest-quality slice for multi-roundrefinement. The proposed network leads to a robust interactive segmentationengine, which can generalize well to various types of user annotations (e.g.,scribbles, boxes). Experimental results on various medical datasets demonstratethe superiority of our approach in comparison with existing techniques. ", "id2": "87", "id3": "None"}
{"id": "89", "content": "Clustering is an unsupervised machine learning method grouping data samplesinto clusters of similar objects. In practice, clustering has been used innumerous applications such as banking customers profiling, document retrieval,image segmentation, and e-commerce recommendation engines. However, theexisting clustering techniques present significant limitations, from which isthe dependability of their stability on the initialization parameters (e.g.number of clusters, centroids). Different solutions were presented in theliterature to overcome this limitation (i.e. internal and external validationmetrics). However, these solutions require high computational complexity andmemory consumption, especially when dealing with big data. In this paper, weapply the recent object detection Deep Learning (DL) model, named YOLO-v5, todetect the initial clustering parameters such as the number of clusters withtheir sizes and centroids. Mainly, the proposed solution consists of adding aDL-based initialization phase making the clustering algorithms free ofinitialization. Two model solutions are provided in this work, one for isolatedclusters and the other one for overlapping clusters. The features of theincoming dataset determine which model to use. Moreover, The results show thatthe proposed solution can provide near-optimal clusters initializationparameters with low computational and resources overhead compared to existingsolutions. ", "id2": "88", "id3": "None"}
{"id": "90", "content": "A strong visual object tracker nowadays relies on its well-crafted modules,which typically consist of manually-designed network architectures to deliverhigh-quality tracking results. Not surprisingly, the manual design processbecomes a particularly challenging barrier, as it demands sufficient priorexperience, enormous effort, intuition and perhaps some good luck. Meanwhile,neural architecture search has gaining grounds in practical applications suchas image segmentation, as a promising method in tackling the issue of automatedsearch of feasible network structures. In this work, we propose a novelcell-level differentiable architecture search mechanism to automate the networkdesign of the tracking module, aiming to adapt backbone features to theobjective of a tracking network during offline training. The proposed approachis simple, efficient, and with no need to stack a series of modules toconstruct a network. Our approach is easy to be incorporated into existingtrackers, which is empirically validated using different differentiablearchitecture search-based methods and tracking objectives. Extensiveexperimental evaluations demonstrate the superior performance of our approachover five commonly-used benchmarks. Meanwhile, our automated searching processtakes 41 (18) hours for the second (first) order DARTS method on theTrackingNet dataset. ", "id2": "89", "id3": "None"}
{"id2": 1011, "id3": "89", "content": "A strong visual object tracker nowadays relies on its well-crafted modules,which typically consist of manually-designed network architectures to deliverhigh-quality tracking results. Not surprisingly, the manual design processbecomes a particularly challenging barrier, as it demands sufficient priorexperience, enormous effort, intuition and perhaps some good luck. Meanwhile,neural architecture search has gaining grounds in practical applications suchas image segmentation, as a promising method in tackling the issue of automatedsearch of feasible network structures. In this work, we propose a novelcell-level differentiable architecture search mechanism to automate the networkdesign of the tracking module, aiming to adapt backbone features to theobjective of a tracking network during offline training. The proposed approachis simple, efficient, and with no need to stack a series of modules toconstruct a network. Our approach is easy to be incorporated into existingtrackers, which is empirically validated using different differentiablearchitecture search-based methods and tracking objectives. Extensiveexperimental evaluations demonstrate the superior performance of our approachover five commonly-used benchmarks. Meanwhile, our automated searching processtakes 41 (18) hours for the second (first) order DARTS method on theTrackingNet dataset."}
{"id": "91", "content": "Deep learning-based segmentation methods are vulnerable to unforeseen datadistribution shifts during deployment, e.g. change of image appearances orcontrasts caused by different scanners, unexpected imaging artifacts etc. Inthis paper, we present a cooperative framework for training image segmentationmodels and a latent space augmentation method for generating hard examples.Both contributions improve model generalization and robustness with limiteddata. The cooperative training framework consists of a fast-thinking network(FTN) and a slow-thinking network (STN). The FTN learns decoupled imagefeatures and shape features for image reconstruction and segmentation tasks.The STN learns shape priors for segmentation correction and refinement. The twonetworks are trained in a cooperative manner. The latent space augmentationgenerates challenging examples for training by masking the decoupled latentspace in both channel-wise and spatial-wise manners. We performed extensiveexperiments on public cardiac imaging datasets. Using only 10 subjects from asingle site for training, we demonstrated improved cross-site segmentationperformance and increased robustness against various unforeseen imagingartifacts compared to strong baseline methods. Particularly, cooperativetraining with latent space data augmentation yields 15% improvement in terms ofaverage Dice score when compared to a standard training method. ", "id2": "90", "id3": "None"}
{"id": "92", "content": "Transformer architecture has emerged to be successful in a number of naturallanguage processing tasks. However, its applications to medical vision remainlargely unexplored. In this study, we present UTNet, a simple yet powerfulhybrid Transformer architecture that integrates self-attention into aconvolutional neural network for enhancing medical image segmentation. UTNetapplies self-attention modules in both encoder and decoder for capturinglong-range dependency at different scales with minimal overhead. To this end,we propose an efficient self-attention mechanism along with relative positionencoding that reduces the complexity of self-attention operation significantlyfrom $O(n^2)$ to approximate $O(n)$. A new self-attention decoder is alsoproposed to recover fine-grained details from the skipped connections in theencoder. Our approach addresses the dilemma that Transformer requires hugeamounts of data to learn vision inductive bias. Our hybrid layer design allowsthe initialization of Transformer into convolutional networks without a need ofpre-training. We have evaluated UTNet on the multi-label, multi-vendor cardiacmagnetic resonance imaging cohort. UTNet demonstrates superior segmentationperformance and robustness against the state-of-the-art approaches, holding thepromise to generalize well on other medical image segmentations. ", "id2": "91", "id3": "None"}
{"id": "93", "content": "Semantic segmentation is one of the basic, yet essential scene understandingtasks for an autonomous agent. The recent developments in supervised machinelearning and neural networks have enjoyed great success in enhancing theperformance of the state-of-the-art techniques for this task. However, theirsuperior performance is highly reliant on the availability of a large-scaleannotated dataset. In this paper, we propose a novel fully unsupervisedsemantic segmentation method, the so-called Information Maximization andAdversarial Regularization Segmentation (InMARS). Inspired by human perceptionwhich parses a scene into perceptual groups, rather than analyzing each pixelindividually, our proposed approach first partitions an input image intomeaningful regions (also known as superpixels). Next, it utilizesMutual-Information-Maximization followed by an adversarial training strategy tocluster these regions into semantically meaningful classes. To customize anadversarial training scheme for the problem, we incorporate adversarial pixelnoise along with spatial perturbations to impose photometrical and geometricalinvariance on the deep neural network. Our experiments demonstrate that ourmethod achieves the state-of-the-art performance on two commonly usedunsupervised semantic segmentation datasets, COCO-Stuff, and Potsdam. ", "id2": "92", "id3": "None"}
{"id": "94", "content": "We introduce $ textit InExtremIS $, a weakly supervised 3D approach to traina deep image segmentation network using particularly weak train-timeannotations: only 6 extreme clicks at the boundary of the objects of interest.Our fully-automatic method is trained end-to-end and does not require anytest-time annotations. From the extreme points, 3D bounding boxes are extractedaround objects of interest. Then, deep geodesics connecting extreme points aregenerated to increase the amount of annotated voxels within the boundingboxes. Finally, a weakly supervised regularised loss derived from a ConditionalRandom Field formulation is used to encourage prediction consistency overhomogeneous regions. Extensive experiments are performed on a large opendataset for Vestibular Schwannoma segmentation. $ textit InExtremIS $ obtainedcompetitive performance, approaching full supervision and outperformingsignificantly other weakly supervised techniques based on bounding boxes.Moreover, given a fixed annotation time budget, $ textit InExtremIS $outperforms full supervision. Our code and data are available online. ", "id2": "93", "id3": "None"}
{"id": "95", "content": "Tensor networks provide an efficient approximation of operations involvinghigh dimensional tensors and have been extensively used in modelling quantummany-body systems. More recently, supervised learning has been attempted withtensor networks, primarily focused on tasks such as image classification. Inthis work, we propose a novel formulation of tensor networks for supervisedimage segmentation which allows them to operate on high resolution medicalimages. We use the matrix product state (MPS) tensor network on non-overlappingpatches of a given input image to predict the segmentation mask by learning apixel-wise linear classification rule in a high dimensional space. The proposedmodel is end-to-end trainable using backpropagation. It is implemented as aStrided Tensor Network to reduce the parameter complexity. The performance ofthe proposed method is evaluated on two public medical imaging datasets andcompared to relevant baselines. The evaluation shows that the strided tensornetwork yields competitive performance compared to CNN-based models while usingfewer resources. Additionally, based on the experiments we discuss thefeasibility of using fully linear models for segmentation tasks. ", "id2": "94", "id3": "None"}
{"id2": 1012, "id3": "94", "content": "Tensor networks provide an efficient approximation of operations involvinghigh dimensional tensors and have been extensively used in modelling quantummany-body systems. More recently, supervised learning has been attempted withtensor networks, primarily focused on tasks such as image classification. Inthis work, we propose a novel formulation of tensor networks for supervisedimage segmentation which allows them to operate on high resolution medicalimages. We use the matrix product state (MPS) tensor network on non-overlappingpatches of a given input image to predict the segmentation mask by learning apixel-wise linear classification rule in a high dimensional space. The proposedmodel is end-to-end trainable using backpropagation. It is implemented as aStrided Tensor Network to reduce the parameter complexity. The performance ofthe proposed method is evaluated on two public medical imaging datasets andcompared to relevant baselines. The evaluation shows that the strided tensornetwork yields competitive performance compared to CNN-based models while usingfewer resources. Additionally, based on the experiments we discuss thefeasibility of using fully linear models for segmentation tasks."}
{"id": "96", "content": "Most existing deep learning-based frameworks for image segmentation assumethat a unique ground truth is known and can be used for performance evaluation.This is true for many applications, but not all. Myocardial segmentation ofMyocardial Contrast Echocardiography (MCE), a critical task in automaticmyocardial perfusion analysis, is an example. Due to the low resolution andserious artifacts in MCE data, annotations from different cardiologists canvary significantly, and it is hard to tell which one is the best. In this case,how can we find a good way to evaluate segmentation performance and how do wetrain the neural network? In this paper, we address the first problem byproposing a new extended Dice to effectively evaluate the segmentationperformance when multiple accepted ground truth is available. Then based on ourproposed metric, we solve the second problem by further incorporating the newmetric into a loss function that enables neural networks to flexibly learngeneral features of myocardium. Experiment results on our clinical MCE data setdemonstrate that the neural network trained with the proposed loss functionoutperforms those existing ones that try to obtain a unique ground truth frommultiple annotations, both quantitatively and qualitatively. Finally, ourgrading study shows that using extended Dice as an evaluation metric can betteridentify segmentation results that need manual correction compared with usingDice. ", "id2": "95", "id3": "None"}
{"id": "97", "content": "We study image segmentation from an information-theoretic perspective,proposing a novel adversarial method that performs unsupervised segmentation bypartitioning images into maximally independent sets. More specifically, wegroup image pixels into foreground and background, with the goal of minimizingpredictability of one set from the other. An easily computed loss drives agreedy search process to maximize inpainting error over these partitions. Ourmethod does not involve training deep networks, is computationally cheap,class-agnostic, and even applicable in isolation to a single unlabeled image.Experiments demonstrate that it achieves a new state-of-the-art in unsupervisedsegmentation quality, while being substantially faster and more general thancompeting approaches. ", "id2": "96", "id3": "None"}
{"id": "98", "content": "This paper proposes two important contributions for conditional GenerativeAdversarial Networks (cGANs) to improve the wide variety of applications thatexploit this architecture. The first main contribution is an analysis of cGANsto show that they are not explicitly conditional. In particular, it will beshown that the discriminator and subsequently the cGAN does not automaticallylearn the conditionality between inputs. The second contribution is a newmethod, called acontrario, that explicitly models conditionality for both partsof the adversarial architecture via a novel acontrario loss that involvestraining the discriminator to learn unconditional (adverse) examples. Thisleads to a novel type of data augmentation approach for GANs (acontrariolearning) which allows to restrict the search space of the generator toconditional outputs using adverse examples. Extensive experimentation iscarried out to evaluate the conditionality of the discriminator by proposing aprobability distribution analysis. Comparisons with the cGAN architecture fordifferent applications show significant improvements in performance on wellknown datasets including, semantic image synthesis, image segmentation andmonocular depth prediction using different metrics including Fr echetInception Distance(FID), mean Intersection over Union (mIoU), Root Mean SquareError log (RMSE log) and Number of statistically-Different Bins (NDB) ", "id2": "97", "id3": "None"}
{"id": "99", "content": "Semantic, instance, and panoptic segmentations have been addressed usingdifferent and specialized frameworks despite their underlying connections. Thispaper presents a unified, simple, and effective framework for these essentiallysimilar tasks. The framework, named K-Net, segments both instances and semanticcategories consistently by a group of learnable kernels, where each kernel isresponsible for generating a mask for either a potential instance or a stuffclass. To remedy the difficulties of distinguishing various instances, wepropose a kernel update strategy that enables each kernel dynamic andconditional on its meaningful group in the input image. K-Net can be trained inan end-to-end manner with bipartite matching, and its training and inferenceare naturally NMS-free and box-free. Without bells and whistles, K-Netsurpasses all previous state-of-the-art single-model results of panopticsegmentation on MS COCO and semantic segmentation on ADE20K with 52.1% PQ and54.3% mIoU, respectively. Its instance segmentation performance is also on parwith Cascade Mask R-CNNon MS COCO with 60%-90% faster inference speeds. Codeand models will be released at https://github.com/open-mmlab/mmdetection. ", "id2": "98", "id3": "None"}
{"id": "100", "content": "Deep learning has proven to be a highly effective problem-solving tool forobject detection and image segmentation across various domains such ashealthcare and autonomous driving. At the heart of this performance lies neuralarchitecture design which relies heavily on domain knowledge and priorexperience on the researchers behalf. More recently, this process of findingthe most optimal architectures, given an initial search space of possibleoperations, was automated by Neural Architecture Search (NAS). In this paper,we evaluate the robustness of one such algorithm known as Efficient NAS (ENAS)against data agnostic poisoning attacks on the original search space withcarefully designed ineffective operations. By evaluating algorithm performanceon the CIFAR-10 dataset, we empirically demonstrate how our novel search spacepoisoning (SSP) approach and multiple-instance poisoning attacks exploit designflaws in the ENAS controller to result in inflated prediction error rates forchild networks. Our results provide insights into the challenges to surmount inusing NAS for more adversarially robust architecture search. ", "id2": "99", "id3": "None"}
{"id": "101", "content": "The recent vision transformer(i.e.for image classification) learns non-localattentive interaction of different patch tokens. However, prior arts misslearning the cross-scale dependencies of different pixels, the semanticcorrespondence of different labels, and the consistency of the featurerepresentations and semantic embeddings, which are critical for biomedicalsegmentation. In this paper, we tackle the above issues by proposing a unifiedtransformer network, termed Multi-Compound Transformer (MCTrans), whichincorporates rich feature learning and semantic structure mining into a unifiedframework. Specifically, MCTrans embeds the multi-scale convolutional featuresas a sequence of tokens and performs intra- and inter-scale self-attention,rather than single-scale attention in previous works. In addition, a learnableproxy embedding is also introduced to model semantic relationship and featureenhancement by using self-attention and cross-attention, respectively. MCTranscan be easily plugged into a UNet-like network and attains a significantimprovement over the state-of-the-art methods in biomedical image segmentationin six standard benchmarks. For example, MCTrans outperforms UNet by 3.64%,3.71%, 4.34%, 2.8%, 1.88%, 1.57% in Pannuke, CVC-Clinic, CVC-Colon, Etis,Kavirs, ISIC2018 dataset, respectively. Code is available athttps://github.com/JiYuanFeng/MCTrans. ", "id2": "100", "id3": "None"}
{"id": "102", "content": "Transformer, which can benefit from global (long-range) information modelingusing self-attention mechanisms, has been successful in natural languageprocessing and 2D image classification recently. However, both local and globalfeatures are crucial for dense prediction tasks, especially for 3D medicalimage segmentation. In this paper, we for the first time exploit Transformer in3D CNN for MRI Brain Tumor Segmentation and propose a novel network namedTransBTS based on the encoder-decoder structure. To capture the local 3Dcontext information, the encoder first utilizes 3D CNN to extract thevolumetric spatial feature maps. Meanwhile, the feature maps are reformedelaborately for tokens that are fed into Transformer for global featuremodeling. The decoder leverages the features embedded by Transformer andperforms progressive upsampling to predict the detailed segmentation map.Extensive experimental results on both BraTS 2019 and 2020 datasets show thatTransBTS achieves comparable or higher results than previous state-of-the-art3D methods for brain tumor segmentation on 3D MRI scans. The source code isavailable at https://github.com/Wenxuan-1119/TransBTS ", "id2": "101", "id3": "None"}
{"id": "103", "content": "Fully convolutional U-shaped neural networks have largely been the dominantapproach for pixel-wise image segmentation. In this work, we tackle two defectsthat hinder their deployment in real-world applications: 1) Predictions lackuncertainty quantification that may be crucial to many decision-making systems;2) Large memory storage and computational consumption demanding extensivehardware resources. To address these issues and improve their practicality wedemonstrate a few-parameter compact Bayesian convolutional architecture, thatachieves a marginal improvement in accuracy in comparison to related work usingsignificantly fewer parameters and compute operations. The architecturecombines parameter-efficient operations such as separable convolutions,bilinear interpolation, multi-scale feature propagation and Bayesian inferencefor per-pixel uncertainty quantification through Monte Carlo Dropout. The bestperforming configurations required fewer than 2.5 million parameters on diversechallenging datasets with few observations. ", "id2": "102", "id3": "None"}
{"id": "104", "content": "In this work, we present a simple yet effective framework to address thedomain translation problem between different sensor modalities with unique dataformats. By relying only on the semantics of the scene, our modular generativeframework can, for the first time, synthesize a panoramic color image from agiven full 3D LiDAR point cloud. The framework starts with semanticsegmentation of the point cloud, which is initially projected onto a sphericalsurface. The same semantic segmentation is applied to the corresponding cameraimage. Next, our new conditional generative model adversarially learns totranslate the predicted LiDAR segment maps to the camera image counterparts.Finally, generated image segments are processed to render the panoramic sceneimages. We provide a thorough quantitative evaluation on the SemanticKittidataset and show that our proposed framework outperforms other strong baselinemodels.  Our source code is available athttps://github.com/halmstad-University/TITAN-NET ", "id2": "103", "id3": "None"}
{"id": "105", "content": "High-resolution image segmentation remains challenging and error-prone due tothe enormous size of intermediate feature maps. Conventional methods avoid thisproblem by using patch based approaches where each patch is segmentedindependently. However, independent patch segmentation induces errors,particularly at the patch boundary due to the lack of contextual information invery high-resolution images where the patch size is much smaller compared tothe full image. To overcome these limitations, in this paper, we propose anovel framework to segment a particular patch by incorporating contextualinformation from its neighboring patches. This allows the segmentation networkto see the target patch with a wider field of view without the need of largerfeature maps. Comparative analysis from a number of experiments shows that ourproposed framework is able to segment high resolution images with significantlyimproved mean Intersection over Union and overall accuracy. ", "id2": "104", "id3": "None"}
{"id2": 1013, "id3": "104", "content": "High-resolution image segmentation remains challenging and error-prone due tothe enormous size of intermediate feature maps. Conventional methods avoid thisproblem by using patch based approaches where each patch is segmentedindependently. However, independent patch segmentation induces errors,particularly at the patch boundary due to the lack of contextual information invery high-resolution images where the patch size is much smaller compared tothe full image. To overcome these limitations, in this paper, we propose anovel framework to segment a particular patch by incorporating contextualinformation from its neighboring patches. This allows the segmentation networkto see the target patch with a wider field of view without the need of largerfeature maps. Comparative analysis from a number of experiments shows that ourproposed framework is able to segment high resolution images with significantlyimproved mean Intersection over Union and overall accuracy."}
{"id": "106", "content": "The scarcity of labeled data often impedes the application of deep learningto the segmentation of medical images. Semi-supervised learning seeks toovercome this limitation by exploiting unlabeled examples in the learningprocess. In this paper, we present a novel semi-supervised segmentation methodthat leverages mutual information (MI) on categorical distributions to achieveboth global representation invariance and local smoothness. In this method, wemaximize the MI for intermediate feature embeddings that are taken from boththe encoder and decoder of a segmentation network. We first propose a global MIloss constraining the encoder to learn an image representation that isinvariant to geometric transformations. Instead of resorting tocomputationally-expensive techniques for estimating the MI on continuousfeature embeddings, we use projection heads to map them to a discrete clusterassignment where MI can be computed efficiently. Our method also includes alocal MI loss to promote spatial consistency in the feature maps of the decoderand provide a smoother segmentation. Since mutual information does not requirea strict ordering of clusters in two different assignments, we incorporate afinal consistency regularization loss on the output which helps align thecluster labels throughout the network. We evaluate the method on fourchallenging publicly-available datasets for medical image segmentation.Experimental results show our method to outperform recently-proposed approachesfor semi-supervised segmentation and provide an accuracy near to fullsupervision while training with very few annotated images. ", "id2": "105", "id3": "None"}
{"id": "107", "content": "Unsupervised domain adaptation (UDA) aims to transfer knowledge learned froma labeled source domain to an unlabeled and unseen target domain, which isusually trained on data from both domains. Access to the source domain data atthe adaptation stage, however, is often limited, due to data storage or privacyissues. To alleviate this, in this work, we target source free UDA forsegmentation, and propose to adapt an off-the-shelf segmentation modelpre-trained in the source domain to the target domain, with an adaptivebatch-wise normalization statistics adaptation framework. Specifically, thedomain-specific low-order batch statistics, i.e., mean and variance, aregradually adapted with an exponential momentum decay scheme, while theconsistency of domain shareable high-order batch statistics, i.e., scaling andshifting parameters, is explicitly enforced by our optimization objective. Thetransferability of each channel is adaptively measured first from which tobalance the contribution of each channel. Moreover, the proposed source freeUDA framework is orthogonal to unsupervised learning methods, e.g.,self-entropy minimization, which can thus be simply added on top of ourframework. Extensive experiments on the BraTS 2018 database show that oursource free UDA framework outperformed existing source-relaxed UDA methods forthe cross-subtype UDA segmentation task and yielded comparable results for thecross-modality UDA segmentation task, compared with a supervised UDA methodswith the source data. ", "id2": "106", "id3": "None"}
{"id": "108", "content": "Semantic segmentation from very fine resolution (VFR) urban scene imagesplays a significant role in several application scenarios including autonomousdriving, land cover classification, and urban planning, etc. However, thetremendous details contained in the VFR image severely limit the potential ofthe existing deep learning approaches. More seriously, the considerablevariations in scale and appearance of objects further deteriorate therepresentational capacity of those se-mantic segmentation methods, leading tothe confusion of adjacent objects. Addressing such is-sues represents apromising research field in the remote sensing community, which paves the wayfor scene-level landscape pattern analysis and decision making. In thismanuscript, we pro-pose a bilateral awareness network (BANet) which contains adependency path and a texture path to fully capture the long-rangerelationships and fine-grained details in VFR images. Specif-ically, thedependency path is conducted based on the ResT, a novel Transformer backbonewith memory-efficient multi-head self-attention, while the texture path isbuilt on the stacked convo-lution operation. Besides, using the linearattention mechanism, a feature aggregation module (FAM) is designed toeffectively fuse the dependency features and texture features. Extensiveexperiments conducted on the three large-scale urban scene image segmentationdatasets, i.e., ISPRS Vaihingen dataset, ISPRS Potsdam dataset, and UAViddataset, demonstrate the effective-ness of our BANet. Specifically, a 64.6%mIoU is achieved on the UAVid dataset. ", "id2": "107", "id3": "None"}
{"id": "109", "content": "Transmission electron microscopy (TEM) is one of the primary tools to showmicrostructural characterization of materials as well as film thickness.However, manual determination of film thickness from TEM images istime-consuming as well as subjective, especially when the films in question arevery thin and the need for measurement precision is very high. Such is the casefor head overcoat (HOC) thickness measurements in the magnetic hard disk driveindustry. It is therefore necessary to develop software to automaticallymeasure HOC thickness. In this paper, for the first time, we propose a HOClayer segmentation method using NASNet-Large as an encoder and then followed bya decoder architecture, which is one of the most commonly used architectures indeep learning for image segmentation. To further improve segmentation results,we are the first to propose a post-processing layer to remove irrelevantportions in the segmentation result. To measure the thickness of the segmentedHOC layer, we propose a regressive convolutional neural network (RCNN) model aswell as orthogonal thickness calculation methods. Experimental resultsdemonstrate a higher dice score for our model which has lower mean squarederror and outperforms current state-of-the-art manual measurement. ", "id2": "108", "id3": "None"}
{"id": "110", "content": "Despite the success of deep learning methods in medical image segmentationtasks, the human-level performance relies on massive training data withhigh-quality annotations, which are expensive and time-consuming to collect.The fact is that there exist low-quality annotations with label noise, whichleads to suboptimal performance of learned models. Two prominent directions forsegmentation learning with noisy labels include pixel-wise noise robusttraining and image-level noise robust training. In this work, we propose anovel framework to address segmenting with noisy labels by distilling effectivesupervision information from both pixel and image levels. In particular, weexplicitly estimate the uncertainty of every pixel as pixel-wise noiseestimation, and propose pixel-wise robust learning by using both the originallabels and pseudo labels. Furthermore, we present an image-level robustlearning method to accommodate more information as the complements topixel-level learning. We conduct extensive experiments on both simulated andreal-world noisy datasets. The results demonstrate the advantageous performanceof our method compared to state-of-the-art baselines for medical imagesegmentation with noisy labels. ", "id2": "109", "id3": "None"}
{"id": "111", "content": "Learning multi-modal representations is an essential step towards real-worldrobotic applications, and various multi-modal fusion models have been developedfor this purpose. However, we observe that existing models, whose objectivesare mostly based on joint training, often suffer from learning inferiorrepresentations of each modality. We name this problem Modality Failure, andhypothesize that the imbalance of modalities and the implicit bias of commonobjectives in fusion method prevent encoders of each modality from sufficientfeature learning. To this end, we propose a new multi-modal learning method,Uni-Modal Teacher, which combines the fusion objective and uni-modaldistillation to tackle the modality failure problem. We show that our methodnot only drastically improves the representation of each modality, but alsoimproves the overall multi-modal task performance. Our method can beeffectively generalized to most multi-modal fusion approaches. We achieve morethan 3% improvement on the VGGSound audio-visual classification task, as wellas improving performance on the NYU depth V2 RGB-D image segmentation task. ", "id2": "110", "id3": "None"}
{"id2": 1014, "id3": "110", "content": "Learning multi-modal representations is an essential step towards real-worldrobotic applications, and various multi-modal fusion models have been developedfor this purpose. However, we observe that existing models, whose objectivesare mostly based on joint training, often suffer from learning inferiorrepresentations of each modality. We name this problem Modality Failure, andhypothesize that the imbalance of modalities and the implicit bias of commonobjectives in fusion method prevent encoders of each modality from sufficientfeature learning. To this end, we propose a new multi-modal learning method,Uni-Modal Teacher, which combines the fusion objective and uni-modaldistillation to tackle the modality failure problem. We show that our methodnot only drastically improves the representation of each modality, but alsoimproves the overall multi-modal task performance. Our method can beeffectively generalized to most multi-modal fusion approaches. We achieve morethan 3% improvement on the VGGSound audio-visual classification task, as wellas improving performance on the NYU depth V2 RGB-D image segmentation task."}
{"id": "112", "content": "Many approaches to 3D image segmentation are based on hierarchical clusteringof supervoxels into image regions. Here we describe a distributed algorithmcapable of handling a tremendous number of supervoxels. The algorithm worksrecursively, the regions are divided into chunks that are processedindependently in parallel by multiple workers. At each round of the recursiveprocedure, the chunk size in all dimensions are doubled until a single chunkencompasses the entire image. The final result is provably independent of thechunking scheme, and the same as if the entire image were processed withoutdivision into chunks. This is nontrivial because a pair of adjacent regions isscored by some statistical property (e.g. mean or median) of the affinities atthe interface, and the interface may extend over arbitrarily many chunks. Thetrick is to delay merge decisions for regions that touch chunk boundaries, andonly complete them in a later round after the regions are fully containedwithin a chunk. We demonstrate the algorithm by clustering an affinity graphwith over 1.5 trillion edges between 135 billion supervoxels derived from a 3Delectron microscopic brain image. ", "id2": "111", "id3": "None"}
{"id": "113", "content": "Vision Transformers (ViT) have been shown to attain highly competitiveperformance for a wide range of vision applications, such as imageclassification, object detection and semantic image segmentation. In comparisonto convolutional neural networks, the Vision Transformers weaker inductivebias is generally found to cause an increased reliance on model regularizationor data augmentation (AugReg for short) when training on smaller trainingdatasets. We conduct a systematic empirical study in order to better understandthe interplay between the amount of training data, AugReg, model size andcompute budget. As one result of this study we find that the combination ofincreased compute and AugReg can yield models with the same performance asmodels trained on an order of magnitude more training data: we train ViT modelsof various sizes on the public ImageNet-21k dataset which either match oroutperform their counterparts trained on the larger, but not publicly availableJFT-300M dataset. ", "id2": "112", "id3": "None"}
{"id": "114", "content": "The success of deep learning heavily depends on the availability of largelabeled training sets. However, it is hard to get large labeled datasets inmedical image domain because of the strict privacy concern and costly labelingefforts. Contrastive learning, an unsupervised learning technique, has beenproved powerful in learning image-level representations from unlabeled data.The learned encoder can then be transferred or fine-tuned to improve theperformance of downstream tasks with limited labels. A critical step incontrastive learning is the generation of contrastive data pairs, which isrelatively simple for natural image classification but quite challenging formedical image segmentation due to the existence of the same tissue or organacross the dataset. As a result, when applied to medical image segmentation,most state-of-the-art contrastive learning frameworks inevitably introduce alot of false-negative pairs and result in degraded segmentation quality. Toaddress this issue, we propose a novel positional contrastive learning (PCL)framework to generate contrastive data pairs by leveraging the positioninformation in volumetric medical images. Experimental results on CT and MRIdatasets demonstrate that the proposed PCL method can substantially improve thesegmentation performance compared to existing methods in both semi-supervisedsetting and transfer learning setting. ", "id2": "113", "id3": "None"}
{"id": "115", "content": "We present a novel approach that combines machine learning based interactiveimage segmentation using supervoxels with a clustering method for the automatedidentification of similarly colored images in large data sets which enables aguided reuse of classifiers. Our approach solves the problem of significantcolor variability prevalent and often unavoidable in biological and medicalimages which typically leads to deteriorated segmentation and quantificationaccuracy thereby greatly reducing the necessary training effort. This increasein efficiency facilitates the quantification of much larger numbers of imagesthereby enabling interactive image analysis for recent new technologicaladvances in high-throughput imaging. The presented methods are applicable foralmost any image type and represent a useful tool for image analysis tasks ingeneral. ", "id2": "114", "id3": "None"}
{"id": "116", "content": "The joint use of multiple imaging modalities for medical image segmentationhas been widely studied in recent years. The fusion of information fromdifferent modalities has demonstrated to improve the segmentation accuracy,with respect to mono-modal segmentations, in several applications. However,acquiring multiple modalities is usually not possible in a clinical setting dueto a limited number of physicians and scanners, and to limit costs and scantime. Most of the time, only one modality is acquired. In this paper, wepropose KD-Net, a framework to transfer knowledge from a trained multi-modalnetwork (teacher) to a mono-modal one (student). The proposed method is anadaptation of the generalized distillation framework where the student networkis trained on a subset (1 modality) of the teachers inputs (n modalities). Weillustrate the effectiveness of the proposed framework in brain tumorsegmentation with the BraTS 2018 dataset. Using different architectures, weshow that the student network effectively learns from the teacher and alwaysoutperforms the baseline mono-modal network in terms of segmentation accuracy. ", "id2": "115", "id3": "None"}
{"id": "117", "content": "Accurate segmentation of medical images into anatomically meaningful regionsis critical for the extraction of quantitative indices or biomarkers. Thecommon pipeline for segmentation comprises regions of interest detection stageand segmentation stage, which are independent of each other and typicallyperformed using separate deep learning networks. The performance of thesegmentation stage highly relies on the extracted set of spatial features andthe receptive fields. In this work, we propose an end-to-end network, calledTrilateral Attention Network (TaNet), for real-time detection and segmentationin medical images. TaNet has a module for region localization, and threesegmentation pathways: 1) handcrafted pathway with hand-designed convolutionalkernels, 2) detail pathway with regular convolutional kernels, and 3) a globalpathway to enlarge the receptive field. The first two pathways encode richhandcrafted and low-level features extracted by hand-designed and regularkernels while the global pathway encodes high-level context information. Byjointly training the network for localization and segmentation using differentsets of features, TaNet achieved superior performance, in terms of accuracy andspeed, when evaluated on an echocardiography dataset for cardiac segmentation.The code and models will be made publicly available in TaNet Github page. ", "id2": "116", "id3": "None"}
{"id": "118", "content": "In this work, we address the task of referring image segmentation (RIS),which aims at predicting a segmentation mask for the object described by anatural language expression. Most existing methods focus on establishingunidirectional or directional relationships between visual and linguisticfeatures to associate two modalities together, while the multi-scale context isignored or insufficiently modeled. Multi-scale context is crucial to localizeand segment those objects that have large scale variations during themulti-modal fusion process. To solve this problem, we propose a simple yeteffective Cascaded Multi-modal Fusion (CMF) module, which stacks multipleatrous convolutional layers in parallel and further introduces a cascadedbranch to fuse visual and linguistic features. The cascaded branch canprogressively integrate multi-scale contextual information and facilitate thealignment of two modalities during the multi-modal fusion process. Experimentalresults on four benchmark datasets demonstrate that our method outperforms moststate-of-the-art methods. Code is available athttps://github.com/jianhua2022/CMF-Refseg. ", "id2": "117", "id3": "None"}
{"id": "119", "content": "This paper addresses the domain shift problem for segmentation. As asolution, we propose OLVA, a novel and lightweight unsupervised domainadaptation method based on a Variational Auto-Encoder (VAE) and OptimalTransport (OT) theory. Thanks to the VAE, our model learns a sharedcross-domain latent space that follows a normal distribution, which reduces thedomain shift. To guarantee valid segmentations, our shared latent space isdesigned to model the shape rather than the intensity variations. We furtherrely on an OT loss to match and align the remaining discrepancy between the twodomains in the latent space. We demonstrate OLVAs effectiveness for thesegmentation of multiple cardiac structures on the public Multi-Modality WholeHeart Segmentation (MM-WHS) dataset, where the source domain consists ofannotated 3D MR images and the unlabelled target domain of 3D CTs. Our resultsshow remarkable improvements with an additional margin of 12.5 % dice scoreover concurrent generative training approaches. ", "id2": "118", "id3": "None"}
{"id": "120", "content": "mmWave radars offer excellent depth resolution owing to their high bandwidthat mmWave radio frequencies. Yet, they suffer intrinsically from poor angularresolution, that is an order-of-magnitude worse than camera systems, and aretherefore not a capable 3-D imaging solution in isolation. We proposeMetamoran, a system that combines the complimentary strengths of radar andcamera systems to obtain depth images at high azimuthal resolutions atdistances of several tens of meters with high accuracy, all from a single fixedvantage point. Metamoran enables rich long-range depth imaging outdoors withapplications to roadside safety infrastructure, surveillance and wide-areamapping. Our key insight is to use the high azimuth resolution from camerasusing computer vision techniques, including image segmentation and monoculardepth estimation, to obtain object shapes and use these as priors for our novelspecular beamforming algorithm. We also design this algorithm to work incluttered environments with weak reflections and in partially occludedscenarios. We perform a detailed evaluation of Metamorans depth imaging andsensing capabilities in 200 diverse scenes at a major U.S. city. Our evaluationshows that Metamoran estimates the depth of an object up to 60~m away with amedian error of 28~cm, an improvement of 13$ times$ compared to a naiveradar+camera baseline and 23$ times$ compared to monocular depth estimation. ", "id2": "119", "id3": "None"}
{"id": "121", "content": "This paper reports a CPU-level real-time stereo matching method for surgicalimages (10 Hz on 640 * 480 image with a single core of i5-9400). The proposedmethod is built on the fast dense inverse searching algorithm, whichestimates the disparity of the stereo images. The overlapping image patches(arbitrary squared image segment) from the images at different scales arealigned based on the photometric consistency presumption. We propose a Bayesianframework to evaluate the probability of the optimized patch disparity atdifferent scales. Moreover, we introduce a spatial Gaussian mixed probabilitydistribution to address the pixel-wise probability within the patch. In-vivoand synthetic experiments show that our method can handle ambiguities resultedfrom the textureless surfaces and the photometric inconsistency caused by theLambertian reflectance. Our Bayesian method correctly balances the probabilityof the patch for stereo images at different scales. Experiments indicate thatthe estimated depth has higher accuracy and fewer outliers than the baselinemethods in the surgical scenario. ", "id2": "120", "id3": "None"}
{"id": "122", "content": "We introduce a new method for generating color images from sketches or edgemaps. Current methods either require some form of additional user-guidance orare limited to the paired translation approach. We argue that segmentationinformation could provide valuable guidance for sketch colorization. To thisend, we propose to leverage semantic image segmentation, as provided by ageneral purpose panoptic segmentation network, to create an additionaladversarial loss function. Our loss function can be integrated to any baselineGAN model. Our method is not limited to datasets that contain segmentationlabels, and it can be trained for unpaired translation tasks. We show theeffectiveness of our method on four different datasets spanning scene levelindoor, outdoor, and children book illustration images using qualitative,quantitative and user study analysis. Our model improves its baseline up to 35points on the FID metric. Our code and pretrained models can be found athttps://github.com/giddyyupp/AdvSegLoss. ", "id2": "121", "id3": "None"}
{"id": "123", "content": "Automatic medical image segmentation has made great progress benefit from thedevelopment of deep learning. However, most existing methods are based onconvolutional neural networks (CNNs), which fail to build long-rangedependencies and global context connections due to the limitation of receptivefield in convolution operation. Inspired by the success of Transformer inmodeling the long-range contextual information, some researchers have expendedconsiderable efforts in designing the robust variants of Transformer-basedU-Net. Moreover, the patch division used in vision transformers usually ignoresthe pixel-level intrinsic structural features inside each patch. To alleviatethese problems, we propose a novel deep medical image segmentation frameworkcalled Dual Swin Transformer U-Net (DS-TransUNet), which might be the firstattempt to concurrently incorporate the advantages of hierarchical SwinTransformer into both encoder and decoder of the standard U-shaped architectureto enhance the semantic segmentation quality of varying medical images. Unlikemany prior Transformer-based solutions, the proposed DS-TransUNet first adoptsdual-scale encoder subnetworks based on Swin Transformer to extract the coarseand fine-grained feature representations of different semantic scales. As thecore component for our DS-TransUNet, a well-designed Transformer InteractiveFusion (TIF) module is proposed to effectively establish global dependenciesbetween features of different scales through the self-attention mechanism.Furthermore, we also introduce the Swin Transformer block into decoder tofurther explore the long-range contextual information during the up-samplingprocess. Extensive experiments across four typical tasks for medical imagesegmentation demonstrate the effectiveness of DS-TransUNet, and show that ourapproach significantly outperforms the state-of-the-art methods. ", "id2": "122", "id3": "None"}
{"id": "124", "content": "Deep learning has demonstrated significant improvements in medical imagesegmentation using a sufficiently large amount of training data with manuallabels. Acquiring well-representative labels requires expert knowledge andexhaustive labors. In this paper, we aim to boost the performance ofsemi-supervised learning for medical image segmentation with limited labelsusing a self-ensembling contrastive learning technique. To this end, we proposeto train an encoder-decoder network at image-level with small amounts oflabeled images, and more importantly, we learn latent representations directlyat feature-level by imposing contrastive loss on unlabeled images. This methodstrengthens intra-class compactness and inter-class separability, so as to geta better pixel classifier. Moreover, we devise a student encoder for onlinelearning and an exponential moving average version of it, called teacherencoder, to improve the performance iteratively in a self-ensembling manner. Toconstruct contrastive samples with unlabeled images, two sampling strategiesthat exploit structure similarity across medical images and utilizepseudo-labels for construction, termed region-aware and anatomical-awarecontrastive sampling, are investigated. We conduct extensive experiments on anMRI and a CT segmentation dataset and demonstrate that in a limited labelsetting, the proposed method achieves state-of-the-art performance. Moreover,the anatomical-aware strategy that prepares contrastive samples on-the-flyusing pseudo-labels realizes better contrastive regularization on featurerepresentations. ", "id2": "123", "id3": "None"}
{"id": "125", "content": "The segmentation of nanoscale electron microscopy (EM) images is crucial butchallenging in connectomics. Recent advances in deep learning have demonstratedthe significant potential of automatic segmentation for tera-scale EM images.However, none of the existing segmentation methods are error-free, and theyrequire proofreading, which is typically implemented as an interactive,semi-automatic process via manual intervention. Herein, we propose a fullyautomatic proofreading method based on reinforcement learning. The main idea isto model the human decision process in proofreading using a reinforcement agentto achieve fully automatic proofreading. We systematically design the proposedsystem by combining multiple reinforcement learning agents in a hierarchicalmanner, where each agent focuses only on a specific task while preservingdependency between agents. Furthermore, we also demonstrate that the episodictask setting of reinforcement learning can efficiently manage a combination ofmerge and split errors concurrently presented in the input. We demonstrate theefficacy of the proposed system by comparing it with state-of-the-artproofreading methods using various testing examples. ", "id2": "124", "id3": "None"}
{"id": "126", "content": "Medical image segmentation is one of the important tasks of computer-aideddiagnosis in medical image analysis. Since most medical images have thecharacteristics of blurred boundaries and uneven intensity distribution,through existing segmentation methods, the discontinuity within the target areaand the discontinuity of the target boundary are likely to lead to rough oreven erroneous boundary delineation. In this paper, we propose a new iterativerefined interactive segmentation method for medical images based on agentreinforcement learning, which focuses on the problem of target segmentationboundaries. We model the dynamic process of drawing the target contour in acertain order as a Markov Decision Process (MDP) based on a deep reinforcementlearning method. In the dynamic process of continuous interaction between theagent and the image, the agent tracks the boundary point by point in orderwithin a limited length range until the contour of the target is completelydrawn. In this process, the agent can quickly improve the segmentationperformance by exploring an interactive policy in the image. The method weproposed is simple and effective. At the same time, we evaluate our method onthe cardiac MRI scan data set. Experimental results show that our method has abetter segmentation effect on the left ventricle in a small number of medicalimage data sets, especially in terms of segmentation boundaries, this method isbetter than existing methods. Based on our proposed method, the dynamicgeneration process of the predicted contour trajectory of the left ventriclewill be displayed online at https://github.com/H1997ym/LV-contour-trajectory. ", "id2": "125", "id3": "None"}
{"id": "127", "content": "This paper addresses fast semantic segmentation on video.Video segmentationoften calls for real-time, or even fasterthan real-time, processing. One commonrecipe for conserving computation arising from feature extraction is topropagate features of few selected keyframes. However, recent advances in fastimage segmentation make these solutions less attractive. To leverage fast imagesegmentation for furthering video segmentation, we propose a simple yetefficient propagation framework. Specifically, we perform lightweight flowestimation in 1/8-downscaled image space for temporal warping in segmentationoutpace space. Moreover, we introduce a guided spatially-varying convolutionfor fusing segmentations derived from the previous and current frames, tomitigate propagation error and enable lightweight feature extraction onnon-keyframes. Experimental results on Cityscapes and CamVid show that ourscheme achieves the state-of-the-art accuracy-throughput trade-off on videosegmentation. ", "id2": "126", "id3": "None"}
{"id": "128", "content": "There are many approaches that use weak-supervision to train networks tosegment 2D images. By contrast, existing 3D approaches rely on full-supervisionof a subset of 2D slices of the 3D image volume. In this paper, we propose anapproach that is truly weakly-supervised in the sense that we only need toprovide a sparse set of 3D point on the surface of target objects, an easy taskthat can be quickly done. We use the 3D points to deform a 3D template so thatit roughly matches the target object outlines and we introduce an architecturethat exploits the supervision provided by coarse template to train a network tofind accurate boundaries.  We evaluate the performance of our approach on Computed Tomography (CT),Magnetic Resonance Imagery (MRI) and Electron Microscopy (EM) image datasets.We will show that it outperforms a more traditional approach toweak-supervision in 3D at a reduced supervision cost. ", "id2": "127", "id3": "None"}
{"id": "129", "content": "Classical supervised methods commonly used often suffer from the requirementof an abudant number of training samples and are unable to generalize on unseendatasets. As a result, the broader application of any trained model is verylimited in clinical settings. However, few-shot approaches can minimize theneed for enormous reliable ground truth labels that are both labor intensiveand expensive. To this end, we propose to exploit an optimization-basedimplicit model agnostic meta-learning  iMAML  algorithm in a few-shot settingfor medical image segmentation. Our approach can leverage the learned weightsfrom a diverse set of training samples and can be deployed on a new unseendataset. We show that unlike classical few-shot learning approaches, our methodhas improved generalization capability. To our knowledge, this is the firstwork that exploits iMAML for medical image segmentation. Our quantitativeresults on publicly available skin and polyp datasets show that the proposedmethod outperforms the naive supervised baseline model and two recent few-shotsegmentation approaches by large margins. ", "id2": "128", "id3": "None"}
{"id": "130", "content": "While cloud/sky image segmentation has extensive real-world applications, alarge amount of labelled data is needed to train a highly accurate models toperform the task. Scarcity of such volumes of cloud/sky images withcorresponding ground-truth binary maps makes it highly difficult to train suchcomplex image segmentation models. In this paper, we demonstrate theeffectiveness of using Generative Adversarial Networks (GANs) to generate datato augment the training set in order to increase the prediction accuracy ofimage segmentation model. We further present a way to estimate ground-truthbinary maps for the GAN-generated images to facilitate their effective use asaugmented images. Finally, we validate our work with different statisticaltechniques. ", "id2": "129", "id3": "None"}
{"id2": 1015, "id3": "129", "content": "While cloud/sky image segmentation has extensive real-world applications, alarge amount of labelled data is needed to train a highly accurate models toperform the task. Scarcity of such volumes of cloud/sky images withcorresponding ground-truth binary maps makes it highly difficult to train suchcomplex image segmentation models. In this paper, we demonstrate theeffectiveness of using Generative Adversarial Networks (GANs) to generate datato augment the training set in order to increase the prediction accuracy ofimage segmentation model. We further present a way to estimate ground-truthbinary maps for the GAN-generated images to facilitate their effective use asaugmented images. Finally, we validate our work with different statisticaltechniques."}
{"id": "131", "content": "Contemporary domain adaptive semantic segmentation aims to address dataannotation challenges by assuming that target domains are completelyunannotated. However, annotating a few target samples is usually verymanageable and worthwhile especially if it improves the adaptation performancesubstantially. This paper presents SSDAS, a Semi-Supervised Domain Adaptiveimage Segmentation network that employs a few labeled target samples as anchorsfor adaptive and progressive feature alignment between labeled source samplesand unlabeled target samples. We position the few labeled target samples asreferences that gauge the similarity between source and target features andguide adaptive inter-domain alignment for learning more similar sourcefeatures. In addition, we replace the dissimilar source features byhigh-confidence target features continuously during the iterative trainingprocess, which achieves progressive intra-domain alignment between confidentand unconfident target features. Extensive experiments show the proposed SSDASgreatly outperforms a number of baselines, i.e., UDA-based semanticsegmentation and SSDA-based image classification. In addition, SSDAS iscomplementary and can be easily incorporated into UDA-based methods withconsistent improvements in domain adaptive semantic segmentation. ", "id2": "130", "id3": "None"}
{"id": "132", "content": "In applied image segmentation tasks, the ability to provide numerous andprecise labels for training is paramount to the accuracy of the model atinference time. However, this overhead is often neglected, and recentlyproposed segmentation architectures rely heavily on the availability andfidelity of ground truth labels to achieve state-of-the-art accuracies. Failureto acknowledge the difficulty in creating adequate ground truths can lead to anover-reliance on pre-trained models or a lack of adoption in real-worldapplications. We introduce Points2Polygons (P2P), a model which makes use ofcontextual metric learning techniques that directly addresses this problem.Points2Polygons performs well against existing fully-supervised segmentationbaselines with limited training data, despite using lightweight segmentationmodels (U-Net with a ResNet18 backbone) and having access to only weak labelsin the form of object centroids and no pre-training. We demonstrate this onseveral different small but non-trivial datasets. We show that metric learningusing contextual data provides key insights for self-supervised tasks ingeneral, and allow segmentation models to easily generalize acrosstraditionally label-intensive domains in computer vision. ", "id2": "131", "id3": "None"}
{"id": "133", "content": "Contrastive learning has shown superior performance in embedding global andspatial invariant features in computer vision (e.g., image classification).However, its overall success of embedding local and spatial variant features isstill limited, especially for semantic segmentation. In a per-pixel predictiontask, more than one label can exist in a single image for segmentation (e.g.,an image contains both cat, dog, and grass), thereby it is difficult to definepositive or negative pairs in a canonical contrastive learning setting. Inthis paper, we propose an attention-guided supervised contrastive learningapproach to highlight a single semantic object every time as the target. Withour design, the same image can be embedded to different semantic clusters withsemantic attention (i.e., coerce semantic masks) as an additional inputchannel. To achieve such attention, a novel two-stage training strategy ispresented. We evaluate the proposed method on multi-organ medical imagesegmentation task, as our major task, with both in-house data and BTCV 2015datasets. Comparing with the supervised and semi-supervised trainingstate-of-the-art in the backbone of ResNet-50, our proposed pipeline yieldssubstantial improvement of 5.53% and 6.09% in Dice score for both medical imagesegmentation cohorts respectively. The performance of the proposed method onnatural images is assessed via PASCAL VOC 2012 dataset, and achieves 2.75%substantial improvement. ", "id2": "132", "id3": "None"}
{"id": "134", "content": "Applications such as autonomous vehicles and medical screening use deeplearning models to localize and identify hundreds of objects in a single frame.In the past, it has been shown how an attacker can fool these models by placingan adversarial patch within a scene. However, these patches must be placed inthe target location and do not explicitly alter the semantics elsewhere in theimage.  In this paper, we introduce a new type of adversarial patch which alters amodels perception of an images semantics. These patches can be placedanywhere within an image to change the classification or semantics of locationsfar from the patch. We call this new class of adversarial examples remoteadversarial patches (RAP).  We implement our own RAP called IPatch and perform an in-depth analysis onimage segmentation RAP attacks using five state-of-the-art architectures witheight different encoders on the CamVid street view dataset. Moreover, wedemonstrate that the attack can be extended to object recognition models withpreliminary results on the popular YOLOv3 model. We found that the patch canchange the classification of a remote target region with a success rate of upto 93% on average. ", "id2": "133", "id3": "None"}
{"id": "135", "content": "Simultaneous localisation and categorization of objects in medical images,also referred to as medical object detection, is of high clinical relevancebecause diagnostic decisions often depend on rating of objects rather than e.g.pixels. For this task, the cumbersome and iterative process of methodconfiguration constitutes a major research bottleneck. Recently, nnU-Net hastackled this challenge for the task of image segmentation with great success.Following nnU-Nets agenda, in this work we systematize and automate theconfiguration process for medical object detection. The resultingself-configuring method, nnDetection, adapts itself without any manualintervention to arbitrary medical detection problems while achieving results enpar with or superior to the state-of-the-art. We demonstrate the effectivenessof nnDetection on two public benchmarks, ADAM and LUNA16, and propose 10further medical object detection tasks on public data sets for comprehensivemethod evaluation. Code is at https://github.com/MIC-DKFZ/nnDetection . ", "id2": "134", "id3": "None"}
{"id": "136", "content": "Performing inference in graphs is a common task within several machinelearning problems, e.g., image segmentation, community detection, among others.For a given undirected connected graph, we tackle the statistical problem ofexactly recovering an unknown ground-truth binary labeling of the nodes from asingle corrupted observation of each edge. Such problem can be formulated as aquadratic combinatorial optimization problem over the boolean hypercube, whereit has been shown before that one can (with high probability and in polynomialtime) exactly recover the ground-truth labeling of graphs that have anisoperimetric number that grows with respect to the number of nodes (e.g.,complete graphs, regular expanders). In this work, we apply a powerfulhierarchy of relaxations, known as the sum-of-squares (SoS) hierarchy, to thecombinatorial problem. Motivated by empirical evidence on the improvement inexact recoverability, we center our attention on the degree-4 SoS relaxationand set out to understand the origin of such improvement from a graphtheoretical perspective. We show that the solution of the dual of the relaxedproblem is related to finding edge weights of the Johnson and Kneser graphs,where the weights fulfill the SoS constraints and intuitively allow the inputgraph to increase its algebraic connectivity. Finally, as byproduct of ouranalysis, we derive a novel Cheeger-type lower bound for the algebraicconnectivity of graphs with signed edge weights. ", "id2": "135", "id3": "None"}
{"id": "137", "content": "Identification of abnormalities in red blood cells (RBC) is key to diagnosinga range of medical conditions from anaemia to liver disease. Currently this isdone manually, a time-consuming and subjective process. This paper presents anautomated process utilising the advantages of machine learning to increasecapacity and standardisation of cell abnormality detection, and its performanceis analysed. Three different machine learning technologies were used: a SupportVector Machine (SVM), a classical machine learning technology; TabNet, a deeplearning architecture for tabular data; U-Net, a semantic segmentation networkdesigned for medical image segmentation. A critical issue was the highlyimbalanced nature of the dataset which impacts the efficacy of machinelearning. To address this, synthesising minority class samples in feature spacewas investigated via Synthetic Minority Over-sampling Technique (SMOTE) andcost-sensitive learning. A combination of these two methods is investigated toimprove the overall performance. These strategies were found to increasesensitivity to minority classes. The impact of unknown cells on semanticsegmentation is demonstrated, with some evidence of the model applying learningof labelled cells to these anonymous cells. These findings indicate bothclassical models and new deep learning networks as promising methods inautomating RBC abnormality detection. ", "id2": "136", "id3": "None"}
{"id": "138", "content": "Despite much recent work, detecting out-of-distribution (OOD) inputs andadversarial attacks (AA) for computer vision models remains a challenge. Inthis work, we introduce a novel technique, DAAIN, to detect OOD inputs and AAfor image segmentation in a unified setting. Our approach monitors the innerworkings of a neural network and learns a density estimator of the activationdistribution. We equip the density estimator with a classification head todiscriminate between regular and anomalous inputs. To deal with thehigh-dimensional activation-space of typical segmentation networks, wesubsample them to obtain a homogeneous spatial and layer-wise coverage. Thesubsampling pattern is chosen once per monitored model and kept fixed for allinputs. Since the attacker has access to neither the detection model nor thesampling key, it becomes harder for them to attack the segmentation network, asthe attack cannot be backpropagated through the detector. We demonstrate theeffectiveness of our approach using an ESPNet trained on the Cityscapes datasetas segmentation model, an affine Normalizing Flow as density estimator and useblue noise to ensure homogeneous sampling. Our model can be trained on a singleGPU making it compute efficient and deployable without requiring specializedaccelerators. ", "id2": "137", "id3": "None"}
{"id": "139", "content": "Currently, developments of deep learning techniques are providinginstrumental to identify, classify, and quantify patterns in medical images.Segmentation is one of the important applications in medical image analysis. Inthis regard, U-Net is the predominant approach to medical image segmentationtasks. However, we found that those U-Net based models have limitations inseveral aspects, for example, millions of parameters in the U-Net consumingconsiderable computation resource and memory, lack of global information, andmissing some tough objects. Therefore, we applied two modifications to improvethe U-Net model: 1) designed and added the dilated channel-wise CNN module, 2)simplified the U shape network. Based on these two modifications, we proposed anovel light-weight architecture -- Channel-wise Feature Pyramid Network forMedicine (CFPNet-M). To evaluate our method, we selected five datasets withdifferent modalities: thermography, electron microscopy, endoscopy, dermoscopy,and digital retinal images. And we compared its performance with several modelshaving different parameter scales. This paper also involves our previousstudies of DC-UNet and some commonly used light-weight neural networks. Weapplied the Tanimoto similarity instead of the Jaccard index for gray-levelimage measurements. By comparison, CFPNet-M achieves comparable segmentationresults on all five medical datasets with only 0.65 million parameters, whichis about 2% of U-Net, and 8.8 MB memory. Meanwhile, the inference speed canreach 80 FPS on a single RTX 2070Ti GPU with the 256 by 192 pixels input size. ", "id2": "138", "id3": "None"}
{"id2": 1016, "id3": "138", "content": "Currently, developments of deep learning techniques are providinginstrumental to identify, classify, and quantify patterns in medical images.Segmentation is one of the important applications in medical image analysis. Inthis regard, U-Net is the predominant approach to medical image segmentationtasks. However, we found that those U-Net based models have limitations inseveral aspects, for example, millions of parameters in the U-Net consumingconsiderable computation resource and memory, lack of global information, andmissing some tough objects. Therefore, we applied two modifications to improvethe U-Net model: 1) designed and added the dilated channel-wise CNN module, 2)simplified the U shape network. Based on these two modifications, we proposed anovel light-weight architecture -- Channel-wise Feature Pyramid Network forMedicine (CFPNet-M). To evaluate our method, we selected five datasets withdifferent modalities: thermography, electron microscopy, endoscopy, dermoscopy,and digital retinal images. And we compared its performance with several modelshaving different parameter scales. This paper also involves our previousstudies of DC-UNet and some commonly used light-weight neural networks. Weapplied the Tanimoto similarity instead of the Jaccard index for gray-levelimage measurements. By comparison, CFPNet-M achieves comparable segmentationresults on all five medical datasets with only 0.65 million parameters, whichis about 2% of U-Net, and 8.8 MB memory. Meanwhile, the inference speed canreach 80 FPS on a single RTX 2070Ti GPU with the 256 by 192 pixels input size."}
{"id": "140", "content": "As the most economical and routine auxiliary examination in the diagnosis ofroot canal treatment, oral X-ray has been widely used by stomatologists. It isstill challenging to segment the tooth root with a blurry boundary for thetraditional image segmentation method. To this end, we propose a model forhigh-resolution segmentation based on polynomial curve fitting with landmarkdetection (HS-PCL). It is based on detecting multiple landmarks evenlydistributed on the edge of the tooth root to fit a smooth polynomial curve asthe segmentation of the tooth root, thereby solving the problem of fuzzy edge.In our model, a maximum number of the shortest distances algorithm (MNSDA) isproposed to automatically reduce the negative influence of the wrong landmarkswhich are detected incorrectly and deviate from the tooth root on the fittingresult. Our numerical experiments demonstrate that the proposed approach notonly reduces Hausdorff95 (HD95) by 33.9% and Average Surface Distance (ASD) by42.1% compared with the state-of-the-art method, but it also achieves excellentresults on the minute quantity of datasets, which greatly improves thefeasibility of automatic root canal therapy evaluation by medical imagecomputing. ", "id2": "139", "id3": "None"}
{"id": "141", "content": "Estimating the amount of electricity that can be produced by rooftopphotovoltaic systems is a time-consuming process that requires on-sitemeasurements, a difficult task to achieve on a large scale. In this paper, wepresent an approach to estimate the solar potential of rooftops based on theirlocation and architectural characteristics, as well as the amount of solarradiation they receive annually. Our technique uses computer vision to achievesemantic segmentation of roof sections and roof objects on the one hand, and amachine learning model based on structured building features to predict roofpitch on the other hand. We then compute the azimuth and maximum number ofsolar panels that can be installed on a rooftop with geometric approaches.Finally, we compute precise shading masks and combine them with solarirradiation data that enables us to estimate the yearly solar potential of arooftop. ", "id2": "140", "id3": "None"}
{"id": "142", "content": "Quantitative bone single-photon emission computed tomography (QBSPECT) hasthe potential to provide a better quantitative assessment of bone metastasisthan planar bone scintigraphy due to its ability to better quantify activity inoverlapping structures. An important element of assessing response of bonemetastasis is accurate image segmentation. However, limited by the propertiesof QBSPECT images, the segmentation of anatomical regions-of-interests (ROIs)still relies heavily on the manual delineation by experts. This work proposes afast and robust automated segmentation method for partitioning a QBSPECT imageinto lesion, bone, and background. We present a new unsupervised segmentationloss function and its semi- and supervised variants for training aconvolutional neural network (ConvNet). The loss functions were developed basedon the objective function of the classical Fuzzy C-means (FCM) algorithm. Weconducted a comprehensive study to compare our proposed methods with ConvNetstrained using supervised loss functions and conventional clustering methods.The Dice similarity coefficient (DSC) and several other metrics were used asfigures of merit as applied to the task of delineating lesion and bone in bothsimulated and clinical SPECT/CT images. We experimentally demonstrated that theproposed methods yielded good segmentation results on a clinical dataset eventhough the training was done using realistic simulated images. A ConvNet-basedimage segmentation method that uses novel loss functions was developed andevaluated. The method can operate in unsupervised, semi-supervised, orfully-supervised modes depending on the availability of annotated trainingdata. The results demonstrated that the proposed method provides fast androbust lesion and bone segmentation for QBSPECT/CT. The method can potentiallybe applied to other medical image segmentation applications. ", "id2": "141", "id3": "None"}
{"id": "143", "content": "Generative flows and diffusion models have been predominantly trained onordinal data, for example natural images. This paper introduces two extensionsof flows and diffusion for categorical data such as language or imagesegmentation: Argmax Flows and Multinomial Diffusion. Argmax Flows are definedby a composition of a continuous distribution (such as a normalizing flow), andan argmax function. To optimize this model, we learn a probabilistic inversefor the argmax that lifts the categorical data to a continuous space.Multinomial Diffusion gradually adds categorical noise in a diffusion process,for which the generative denoising process is learned. We demonstrate that ourmethod outperforms existing dequantization approaches on text modelling andmodelling on image segmentation maps in log-likelihood. ", "id2": "142", "id3": "None"}
{"id": "144", "content": "In this paper, we show how uncertainty estimation can be leveraged to enablesafety critical image segmentation in autonomous driving, by triggering afallback behavior if a target accuracy cannot be guaranteed. We introduce a newuncertainty measure based on disagreeing predictions as measured by adissimilarity function. We propose to estimate this dissimilarity by training adeep neural architecture in parallel to the task-specific network. It allowsthis observer to be dedicated to the uncertainty estimation, and let thetask-specific network make predictions. We propose to use self-supervision totrain the observer, which implies that our method does not require additionaltraining data. We show experimentally that our proposed approach is much lesscomputationally intensive at inference time than competing methods (e.g.MCDropout), while delivering better results on safety-oriented evaluationmetrics on the CamVid dataset, especially in the case of glare artifacts. ", "id2": "143", "id3": "None"}
{"id": "145", "content": "We propose a novel method for fine-grained high-quality image segmentation ofboth objects and scenes. Inspired by dilation and erosion from morphologicalimage processing techniques, we treat the pixel level segmentation problems assqueezing object boundary. From this perspective, we propose  textbf BoundarySqueeze  module: a novel and efficient module that squeezes the object boundaryfrom both inner and outer directions which leads to precise maskrepresentation. To generate such squeezed representation, we propose a newbidirectionally flow-based warping process and design specific loss signals tosupervise the learning process. Boundary Squeeze Module can be easily appliedto both instance and semantic segmentation tasks as a plug-and-play module bybuilding on top of existing models. We show that our simple yet effectivedesign can lead to high qualitative results on several different datasets andwe also provide several different metrics on boundary to prove theeffectiveness over previous work. Moreover, the proposed module islight-weighted and thus has potential for practical usage. Our method yieldslarge gains on COCO, Cityscapes, for both instance and semantic segmentationand outperforms previous state-of-the-art PointRend in both accuracy and speedunder the same setting. Code and model will be available. ", "id2": "144", "id3": "None"}
{"id": "146", "content": "Interactive single-image segmentation is ubiquitous in the scientific andcommercial imaging software. In this work, we focus on the single-imagesegmentation problem only with some seeds such as scribbles. Inspired by thedynamic receptive field in the human beings visual system, we propose theGaussian dynamic convolution (GDC) to fast and efficiently aggregate thecontextual information for neural networks. The core idea is randomly selectingthe spatial sampling area according to the Gaussian distribution offsets. OurGDC can be easily used as a module to build lightweight or complex segmentationnetworks. We adopt the proposed GDC to address the typical single-imagesegmentation tasks. Furthermore, we also build a Gaussian dynamic pyramidPooling to show its potential and generality in common semantic segmentation.Experiments demonstrate that the GDC outperforms other existing convolutions onthree benchmark segmentation datasets including Pascal-Context, Pascal-VOC2012, and Cityscapes. Additional experiments are also conducted to illustratethat the GDC can produce richer and more vivid features compared with otherconvolutions. In general, our GDC is conducive to the convolutional neuralnetworks to form an overall impression of the image. ", "id2": "145", "id3": "None"}
{"id2": 1017, "id3": "145", "content": "Interactive single-image segmentation is ubiquitous in the scientific andcommercial imaging software. In this work, we focus on the single-imagesegmentation problem only with some seeds such as scribbles. Inspired by thedynamic receptive field in the human beings visual system, we propose theGaussian dynamic convolution (GDC) to fast and efficiently aggregate thecontextual information for neural networks. The core idea is randomly selectingthe spatial sampling area according to the Gaussian distribution offsets. OurGDC can be easily used as a module to build lightweight or complex segmentationnetworks. We adopt the proposed GDC to address the typical single-imagesegmentation tasks. Furthermore, we also build a Gaussian dynamic pyramidPooling to show its potential and generality in common semantic segmentation.Experiments demonstrate that the GDC outperforms other existing convolutions onthree benchmark segmentation datasets including Pascal-Context, Pascal-VOC2012, and Cityscapes. Additional experiments are also conducted to illustratethat the GDC can produce richer and more vivid features compared with otherconvolutions. In general, our GDC is conducive to the convolutional neuralnetworks to form an overall impression of the image."}
{"id": "147", "content": "We present a Neural Network based Handwritten Text Recognition (HTR) modelarchitecture that can be trained to recognize full pages of handwritten orprinted text without image segmentation. Being based on Image to Sequencearchitecture, it can extract text present in an image and then sequence itcorrectly without imposing any constraints regarding orientation, layout andsize of text and non-text. Further, it can also be trained to generateauxiliary markup related to formatting, layout and content. We use characterlevel vocabulary, thereby enabling language and terminology of any subject. Themodel achieves a new state-of-art in paragraph level recognition on the IAMdataset. When evaluated on scans of real world handwritten free form testanswers - beset with curved and slanted lines, drawings, tables, math,chemistry and other symbols - it performs better than all commerciallyavailable HTR cloud APIs. It is deployed in production as part of a commercialweb application. ", "id2": "146", "id3": "None"}
{"id": "148", "content": "This paper presents a game, controlled by computer vision, in identificationof hand gestures (hand-tracking). The proposed work is based on imagesegmentation and construction of a convex hull with Jarvis Algorithm , anddetermination of the pattern based on the extraction of area characteristics inthe convex hull. ", "id2": "147", "id3": "None"}
{"id": "149", "content": "Active contours Model (ACM) has been extensively used in computer vision andimage processing. In recent studies, Convolutional Neural Networks (CNNs) havebeen combined with active contours replacing the user in the process of contourevolution and image segmentation to eliminate limitations associated with ACMsdependence on parameters of the energy functional and initialization. However,prior works did not aim for automatic initialization which is addressed here.In addition to manual initialization, current methods are highly sensitive toinitial location and fail to delineate borders accurately. We propose a fullyautomatic image segmentation method to address problems of manualinitialization, insufficient capture range, and poor convergence to boundaries,in addition to the problem of assignment of energy functional parameters. Wetrain two CNNs, which predict active contour weighting parameters and generatea ground truth mask to extract Distance Transform (DT) and an initializationcircle. Distance transform is used to form a vector field pointing from eachpixel of the image towards the closest point on the boundary, the size of whichis equal to the Euclidean distance map. We evaluate our method on four publiclyavailable datasets including two building instance segmentation datasets,Vaihingen and Bing huts, and two mammography image datasets, INBreast andDDSM-BCRP. Our approach outperforms latest research by 0.59 ans 2.39 percent inmean Intersection-over-Union (mIoU), 7.38 and 8.62 percent in Boundary F-score(BoundF) for Vaihingen and Bing huts datasets, respectively. Dice similaritycoefficient for the INBreast and DDSM-BCRP datasets is 94.23% and 90.89%,respectively indicating our method is comparable to state-of-the-artframeworks. ", "id2": "148", "id3": "None"}
{"id": "150", "content": "Image Segmentation has been an active field of research as it has a widerange of applications, ranging from automated disease detection to self-drivingcars. In recent years, various research papers proposed different lossfunctions used in case of biased data, sparse segmentation, and unbalanceddataset. In this paper, we introduce SemSegLoss, a python package consisting ofsome of the well-known loss functions widely used for image segmentation. It isdeveloped with the intent to help researchers in the development of novel lossfunctions and perform an extensive set of experiments on model architecturesfor various applications. The ease-of-use and flexibility of the presentedpackage have allowed reducing the development time and increased evaluationstrategies of machine learning models for semantic segmentation. Furthermore,different applications that use image segmentation can use SemSegLoss becauseof the generality of its functions. This wide range of applications will leadto the development and growth of AI across all industries. ", "id2": "149", "id3": "None"}
{"id": "151", "content": "Recent research has shown that numerous human-interpretable directions existin the latent space of GANs. In this paper, we develop an automatic procedurefor finding directions that lead to foreground-background image separation, andwe use these directions to train an image segmentation model without humansupervision. Our method is generator-agnostic, producing strong segmentationresults with a wide range of different GAN architectures. Furthermore, byleveraging GANs pretrained on large datasets such as ImageNet, we are able tosegment images from a range of domains without further training or finetuning.Evaluating our method on image segmentation benchmarks, we compare favorably toprior work while using neither human supervision nor access to the trainingdata. Broadly, our results demonstrate that automatically extractingforeground-background structure from pretrained deep generative models canserve as a remarkably effective substitute for human supervision. ", "id2": "150", "id3": "None"}
{"id": "152", "content": "Recent works in medical image segmentation have actively explored variousdeep learning architectures or objective functions to encode high-levelfeatures from volumetric data owing to limited image annotations. However, mostexisting approaches tend to ignore cross-volume global context and definecontext relations in the decision space. In this work, we propose a novelvoxel-level Siamese representation learning method for abdominal multi-organsegmentation to improve representation space. The proposed method enforcesvoxel-wise feature relations in the representation space for leveraging limiteddatasets more comprehensively to achieve better performance. Inspired by recentprogress in contrastive learning, we suppressed voxel-wise relations from thesame class to be projected to the same point without using negative samples.Moreover, we introduce a multi-resolution context aggregation method thataggregates features from multiple hidden layers, which encodes both the globaland local contexts for segmentation. Our experiments on the multi-organ datasetoutperformed the existing approaches by 2% in Dice score coefficient. Thequalitative visualizations of the representation spaces demonstrate that theimprovements were gained primarily by a disentangled feature space. ", "id2": "151", "id3": "None"}
{"id": "153", "content": "Medical image segmentation models are typically supervised by expertannotations at the pixel-level, which can be expensive to acquire. In thiswork, we propose a method that combines the high quality of pixel-level expertannotations with the scale of coarse DNN-generated saliency maps for trainingmulti-label semantic segmentation models. We demonstrate the application of oursemi-supervised method, which we call CheXseg, on multi-label chest X-rayinterpretation. We find that CheXseg improves upon the performance (mIoU) offully-supervised methods that use only pixel-level expert annotations by 9.7%and weakly-supervised methods that use only DNN-generated saliency maps by73.1%. Our best method is able to match radiologist agreement on three out often pathologies and reduces the overall performance gap by 57.2% as compared toweakly-supervised methods. ", "id2": "152", "id3": "None"}
{"id": "154", "content": "Scalable sensor simulation is an important yet challenging open problem forsafety-critical domains such as self-driving. Current works in image simulationeither fail to be photorealistic or do not model the 3D environment and thedynamic objects within, losing high-level control and physical realism. In thispaper, we present GeoSim, a geometry-aware image composition process whichsynthesizes novel urban driving scenarios by augmenting existing images withdynamic objects extracted from other scenes and rendered at novel poses.Towards this goal, we first build a diverse bank of 3D objects with bothrealistic geometry and appearance from sensor data. During simulation, weperform a novel geometry-aware simulation-by-composition procedure which 1)proposes plausible and realistic object placements into a given scene, 2)render novel views of dynamic objects from the asset bank, and 3) composes andblends the rendered image segments. The resulting synthetic images arerealistic, traffic-aware, and geometrically consistent, allowing our approachto scale to complex use cases. We demonstrate two such important applications:long-range realistic video simulation across multiple camera sensors, andsynthetic data generation for data augmentation on downstream segmentationtasks. Please check https://tmux.top/publication/geosim/ for high-resolutionvideo results. ", "id2": "153", "id3": "None"}
{"id": "155", "content": "Given a natural language expression and an image/video, the goal of referringsegmentation is to produce the pixel-level masks of the entities described bythe subject of the expression. Previous approaches tackle this problem byimplicit feature interaction and fusion between visual and linguisticmodalities in a one-stage manner. However, human tends to solve the referringproblem in a progressive manner based on informative words in the expression,i.e., first roughly locating candidate entities and then distinguishing thetarget one. In this paper, we propose a Cross-Modal Progressive Comprehension(CMPC) scheme to effectively mimic human behaviors and implement it as a CMPC-I(Image) module and a CMPC-V (Video) module to improve referring image and videosegmentation models. For image data, our CMPC-I module first employs entity andattribute words to perceive all the related entities that might be consideredby the expression. Then, the relational words are adopted to highlight thetarget entity as well as suppress other irrelevant ones by spatial graphreasoning. For video data, our CMPC-V module further exploits action wordsbased on CMPC-I to highlight the correct entity matched with the action cues bytemporal graph reasoning. In addition to the CMPC, we also introduce a simpleyet effective Text-Guided Feature Exchange (TGFE) module to integrate thereasoned multimodal features corresponding to different levels in the visualbackbone under the guidance of textual information. In this way, multi-levelfeatures can communicate with each other and be mutually refined based on thetextual context. Combining CMPC-I or CMPC-V with TGFE can form our image orvideo version referring segmentation frameworks and our frameworks achieve newstate-of-the-art performances on four referring image segmentation benchmarksand three referring video segmentation benchmarks respectively. ", "id2": "154", "id3": "None"}
{"id": "156", "content": "Automated segmentation in medical image analysis is a challenging task thatrequires a large amount of manually labeled data. However, manually annotatingmedical data is often laborious, and most existing learning-based approachesfail to accurately delineate object boundaries without effective geometricconstraints. Contrastive learning, a sub-area of self-supervised learning, hasrecently been noted as a promising direction in multiple application fields. Inthis work, we present a novel Contrastive Voxel-wise Representation Learning(CVRL) method with geometric constraints to learn global-local visualrepresentations for volumetric medical image segmentation with limitedannotations. Our framework can effectively learn global and local features bycapturing 3D spatial context and rich anatomical information. Specifically, weintroduce a voxel-to-volume contrastive algorithm to learn global informationfrom 3D images, and propose to perform local voxel-to-voxel contrast toexplicitly make use of local cues in the embedding space. Moreover, weintegrate an elastic interaction-based active contour model as a geometricregularization term to enable fast and reliable object delineations in anend-to-end learning manner. Results on the Atrial Segmentation Challengedataset demonstrate superiority of our proposed scheme, especially in a settingwith a very limited number of annotated data. ", "id2": "155", "id3": "None"}
{"id": "157", "content": "A connectivity graph of neurons at the resolution of single synapses providesscientists with a tool for understanding the nervous system in health anddisease. Recent advances in automatic image segmentation and synapse predictionin electron microscopy (EM) datasets of the brain have made reconstructions ofneurons possible at the nanometer scale. However, automatic segmentationsometimes struggles to segment large neurons correctly, requiring human effortto proofread its output. General proofreading involves inspecting large volumesto correct segmentation errors at the pixel level, a visually intensive andtime-consuming process. This paper presents the design and implementation of ananalytics framework that streamlines proofreading, focusing onconnectivity-related errors. We accomplish this with automated likely-errordetection and synapse clustering that drives the proofreading effort withhighly interactive 3D visualizations. In particular, our strategy centers onproofreading the local circuit of a single cell to ensure a basic level ofcompleteness. We demonstrate our frameworks utility with a user study andreport quantitative and subjective feedback from our users. Overall, users findthe framework more efficient for proofreading, understanding evolving graphs,and sharing error correction strategies. ", "id2": "156", "id3": "None"}
{"id": "158", "content": "Image segmentation refers to the separation of objects from the background,and has been one of the most challenging aspects of digital image processing.Practically it is impossible to design a segmentation algorithm which has 100%accuracy, and therefore numerous segmentation techniques have been proposed inthe literature, each with certain limitations. In this paper, a novelFalling-Ball algorithm is presented, which is a region-based segmentationalgorithm, and an alternative to watershed transform (based on waterfallmodel). The proposed algorithm detects the catchment basins by assuming that aball falling from hilly terrains will stop in a catchment basin. Once catchmentbasins are identified, the association of each pixel with one of the catchmentbasin is obtained using multi-criterion fuzzy logic. Edges are constructed bydividing image into different catchment basins with the help of a membershipfunction. Finally closed contour algorithm is applied to find closed regionsand objects within closed regions are segmented using intensity information.The performance of the proposed algorithm is evaluated both objectively as wellas subjectively. Simulation results show that the proposed algorithms givessuperior performance over conventional Sobel edge detection methods and thewatershed segmentation algorithm. For comparative analysis, various comparisonmethods are used for demonstrating the superiority of proposed methods overexisting segmentation methods. ", "id2": "157", "id3": "None"}
{"id": "159", "content": "Food image segmentation is a critical and indispensible task for developinghealth-related applications such as estimating food calories and nutrients.Existing food image segmentation models are underperforming due to two reasons:(1) there is a lack of high quality food image datasets with fine-grainedingredient labels and pixel-wise location masks -- the existing datasets eithercarry coarse ingredient labels or are small in size; and (2) the complexappearance of food makes it difficult to localize and recognize ingredients infood images, e.g., the ingredients may overlap one another in the same image,and the identical ingredient may appear distinctly in different food images. Inthis work, we build a new food image dataset FoodSeg103 (and its extensionFoodSeg154) containing 9,490 images. We annotate these images with 154ingredient classes and each image has an average of 6 ingredient labels andpixel-wise masks. In addition, we propose a multi-modality pre-trainingapproach called ReLeM that explicitly equips a segmentation model with rich andsemantic food knowledge. In experiments, we use three popular semanticsegmentation methods (i.e., Dilated Convolution based, Feature Pyramid based,and Vision Transformer based) as baselines, and evaluate them as well as ReLeMon our new datasets. We believe that the FoodSeg103 (and its extensionFoodSeg154) and the pre-trained models using ReLeM can serve as a benchmark tofacilitate future works on fine-grained food image understanding. We make allthese datasets and methods public at url https://xiongweiwu.github.io/foodseg103.html . ", "id2": "158", "id3": "None"}
{"id": "160", "content": "Although deep models have greatly improved the accuracy and robustness ofimage segmentation, obtaining segmentation results with highly accurateboundaries and fine structures is still a challenging problem. In this paper,we propose a simple yet powerful Boundary-Aware Segmentation Network (BASNet),which comprises a predict-refine architecture and a hybrid loss, for highlyaccurate image segmentation. The predict-refine architecture consists of adensely supervised encoder-decoder network and a residual refinement module,which are respectively used to predict and refine a segmentation probabilitymap. The hybrid loss is a combination of the binary cross entropy, structuralsimilarity and intersection-over-union losses, which guide the network to learnthree-level (ie, pixel-, patch- and map- level) hierarchy representations. Weevaluate our BASNet on two reverse tasks including salient object segmentation,camouflaged object segmentation, showing that it achieves very competitiveperformance with sharp segmentation boundaries. Importantly, BASNet runs atover 70 fps on a single GPU which benefits many potential real applications.Based on BASNet, we further developed two (close to) commercial applications:AR COPY & PASTE, in which BASNet is integrated with augmented reality forCOPYING and PASTING real-world objects, and OBJECT CUT, which is aweb-based tool for automatic object background removal. Both applications havealready drawn huge amount of attention and have important real-world impacts.The code and two applications will be publicly available at:https://github.com/NathanUA/BASNet. ", "id2": "159", "id3": "None"}
{"id": "161", "content": "Training neural networks with auxiliary tasks is a common practice forimproving the performance on a main task of interest. Two main challenges arisein this multi-task learning setting: (i) designing useful auxiliary tasks; and(ii) combining auxiliary tasks into a single coherent loss. Here, we propose anovel framework, AuxiLearn, that targets both challenges based on implicitdifferentiation. First, when useful auxiliaries are known, we propose learninga network that combines all losses into a single coherent objective function.This network can learn non-linear interactions between tasks. Second, when nouseful auxiliary task is known, we describe how to learn a network thatgenerates a meaningful, novel auxiliary task. We evaluate AuxiLearn in a seriesof tasks and domains, including image segmentation and learning with attributesin the low data regime, and find that it consistently outperforms competingmethods. ", "id2": "160", "id3": "None"}
{"id": "162", "content": "Segmenting an entire 3D image often has high computational complexity andrequires large memory consumption; by contrast, performing volumetricsegmentation in a slice-by-slice manner is efficient but does not fullyleverage the 3D data. To address this challenge, we propose a multi-dimensionalattention network (MDA-Net) to efficiently integrate slice-wise, spatial, andchannel-wise attention into a U-Net based network, which results in highsegmentation accuracy with a low computational cost. We evaluate our model onthe MICCAI iSeg and IBSR datasets, and the experimental results demonstrateconsistent improvements over existing methods. ", "id2": "161", "id3": "None"}
{"id": "163", "content": "Active contour models have been widely used in image segmentation, and thelevel set method (LSM) is the most popular approach for solving the models, viaimplicitly representing the contour by a level set function. However, the LSMsuffers from high computational burden and numerical instability, requiringadditional regularization terms or re-initialization techniques. In this paper,we use characteristic functions to implicitly represent the contours, propose anew representation to the geodesic active contours and derive an efficientalgorithm termed as the iterative convolution-thresholding method (ICTM).Compared to the LSM, the ICTM is simpler and much more efficient. In addition,the ICTM enjoys most desired features of the level set-based methods. Extensiveexperiments, on 2D synthetic, 2D ultrasound, 3D CT, and 3D MR images fornodule, organ and lesion segmentation, demonstrate that the proposed method notonly obtains comparable or even better segmentation results (compared to theLSM) but also achieves significant acceleration. ", "id2": "162", "id3": "None"}
{"id": "164", "content": "Minimal paths are regarded as a powerful and efficient tool for boundarydetection and image segmentation due to its global optimality and thewell-established numerical solutions such as fast marching method. In thispaper, we introduce a flexible interactive image segmentation model based onthe Eikonal partial differential equation (PDE) framework in conjunction withregion-based homogeneity enhancement. A key ingredient in the introduced modelis the construction of local geodesic metrics, which are capable of integratinganisotropic and asymmetric edge features, implicit region-based homogeneityfeatures and/or curvature regularization. The incorporation of the region-basedhomogeneity features into the metrics considered relies on an implicitrepresentation of these features, which is one of the contributions of thiswork. Moreover, we also introduce a way to build simple closed contours as theconcatenation of two disjoint open curves. Experimental results prove that theproposed model indeed outperforms state-of-the-art minimal paths-based imagesegmentation approaches. ", "id2": "163", "id3": "None"}
{"id2": 1018, "id3": "163", "content": "Minimal paths are regarded as a powerful and efficient tool for boundarydetection and image segmentation due to its global optimality and thewell-established numerical solutions such as fast marching method. In thispaper, we introduce a flexible interactive image segmentation model based onthe Eikonal partial differential equation (PDE) framework in conjunction withregion-based homogeneity enhancement. A key ingredient in the introduced modelis the construction of local geodesic metrics, which are capable of integratinganisotropic and asymmetric edge features, implicit region-based homogeneityfeatures and/or curvature regularization. The incorporation of the region-basedhomogeneity features into the metrics considered relies on an implicitrepresentation of these features, which is one of the contributions of thiswork. Moreover, we also introduce a way to build simple closed contours as theconcatenation of two disjoint open curves. Experimental results prove that theproposed model indeed outperforms state-of-the-art minimal paths-based imagesegmentation approaches."}
{"id": "165", "content": "While multiple studies have explored the relation between inter-ratervariability and deep learning model uncertainty in medical segmentation tasks,little is known about the impact of individual rater style. This studyquantifies rater style in the form of bias and consistency and explores theirimpacts when used to train deep learning models. Two multi-rater publicdatasets were used, consisting of brain multiple sclerosis lesion and spinalcord grey matter segmentation. On both datasets, results show a correlation($R^2 = 0.60$ and $0.93$) between rater bias and deep learning uncertainty. Theimpact of label fusion between raters annotations on this relationship is alsoexplored, and we show that multi-center consensuses are more effective thansingle-center consensuses to reduce uncertainty, since rater style is mostlycenter-specific. ", "id2": "164", "id3": "None"}
{"id": "166", "content": "Incorporating shape information is essential for the delineation of manyorgans and anatomical structures in medical images. While previous work hasmainly focused on parametric spatial transformations applied on referencetemplate shapes, in this paper, we address the Bayesian inference of parametricshape models for segmenting medical images with the objective to provideinterpretable results. The proposed framework defines a likelihood appearanceprobability and a prior label probability based on a generic shape functionthrough a logistic function. A reference length parameter defined in thesigmoid controls the trade-off between shape and appearance information. Theinference of shape parameters is performed within an Expectation-Maximisationapproach where a Gauss-Newton optimization stage allows to provide anapproximation of the posterior probability of shape parameters. This frameworkis applied to the segmentation of cochlea structures from clinical CT imagesconstrained by a 10 parameter shape model. It is evaluated on three differentdatasets, one of which includes more than 200 patient images. The results showperformances comparable to supervised methods and better than previouslyproposed unsupervised ones. It also enables an analysis of parameterdistributions and the quantification of segmentation uncertainty including theeffect of the shape model. ", "id2": "165", "id3": "None"}
{"id": "167", "content": "Recently, referring image segmentation has aroused widespread interest.Previous methods perform the multi-modal fusion between language and vision atthe decoding side of the network. And, linguistic feature interacts with visualfeature of each scale separately, which ignores the continuous guidance oflanguage to multi-scale visual features. In this work, we propose an encoderfusion network (EFN), which transforms the visual encoder into a multi-modalfeature learning network, and uses language to refine the multi-modal featuresprogressively. Moreover, a co-attention mechanism is embedded in the EFN torealize the parallel update of multi-modal features, which can promote theconsistent of the cross-modal information representation in the semantic space.Finally, we propose a boundary enhancement module (BEM) to make the network paymore attention to the fine structure. The experiment results on four benchmarkdatasets demonstrate that the proposed approach achieves the state-of-the-artperformance under different evaluation metrics without any post-processing. ", "id2": "166", "id3": "None"}
{"id": "168", "content": "Citrus segmentation is a key step of automatic citrus picking. While mostcurrent image segmentation approaches achieve good segmentation results bypixel-wise segmentation, these supervised learning-based methods require alarge amount of annotated data, and do not consider the continuous temporalchanges of citrus position in real-world applications. In this paper, we firsttrain a simple CNN with a small number of labelled citrus images in asupervised manner, which can roughly predict the citrus location from eachframe. Then, we extend a state-of-the-art unsupervised learning approach topre-learn the citruss potential movements between frames from unlabelledcitruss videos. To take advantages of both networks, we employ the multimodaltransformer to combine supervised learned static information and unsupervisedlearned movement information. The experimental results show that combing bothnetwork allows the prediction accuracy reached at 88.3$ %$ IOU and 93.6$ %$precision, outperforming the original supervised baseline 1.2$ %$ and 2.4$ %$.Compared with most of the existing citrus segmentation methods, our method usesa small amount of supervised data and a large number of unsupervised data,while learning the pixel level location information and the temporalinformation of citrus changes to enhance the segmentation effect. ", "id2": "167", "id3": "None"}
{"id2": 1019, "id3": "167", "content": "Citrus segmentation is a key step of automatic citrus picking. While mostcurrent image segmentation approaches achieve good segmentation results bypixel-wise segmentation, these supervised learning-based methods require alarge amount of annotated data, and do not consider the continuous temporalchanges of citrus position in real-world applications. In this paper, we firsttrain a simple CNN with a small number of labelled citrus images in asupervised manner, which can roughly predict the citrus location from eachframe. Then, we extend a state-of-the-art unsupervised learning approach topre-learn the citruss potential movements between frames from unlabelledcitruss videos. To take advantages of both networks, we employ the multimodaltransformer to combine supervised learned static information and unsupervisedlearned movement information. The experimental results show that combing bothnetwork allows the prediction accuracy reached at 88.3$ %$ IOU and 93.6$ %$precision, outperforming the original supervised baseline 1.2$ %$ and 2.4$ %$.Compared with most of the existing citrus segmentation methods, our method usesa small amount of supervised data and a large number of unsupervised data,while learning the pixel level location information and the temporalinformation of citrus changes to enhance the segmentation effect."}
{"id": "169", "content": "This work explores the use of computer vision for image segmentation andclassification of medical fluid samples in transparent containers (for example,tubes, syringes, infusion bags). Handling fluids such as infusion fluids,blood, and urine samples is a significant part of the work carried out inmedical labs and hospitals. The ability to accurately identify and segment theliquids and the vessels that contain them from images can help in automatingsuch processes. Modern computer vision typically involves training deep neuralnets on large datasets of annotated images. This work presents a new datasetcontaining 1,300 annotated images of medical samples involving vesselscontaining liquids and solid material. The images are annotated with the typeof liquid (e.g., blood, urine), the phase of the material (e.g., liquid, solid,foam, suspension), the type of vessel (e.g., syringe, tube, cup, infusionbottle/bag), and the properties of the vessel (transparent, opaque). Inaddition, vessel parts such as corks, labels, spikes, and valves are annotated.Relations and hierarchies between vessels and materials are also annotated,such as which vessel contains which material or which vessels are linked orcontain each other. Three neural networks are trained on the dataset: Onenetwork learns to detect vessels, a second net detects the materials and partsinside each vessel, and a third net identifies relationships and connectivitybetween vessels. ", "id2": "168", "id3": "None"}
{"id": "170", "content": "The Voronoi diagram-based dual-front active contour models are known as apowerful and efficient way for addressing the image segmentation and domainpartitioning problems. In the basic formulation of the dual-front models, theevolving contours can be considered as the interfaces of adjacent Voronoiregions. Among these dual-front models, a crucial ingredient is regarded as thegeodesic metrics by which the geodesic distances and the corresponding Voronoidiagram can be estimated. In this paper, we introduce a type of asymmetricquadratic metrics dual-front model. The metrics considered are built by theintegration of the image features and a vector field derived from the evolvingcontours. The use of the asymmetry enhancement can reduce the risk of contourshortcut or leakage problems especially when the initial contours are far awayfrom the target boundaries or the images have complicated intensitydistributions. Moreover, the proposed dual-front model can be applied for imagesegmentation in conjunction with various region-based homogeneity terms. Thenumerical experiments on both synthetic and real images show that the proposeddual-front model indeed achieves encouraging results. ", "id2": "169", "id3": "None"}
{"id": "171", "content": "Standard losses for training deep segmentation networks could be seen asindividual classifications of pixels, instead of supervising the global shapeof the predicted segmentations. While effective, they require exact knowledgeof the label of each pixel in an image.  This study investigates how effective global geometric shape descriptorscould be, when used on their own as segmentation losses for training deepnetworks. Not only interesting theoretically, there exist deeper motivations toposing segmentation problems as a reconstruction of shape descriptors:Annotations to obtain approximations of low-order shape moments could be muchless cumbersome than their full-mask counterparts, and anatomical priors couldbe readily encoded into invariant shape descriptions, which might alleviate theannotation burden. Also, and most importantly, we hypothesize that, given atask, certain shape descriptions might be invariant across image acquisitionprotocols/modalities and subject populations, which might open interestingresearch avenues for generalization in medical image segmentation.  We introduce and formulate a few shape descriptors in the context of deepsegmentation, and evaluate their potential as standalone losses on twodifferent challenging tasks. Inspired by recent works in constrainedoptimization for deep networks, we propose a way to use those descriptors tosupervise segmentation, without any pixel-level label. Very surprisingly, aslittle as 4 descriptors values per class can approach the performance of asegmentation mask with 65k individual discrete labels. We also found that shapedescriptors can be a valid way to encode anatomical priors about the task,enabling to leverage expert knowledge without additional annotations. Ourimplementation is publicly available and can be easily extended to other tasksand descriptors: https://github.com/hkervadec/shape_descriptors ", "id2": "170", "id3": "None"}
{"id": "172", "content": "The field of view (FOV) of convolutional neural networks is highly related tothe accuracy of inference. Dilated convolutions are known as an effectivesolution to the problems which require large FOVs. However, for general-purposehardware or dedicated hardware, it usually takes extra time to handle dilatedconvolutions compared with standard convolutions. In this paper, we propose anetwork module, Cascaded and Separable Structure of Dilated (CASSOD)Convolution, and a special hardware system to handle the CASSOD networksefficiently. A CASSOD-Net includes multiple cascaded $2  times 2$ dilatedfilters, which can be used to replace the traditional $3  times 3$ dilatedfilters without decreasing the accuracy of inference. Two example applications,face detection and image segmentation, are tested with dilated convolutions andthe proposed CASSOD modules. The new network for face detection achieves higheraccuracy than the previous work with only 47% of filter weights in the dilatedconvolution layers of the context module. Moreover, the proposed hardwaresystem can accelerate the computations of dilated convolutions, and it is 2.78times faster than traditional hardware systems when the filter size is $3 times 3$. ", "id2": "171", "id3": "None"}
{"id": "173", "content": "We propose a parameter efficient Bayesian layer for hierarchicalconvolutional Gaussian Processes that incorporates Gaussian Processes operatingin Wasserstein-2 space to reliably propagate uncertainty. This directlyreplaces convolving Gaussian Processes with a distance-preserving affineoperator on distributions. Our experiments on brain tissue-segmentation showthat the resulting architecture approaches the performance of well-establisheddeterministic segmentation algorithms (U-Net), which has never been achievedwith previous hierarchical Gaussian Processes. Moreover, by applying the samesegmentation model to out-of-distribution data (i.e., images with pathologysuch as brain tumors), we show that our uncertainty estimates result inout-of-distribution detection that outperforms the capabilities of previousBayesian networks and reconstruction-based approaches that learn normativedistributions. ", "id2": "172", "id3": "None"}
{"id": "174", "content": "Siamese-based trackers have achived promising performance on visual objecttracking tasks. Most existing Siamese-based trackers contain two separatebranches for tracking, including classification branch and bounding boxregression branch. In addition, image segmentation provides an alternative wayto obetain the more accurate target region. In this paper, we propose a noveltracker with two-stages: detection and segmentation. The detection stage iscapable of locating the target by Siamese networks. Then more accurate trackingresults are obtained by segmentation module given the coarse state estimationin the first stage. We conduct experiments on four benchmarks. Our approachachieves state-of-the-art results, with the EAO of 52.6$ %$ on VOT2016,51.3$ %$ on VOT2018, and 39.0$ %$ on VOT2019 datasets, respectively. ", "id2": "173", "id3": "None"}
{"id": "175", "content": "Pixel-wise segmentation is one of the most data and annotation hungry tasksin our field. Providing representative and accurate annotations is oftenmission-critical especially for challenging medical applications. In thispaper, we propose a semi-weakly supervised segmentation algorithm to overcomethis barrier. Our approach is based on a new formulation of deep supervisionand student-teacher model and allows for easy integration of differentsupervision signals. In contrast to previous work, we show that care has to betaken how deep supervision is integrated in lower layers and we presentmulti-label deep supervision as the most important secret ingredient forsuccess. With our novel training regime for segmentation that flexibly makesuse of images that are either fully labeled, marked with bounding boxes, justglobal labels, or not at all, we are able to cut the requirement for expensivelabels by 94.22% - narrowing the gap to the best fully supervised baseline toonly 5% mean IoU. Our approach is validated by extensive experiments on retinalfluid segmentation and we provide an in-depth analysis of the anticipatedeffect each annotation type can have in boosting segmentation performance. ", "id2": "174", "id3": "None"}
{"id": "176", "content": "BiSeNet has been proved to be a popular two-stream network for real-timesegmentation. However, its principle of adding an extra path to encode spatialinformation is time-consuming, and the backbones borrowed from pretrainedtasks, e.g., image classification, may be inefficient for image segmentationdue to the deficiency of task-specific design. To handle these problems, wepropose a novel and efficient structure named Short-Term Dense Concatenatenetwork (STDC network) by removing structure redundancy. Specifically, wegradually reduce the dimension of feature maps and use the aggregation of themfor image representation, which forms the basic module of STDC network. In thedecoder, we propose a Detail Aggregation module by integrating the learning ofspatial information into low-level layers in single-stream manner. Finally, thelow-level features and deep features are fused to predict the finalsegmentation results. Extensive experiments on Cityscapes and CamVid datasetdemonstrate the effectiveness of our method by achieving promising trade-offbetween segmentation accuracy and inference speed. On Cityscapes, we achieve71.9% mIoU on the test set with a speed of 250.4 FPS on NVIDIA GTX 1080Ti,which is 45.2% faster than the latest methods, and achieve 76.8% mIoU with 97.0FPS while inferring on higher resolution images. ", "id2": "175", "id3": "None"}
{"id": "177", "content": "Medical images are often accompanied by metadata describing the image(vendor, acquisition parameters) and the patient (disease type or severity,demographics, genomics). This metadata is usually disregarded by imagesegmentation methods. In this work, we adapt a linear conditioning methodcalled FiLM (Feature-wise Linear Modulation) for image segmentation tasks. ThisFiLM adaptation enables integrating metadata into segmentation models forbetter performance. We observed an average Dice score increase of 5.1% onspinal cord tumor segmentation when incorporating the tumor type with FiLM. Themetadata modulates the segmentation process through low-cost affinetransformations applied on feature maps which can be included in any neuralnetworks architecture. Additionally, we assess the relevance of segmentationFiLM layers for tackling common challenges in medical imaging: multi-classtraining with missing segmentations, model adaptation to multiple tasks, andtraining with a limited or unbalanced number of annotated data. Our resultsdemonstrated the following benefits of FiLM for segmentation: FiLMed U-Net wasrobust to missing labels and reached higher Dice scores with few labels (up to16.7%) compared to single-task U-Net. The code is open-source and available atwww.ivadomed.org. ", "id2": "176", "id3": "None"}
{"id": "178", "content": "Segmentation of organs or lesions from medical images plays an essential rolein many clinical applications such as diagnosis and treatment planning. ThoughConvolutional Neural Networks (CNN) have achieved the state-of-the-artperformance for automatic segmentation, they are often limited by the lack ofclinically acceptable accuracy and robustness in complex cases. Therefore,interactive segmentation is a practical alternative to these methods. However,traditional interactive segmentation methods require a large amount of userinteractions, and recently proposed CNN-based interactive segmentation methodsare limited by poor performance on previously unseen objects. To solve theseproblems, we propose a novel deep learning-based interactive segmentationmethod that not only has high efficiency due to only requiring clicks as userinputs but also generalizes well to a range of previously unseen objects.Specifically, we first encode user-provided interior margin points via ourproposed exponentialized geodesic distance that enables a CNN to achieve a goodinitial segmentation result of both previously seen and unseen objects, then weuse a novel information fusion method that combines the initial segmentationwith only few additional user clicks to efficiently obtain a refinedsegmentation. We validated our proposed framework through extensive experimentson 2D and 3D medical image segmentation tasks with a wide range of previousunseen objects that were not present in the training set. Experimental resultsshowed that our proposed framework 1) achieves accurate results with fewer userinteractions and less time compared with state-of-the-art interactiveframeworks and 2) generalizes well to previously unseen objects. ", "id2": "177", "id3": "None"}
{"id": "179", "content": "Deep neural networks (DNNs) have demonstrated their great potential in recentyears, exceeding the per-formance of human experts in a wide range ofapplications. Due to their large sizes, however, compressiontechniques such asweight quantization and pruning are usually applied before they can beaccommodated onthe edge. It is generally believed that quantization leads toperformance degradation, and plenty of existingworks have explored quantizationstrategies aiming at minimum accuracy loss. In this paper, we arguethatquantization, which essentially imposes regularization on weightrepresentations, can sometimes help toimprove accuracy. We conductcomprehensive experiments on three widely used applications: fully con-nectednetwork (FCN) for biomedical image segmentation, convolutional neural network(CNN) for imageclassification on ImageNet, and recurrent neural network (RNN)for automatic speech recognition, and experi-mental results show thatquantization can improve the accuracy by 1%, 1.95%, 4.23% on the threeapplicationsrespectively with 3.5x-6.4x memory reduction. ", "id2": "178", "id3": "None"}
{"id": "180", "content": "Advances in object-centric generative models (OCGMs) have culminated in thedevelopment of a broad range of methods for unsupervised object segmentationand interpretable object-centric scene generation. These methods, however, arelimited to simulated and real-world datasets with limited visual complexity.Moreover, object representations are often inferred using RNNs which do notscale well to large images or iterative refinement which avoids imposing anunnatural ordering on objects in an image but requires the a prioriinitialisation of a fixed number of object representations. In contrast toestablished paradigms, this work proposes an embedding-based approach in whichembeddings of pixels are clustered in a differentiable fashion using astochastic, non-parametric stick-breaking process. Similar to iterativerefinement, this clustering procedure also leads to randomly ordered objectrepresentations, but without the need of initialising a fixed number ofclusters a priori. This is used to develop a new model, GENESIS-V2, which caninfer a variable number of object representations without using RNNs oriterative refinement. We show that GENESIS-V2 outperforms previous methods forunsupervised image segmentation and object-centric scene generation onestablished synthetic datasets as well as more complex real-world datasets. ", "id2": "179", "id3": "None"}
{"id": "181", "content": "Semantic image segmentation aims to obtain object labels with preciseboundaries, which usually suffers from overfitting. Recently, various dataaugmentation strategies like regional dropout and mix strategies have beenproposed to address the problem. These strategies have proved to be effectivefor guiding the model to attend on less discriminative parts. However, currentstrategies operate at the image level, and objects and the background arecoupled. Thus, the boundaries are not well augmented due to the fixed semanticscenario. In this paper, we propose ObjectAug to perform object-levelaugmentation for semantic image segmentation. ObjectAug first decouples theimage into individual objects and the background using the semantic labels.Next, each object is augmented individually with commonly used augmentationmethods (e.g., scaling, shifting, and rotation). Then, the black area broughtby object augmentation is further restored using image inpainting. Finally, theaugmented objects and background are assembled as an augmented image. In thisway, the boundaries can be fully explored in the various semantic scenarios. Inaddition, ObjectAug can support category-aware augmentation that gives variouspossibilities to objects in each category, and can be easily combined withexisting image-level augmentation methods to further boost performance.Comprehensive experiments are conducted on both natural image and medical imagedatasets. Experiment results demonstrate that our ObjectAug can evidentlyimprove segmentation performance. ", "id2": "180", "id3": "None"}
{"id": "182", "content": "Video segmentation for the human head and shoulders is essential in creatingelegant media for videoconferencing and virtual reality applications. The mainchallenge is to process high-quality background subtraction in a real-timemanner and address the segmentation issues under motion blurs, e.g., shakingthe head or waving hands during conference video. To overcome the motion blurproblem in video segmentation, we propose a novel flow-based encoder-decodernetwork (FUNet) that combines both traditional Horn-Schunck optical-flowestimation technique and convolutional neural networks to perform robustreal-time video segmentation. We also introduce a video and image segmentationdataset: ConferenceVideoSegmentationDataset. Code and pre-trained models areavailable on our GitHub repository: url https://github.com/kuangzijian/Flow-Based-Video-Matting . ", "id2": "181", "id3": "None"}
{"id": "183", "content": "We introduce DatasetGAN: an automatic procedure to generate massive datasetsof high-quality semantically segmented images requiring minimal human effort.Current deep networks are extremely data-hungry, benefiting from training onlarge-scale datasets, which are time consuming to annotate. Our method relieson the power of recent GANs to generate realistic images. We show how the GANlatent code can be decoded to produce a semantic segmentation of the image.Training the decoder only needs a few labeled examples to generalize to therest of the latent space, resulting in an infinite annotated dataset generator!These generated datasets can then be used for training any computer visionarchitecture just as real datasets are. As only a few images need to bemanually segmented, it becomes possible to annotate images in extreme detailand generate datasets with rich object and part segmentations. To showcase thepower of our approach, we generated datasets for 7 image segmentation taskswhich include pixel-level labels for 34 human face parts, and 32 car parts. Ourapproach outperforms all semi-supervised baselines significantly and is on parwith fully supervised methods, which in some cases require as much as 100x moreannotated data as our method. ", "id2": "182", "id3": "None"}
{"id": "184", "content": "In supervised learning for medical image analysis, sample selectionmethodologies are fundamental to attain optimum system performance promptly andwith minimal expert interactions (e.g. label querying in an active learningsetup). In this paper we propose a novel sample selection methodology based ondeep features leveraging information contained in interpretability saliencymaps. In the absence of ground truth labels for informative samples, we use anovel self supervised learning based approach for training a classifier thatlearns to identify the most informative sample in a given batch of images. Wedemonstrate the benefits of the proposed approach, termedInterpretability-Driven Sample Selection (IDEAL), in an active learning setupaimed at lung disease classification and histopathology image segmentation. Weanalyze three different approaches to determine sample informativeness frominterpretability saliency maps: (i) an observational model stemming fromfindings on previous uncertainty-based sample selection approaches, (ii) aradiomics-based model, and (iii) a novel data-driven self-supervised approach.We compare IDEAL to other baselines using the publicly available NIH chestX-ray dataset for lung disease classification, and a public histopathologysegmentation dataset (GLaS), demonstrating the potential of usinginterpretability information for sample selection in active learning systems.Results show our proposed self supervised approach outperforms other approachesin selecting informative samples leading to state of the art performance withfewer samples. ", "id2": "183", "id3": "None"}
{"id": "185", "content": "Training deep networks with limited labeled data while achieving a stronggeneralization ability is key in the quest to reduce human annotation efforts.This is the goal of semi-supervised learning, which exploits more widelyavailable unlabeled data to complement small labeled data sets. In this paper,we propose a novel framework for discriminative pixel-level tasks using agenerative model of both images and labels. Concretely, we learn a generativeadversarial network that captures the joint image-label distribution and istrained efficiently using a large set of unlabeled images supplemented withonly few labeled ones. We build our architecture on top of StyleGAN2, augmentedwith a label synthesis branch. Image labeling at test time is achieved by firstembedding the target image into the joint latent space via an encoder networkand test-time optimization, and then generating the label from the inferredembedding. We evaluate our approach in two important domains: medical imagesegmentation and part-based face segmentation. We demonstrate strong in-domainperformance compared to several baselines, and are the first to showcaseextreme out-of-domain generalization, such as transferring from CT to MRI inmedical imaging, and photographs of real faces to paintings, sculptures, andeven cartoons and animal faces. Project Page: url https://nv-tlabs.github.io/semanticGAN/  ", "id2": "184", "id3": "None"}
{"id": "186", "content": "The task of image segmentation is inherently noisy due to ambiguitiesregarding the exact location of boundaries between anatomical structures. Weargue that this information can be extracted from the expert annotations at noextra cost, and when integrated into state-of-the-art neural networks, it canlead to improved calibration between soft probabilistic predictions and theunderlying uncertainty. We built upon label smoothing (LS) where a network istrained on blurred versions of the ground truth labels which has been shownto be effective for calibrating output predictions. However, LS is not takingthe local structure into account and results in overly smoothed predictionswith low confidence even for non-ambiguous regions. Here, we propose SpatiallyVarying Label Smoothing (SVLS), a soft labeling technique that captures thestructural uncertainty in semantic segmentation. SVLS also naturally lendsitself to incorporate inter-rater uncertainty when multiple labelmaps areavailable. The proposed approach is extensively validated on four clinicalsegmentation tasks with different imaging modalities, number of classes andsingle and multi-rater expert annotations. The results demonstrate that SVLS,despite its simplicity, obtains superior boundary prediction with improveduncertainty and model calibration. ", "id2": "185", "id3": "None"}
{"id2": 1020, "id3": "185", "content": "The task of image segmentation is inherently noisy due to ambiguitiesregarding the exact location of boundaries between anatomical structures. Weargue that this information can be extracted from the expert annotations at noextra cost, and when integrated into state-of-the-art neural networks, it canlead to improved calibration between soft probabilistic predictions and theunderlying uncertainty. We built upon label smoothing (LS) where a network istrained on blurred versions of the ground truth labels which has been shownto be effective for calibrating output predictions. However, LS is not takingthe local structure into account and results in overly smoothed predictionswith low confidence even for non-ambiguous regions. Here, we propose SpatiallyVarying Label Smoothing (SVLS), a soft labeling technique that captures thestructural uncertainty in semantic segmentation. SVLS also naturally lendsitself to incorporate inter-rater uncertainty when multiple labelmaps areavailable. The proposed approach is extensively validated on four clinicalsegmentation tasks with different imaging modalities, number of classes andsingle and multi-rater expert annotations. The results demonstrate that SVLS,despite its simplicity, obtains superior boundary prediction with improveduncertainty and model calibration."}
{"id": "187", "content": "While convolutional neural networks (CNNs) trained by back-propagation haveseen unprecedented success at semantic segmentation tasks, they are known tostruggle on out-of-distribution data. Markov random fields (MRFs) on the otherhand, encode simpler distributions over labels that, although less flexiblethan UNets, are less prone to over-fitting. In this paper, we propose to fuseboth strategies by computing the product of distributions of a UNet and an MRF.As this product is intractable, we solve for an approximate distribution usingan iterative mean-field approach. The resulting MRF-UNet is trained jointly byback-propagation. Compared to other works using conditional random fields(CRFs), the MRF has no dependency on the imaging data, which should allow forless over-fitting. We show on 3D neuroimaging data that this novel networkimproves generalisation to out-of-distribution samples. Furthermore, it allowsthe overall number of parameters to be reduced while preserving high accuracy.These results suggest that a classic MRF smoothness prior can allow for lessover-fitting when principally integrated into a CNN model. Our implementationis available at https://github.com/balbasty/nitorch. ", "id2": "186", "id3": "None"}
{"id": "188", "content": "We propose to adapt segmentation networks with a constrained formulation,which embeds domain-invariant prior knowledge about the segmentation regions.Such knowledge may take the form of simple anatomical information, e.g.,structure size or shape, estimated from source samples or known a priori. Ourmethod imposes domain-invariant inequality constraints on the network outputsof unlabeled target samples. It implicitly matches prediction statisticsbetween target and source domains with permitted uncertainty of priorknowledge. We address our constrained problem with a differentiable penalty,fully suited for standard stochastic gradient descent approaches, removing theneed for computationally expensive Lagrangian optimization with dualprojections. Unlike current two-step adversarial training, our formulation isbased on a single loss in a single network, which simplifies adaptation byavoiding extra adversarial steps, while improving convergence and quality oftraining.  The comparison of our approach with state-of-the-art adversarial methodsreveals substantially better performance on the challenging task of adaptingspine segmentation across different MRI modalities. Our results also show arobustness to imprecision of size priors, approaching the accuracy of a fullysupervised model trained directly in a target domain.Our method can be readilyused for various constraints and segmentation problems. ", "id2": "187", "id3": "None"}
{"id": "189", "content": "Colorectal cancer (CRC) is the first cause of death in many countries. CRCoriginates from a small clump of cells on the lining of the colon calledpolyps, which over time might grow and become malignant. Early detection andremoval of polyps are therefore necessary for the prevention of colon cancer.In this paper, we introduce an ensemble of medical polyp segmentationalgorithms. Based on an observation that different segmentation algorithms willperform well on different subsets of examples because of the nature and size oftraining sets they have been exposed to and because of method-intrinsicfactors, we propose to measure the confidence in the prediction of eachalgorithm and then use an associate threshold to determine whether theconfidence is acceptable or not. An algorithm is selected for the ensemble ifthe confidence is below its associate threshold. The optimal threshold for eachsegmentation algorithm is found by using Comprehensive Learning Particle SwarmOptimization (CLPSO), a swarm intelligence algorithm. The Dice coefficient, apopular performance metric for image segmentation, is used as the fitnesscriteria. Experimental results on two polyp segmentation datasets MICCAI2015and Kvasir-SEG confirm that our ensemble achieves better results compared tosome well-known segmentation algorithms. ", "id2": "188", "id3": "None"}
{"id": "190", "content": "In recent years, deep learning has rapidly become a method of choice for thesegmentation of medical images. Deep Neural Network (DNN) architectures such asUNet have achieved state-of-the-art results on many medical datasets. Tofurther improve the performance in the segmentation task, we develop anensemble system which combines various deep learning architectures. We proposea two-layer ensemble of deep learning models for the segmentation of medicalimages. The prediction for each training image pixel made by each model in thefirst layer is used as the augmented data of the training image for the secondlayer of the ensemble. The prediction of the second layer is then combined byusing a weights-based scheme in which each model contributes differently to thecombined result. The weights are found by solving linear regression problems.Experiments conducted on two popular medical datasets namely CAMUS andKvasir-SEG show that the proposed method achieves better results concerning twoperformance metrics (Dice Coefficient and Hausdorff distance) compared to somewell-known benchmark algorithms. ", "id2": "189", "id3": "None"}
{"id": "191", "content": "Semantic segmentation has a wide array of applications ranging frommedical-image analysis, scene understanding, autonomous driving and roboticnavigation. This work deals with medical image segmentation and in particularwith accurate polyp detection and segmentation during colonoscopy examinations.Several convolutional neural network architectures have been proposed toeffectively deal with this task and with the problem of segmenting objects atdifferent scale input. The basic architecture in image segmentation consists ofan encoder and a decoder: the first uses convolutional filters to extractfeatures from the image, the second is responsible for generating the finaloutput. In this work, we compare some variant of the DeepLab architectureobtained by varying the decoder backbone. We compare several decoderarchitectures, including ResNet, Xception, EfficentNet, MobileNet and weperturb their layers by substituting ReLU activation layers with otherfunctions. The resulting methods are used to create deep ensembles which areshown to be very effective. Our experimental evaluations show that our bestensemble produces good segmentation results by achieving high evaluation scoreswith a dice coefficient of 0.884, and a mean Intersection over Union (mIoU) of0.818 for the Kvasir-SEG dataset. To improve reproducibility and researchefficiency the MATLAB source code used for this research is available atGitHub: https://github.com/LorisNanni. ", "id2": "190", "id3": "None"}
{"id": "192", "content": "Domain adaptation (DA) has drawn high interests for its capacity to adapt amodel trained on labeled source data to perform well on unlabeled or weaklylabeled target data from a different domain. Most common DA techniques requirethe concurrent access to the input images of both the source and targetdomains. However, in practice, it is common that the source images are notavailable in the adaptation phase. This is a very frequent DA scenario inmedical imaging, for instance, when the source and target images come fromdifferent clinical sites. We propose a novel formulation for adaptingsegmentation networks, which relaxes such a constraint. Our formulation isbased on minimizing a label-free entropy loss defined over target-domain data,which we further guide with a domain invariant prior on the segmentationregions. Many priors can be used, derived from anatomical information. Here, aclass-ratio prior is learned via an auxiliary network and integrated in theform of a Kullback-Leibler (KL) divergence in our overall loss function. Weshow the effectiveness of our prior-aware entropy minimization in adaptingspine segmentation across different MRI modalities. Our method yieldscomparable results to several state-of-the-art adaptation techniques, eventhough is has access to less information, the source images being absent in theadaptation phase. Our straight-forward adaptation strategy only uses onenetwork, contrary to popular adversarial techniques, which cannot performwithout the presence of the source images. Our framework can be readily usedwith various priors and segmentation problems. ", "id2": "191", "id3": "None"}
{"id": "193", "content": "Computer vision has shown promising results in medical image processing.Pneumothorax is a deadly condition and if not diagnosed and treated at timethen it causes death. It can be diagnosed with chest X-ray images. We need anexpert and experienced radiologist to predict whether a person is sufferingfrom pneumothorax or not by looking at the chest X-ray images. Everyone doesnot have access to such a facility. Moreover, in some cases, we need quickdiagnoses. So we propose an image segmentation model to predict and give theoutput a mask that will assist the doctor in taking this crucial decision. DeepLearning has proved their worth in many areas and outperformed manstate-of-the-art models. We want to use the power of these deep learning modelto solve this problem. We have used U-net [13] architecture with ResNet [17] asa backbone and achieved promising results. U-net [13] performs very well inmedical image processing and semantic segmentation. Our problem falls in thesemantic segmentation category. ", "id2": "192", "id3": "None"}
{"id": "194", "content": "Understanding the structure of Earths polar ice sheets is important formodeling how global warming will impact polar ice and, in turn, the Earthsclimate. Ground-penetrating radar is able to collect observations of theinternal structure of snow and ice, but the process of manually labeling theseobservations is slow and laborious. Recent work has developed automatictechniques for finding the boundaries between the ice and the bedrock, butfinding internal layers - the subtle boundaries that indicate where one yearsice accumulation ended and the next began - is much more challenging becausethe number of layers varies and the boundaries often merge and split. In thispaper, we propose a novel deep neural network for solving a general class oftiered segmentation problems. We then apply it to detecting internal layers inpolar ice, evaluating on a large-scale dataset of polar ice radar data withhuman-labeled annotations as ground truth. ", "id2": "193", "id3": "None"}
{"id": "195", "content": "The importance of hierarchical image organization has been witnessed by awide spectrum of applications in computer vision and graphics. Different fromimage segmentation with the spatial whole-part consideration, this work designsa modern framework for disassembling an image into a family of derived signalsfrom a scale-space perspective. Specifically, we first offer a formaldefinition of image disassembly. Then, by concerning desired properties, suchas peeling hierarchy and structure preservation, we convert the originalcomplex problem into a series of two-component separation sub-problems,significantly reducing the complexity. The proposed framework is flexible toboth supervised and unsupervised settings. A compact recurrent network, namelyhierarchical image peeling net, is customized to efficiently and effectivelyfulfill the task, which is about 3.5Mb in size, and can handle 1080p images inmore than 60 fps per recurrence on a GTX 2080Ti GPU, making it attractive forpractical use. Both theoretical findings and experimental results are providedto demonstrate the efficacy of the proposed framework, reveal its superiorityover other state-of-the-art alternatives, and show its potential to variousapplicable scenarios. Our code is available at url https://github.com/ForawardStar/HIPe . ", "id2": "194", "id3": "None"}
{"id": "196", "content": "We present ObSuRF, a method which turns a single image of a scene into a 3Dmodel represented as a set of Neural Radiance Fields (NeRFs), with each NeRFcorresponding to a different object. A single forward pass of an encodernetwork outputs a set of latent vectors describing the objects in the scene.These vectors are used independently to condition a NeRF decoder, defining thegeometry and appearance of each object. We make learning more computationallyefficient by deriving a novel loss, which allows training NeRFs on RGB-D inputswithout explicit ray marching. After confirming that the model performs equalor better than state of the art on three 2D image segmentation benchmarks, weapply it to two multi-object 3D datasets: A multiview version of CLEVR, and anovel dataset in which scenes are populated by ShapeNet models. We find thatafter training ObSuRF on RGB-D views of training scenes, it is capable of notonly recovering the 3D geometry of a scene depicted in a single input image,but also to segment it into objects, despite receiving no supervision in thatregard. ", "id2": "195", "id3": "None"}
{"id": "197", "content": "Deep Neural Networks (DNNs) have become ubiquitous in medical imageprocessing and analysis. Among them, U-Nets are very popular in various imagesegmentation tasks. Yet, little is known about how information flows throughthese networks and whether they are indeed properly designed for the tasks theyare being proposed for. In this paper, we employ information-theoretic tools inorder to gain insight into information flow through U-Nets. In particular, weshow how mutual information between input/output and an intermediate layer canbe a useful tool to understand information flow through various portions of aU-Net, assess its architectural efficiency, and even propose more efficientdesigns. ", "id2": "196", "id3": "None"}
{"id": "198", "content": "MeanShift is a popular mode-seeking clustering algorithm used in a wide rangeof applications in machine learning. However, it is known to be prohibitivelyslow, with quadratic runtime per iteration. We propose MeanShift++, anextremely fast mode-seeking algorithm based on MeanShift that uses a grid-basedapproach to speed up the mean shift step, replacing the computationallyexpensive neighbors search with a density-weighted mean of adjacent grid cells.In addition, we show that this grid-based technique for density estimationcomes with theoretical guarantees. The runtime is linear in the number ofpoints and exponential in dimension, which makes MeanShift++ ideal onlow-dimensional applications such as image segmentation and object tracking. Weprovide extensive experimental analysis showing that MeanShift++ can be morethan 10,000x faster than MeanShift with competitive clustering results onbenchmark datasets and nearly identical image segmentations as MeanShift.Finally, we show promising results for object tracking. ", "id2": "197", "id3": "None"}
{"id": "199", "content": "With the increase in available large clinical and experimental datasets,there has been substantial amount of work being done on addressing thechallenges in the area of biomedical image analysis. Image segmentation, whichis crucial for any quantitative analysis, has especially attracted attention.Recent hardware advancement has led to the success of deep learning approaches.However, although deep learning models are being trained on large datasets,existing methods do not use the information from different learning epochseffectively. In this work, we leverage the information of each training epochto prune the prediction maps of the subsequent epochs. We propose a novelarchitecture called feedback attention network (FANet) that unifies theprevious epoch mask with the feature map of the current training epoch. Theprevious epoch mask is then used to provide a hard attention to the learntfeature maps at different convolutional layers. The network also allows torectify the predictions in an iterative fashion during the test time. We showthat our proposed feedback attention model provides a substantial improvementon most segmentation metrics tested on seven publicly available biomedicalimaging datasets demonstrating the effectiveness of the proposed FANet. ", "id2": "198", "id3": "None"}
{"id": "200", "content": "While state of the art image segmentation models typically outputsegmentations in raster format, applications in geographic information systemsoften require vector polygons. To help bridge the gap between deep networkoutput and the format used in downstream tasks, we add a frame field output toa deep segmentation model for extracting buildings from remote sensing images.We train a deep neural network that aligns a predicted frame field to groundtruth contours. This additional objective improves segmentation quality byleveraging multi-task learning and provides structural information that laterfacilitates polygonization; we also introduce a polygonization algorithm thatutilizes the frame field along with the raster segmentation. Our code isavailable at https://github.com/Lydorn/Polygonization-by-Frame-Field-Learning. ", "id2": "199", "id3": "None"}
{"id": "201", "content": "Image segmentation is to extract meaningful objects from a given image. Fordegraded images due to occlusions, obscurities or noises, the accuracy of thesegmentation result can be severely affected. To alleviate this problem, priorinformation about the target object is usually introduced. In [10], atopology-preserving registration-based segmentation model was proposed, whichis restricted to segment 2D images only. In this paper, we propose a novel 3Dtopology-preserving registration-based segmentation model with the hyperelasticregularization, which can handle both 2D and 3D images. The existence of thesolution of the proposed model is established. We also propose a convergingiterative scheme to solve the proposed model. Numerical experiments have beencarried out on the synthetic and real images, which demonstrate theeffectiveness of our proposed model. ", "id2": "200", "id3": "None"}
{"id": "202", "content": "In the recent years, researchers proposed a number of successful methods toperform out-of-distribution (OOD) detection in deep neural networks (DNNs). Sofar the scope of the highly accurate methods has been limited to image levelclassification tasks. However, attempts for generally applicable methods beyondclassification did not attain similar performance. In this paper, we addressthis limitation by proposing a simple yet effective task-agnostic OOD detectionmethod. We estimate the probability density functions (pdfs) of intermediatefeatures of a pre-trained DNN by performing kernel density estimation (KDE) onthe training dataset. As direct application of KDE to feature maps is hinderedby their high dimensionality, we use a set of lower-dimensional marginalizedKDE models instead of a single high-dimensional one. At test time, we evaluatethe pdfs on a test sample and produce a confidence score that indicates thesample is OOD. The use of KDE eliminates the need for making simplifyingassumptions about the underlying feature pdfs and makes the proposed methodtask-agnostic. We perform extensive experiments on classification tasks usingbenchmark datasets for OOD detection. Additionally, we perform experiments onmedical image segmentation tasks using brain MRI datasets. The resultsdemonstrate that the proposed method consistently achieves high OOD detectionperformance in both classification and segmentation tasks and improvesstate-of-the-art in almost all cases. Code is available at url https://github.com/eerdil/task_agnostic_ood  ", "id2": "201", "id3": "None"}
{"id": "203", "content": "We present Boundary IoU (Intersection-over-Union), a new segmentationevaluation measure focused on boundary quality. We perform an extensiveanalysis across different error types and object sizes and show that BoundaryIoU is significantly more sensitive than the standard Mask IoU measure toboundary errors for large objects and does not over-penalize errors on smallerobjects. The new quality measure displays several desirable characteristicslike symmetry w.r.t. prediction/ground truth pairs and balanced responsivenessacross scales, which makes it more suitable for segmentation evaluation thanother boundary-focused measures like Trimap IoU and F-measure. Based onBoundary IoU, we update the standard evaluation protocols for instance andpanoptic segmentation tasks by proposing the Boundary AP (Average Precision)and Boundary PQ (Panoptic Quality) metrics, respectively. Our experiments showthat the new evaluation metrics track boundary quality improvements that aregenerally overlooked by current Mask IoU-based evaluation metrics. We hope thatthe adoption of the new boundary-sensitive evaluation metrics will lead torapid progress in segmentation methods that improve boundary quality. ", "id2": "202", "id3": "None"}
{"id": "204", "content": "Referring image segmentation aims to segment the objects referred by anatural language expression. Previous methods usually focus on designing animplicit and recurrent feature interaction mechanism to fuse thevisual-linguistic features to directly generate the final segmentation maskwithout explicitly modeling the localization information of the referentinstances. To tackle these problems, we view this task from another perspectiveby decoupling it into a Locate-Then-Segment (LTS) scheme. Given a languageexpression, people generally first perform attention to the correspondingtarget image regions, then generate a fine segmentation mask about the objectbased on its context. The LTS first extracts and fuses both visual and textualfeatures to get a cross-modal representation, then applies a cross-modelinteraction on the visual-textual features to locate the referred object withposition prior, and finally generates the segmentation result with alight-weight segmentation network. Our LTS is simple but surprisinglyeffective. On three popular benchmark datasets, the LTS outperforms all theprevious state-of-the-art methods by a large margin (e.g., +3.2% on RefCOCO+and +3.4% on RefCOCOg). In addition, our model is more interpretable withexplicitly locating the object, which is also proved by visualizationexperiments. We believe this framework is promising to serve as a strongbaseline for referring image segmentation. ", "id2": "203", "id3": "None"}
{"id": "205", "content": "Probabilistic image segmentation encodes varying prediction confidence andinherent ambiguity in the segmentation problem. While different probabilisticsegmentation models are designed to capture different aspects of segmentationuncertainty and ambiguity, these modelling differences are rarely discussed inthe context of applications of uncertainty. We consider two common use cases ofsegmentation uncertainty, namely assessment of segmentation quality and activelearning. We consider four established strategies for probabilisticsegmentation, discuss their modelling capabilities, and investigate theirperformance in these two tasks. We find that for all models and both tasks,returned uncertainty correlates positively with segmentation error, but doesnot prove to be useful for active learning. ", "id2": "204", "id3": "None"}
{"id": "206", "content": "Few-shot segmentation has been attracting a lot of attention due to itseffectiveness to segment unseen object classes with a few annotated samples.Most existing approaches use masked Global Average Pooling (GAP) to encode anannotated support image to a feature vector to facilitate query imagesegmentation. However, this pipeline unavoidably loses some discriminativeinformation due to the average operation. In this paper, we propose a simplebut effective self-guided learning approach, where the lost criticalinformation is mined. Specifically, through making an initial prediction forthe annotated support image, the covered and uncovered foreground regions areencoded to the primary and auxiliary support vectors using masked GAP,respectively. By aggregating both primary and auxiliary support vectors, bettersegmentation performances are obtained on query images. Enlightened by ourself-guided module for 1-shot segmentation, we propose a cross-guided modulefor multiple shot segmentation, where the final mask is fused using predictionsfrom multiple annotated samples with high-quality support vectors contributingmore and vice versa. This module improves the final prediction in the inferencestage without re-training. Extensive experiments show that our approachachieves new state-of-the-art performances on both PASCAL-5i and COCO-20idatasets. ", "id2": "205", "id3": "None"}
{"id": "207", "content": "Recently, neural architecture search (NAS) has been applied to automaticallysearch high-performance networks for medical image segmentation. The NAS searchspace usually contains a network topology level (controlling connections amongcells with different spatial scales) and a cell level (operations within eachcell). Existing methods either require long searching time for large-scale 3Dimage datasets, or are limited to pre-defined topologies (such as U-shaped orsingle-path). In this work, we focus on three important aspects of NAS in 3Dmedical image segmentation: flexible multi-path network topology, high searchefficiency, and budgeted GPU memory usage. A novel differentiable searchframework is proposed to support fast gradient-based search within a highlyflexible network topology search space. The discretization of the searchedoptimal continuous model in differentiable scheme may produce a sub-optimalfinal discrete model (discretization gap). Therefore, we propose a topologyloss to alleviate this problem. In addition, the GPU memory usage for thesearched 3D model is limited with budget constraints during search. OurDifferentiable Network Topology Search scheme (DiNTS) is evaluated on theMedical Segmentation Decathlon (MSD) challenge, which contains ten challengingsegmentation tasks. Our method achieves the state-of-the-art performance andthe top ranking on the MSD challenge leaderboard. ", "id2": "206", "id3": "None"}
{"id": "208", "content": "Transformers are increasingly dominating multi-modal reasoning tasks, such asvisual question answering, achieving state-of-the-art results thanks to theirability to contextualize information using the self-attention and co-attentionmechanisms. These attention modules also play a role in other computer visiontasks including object detection and image segmentation. Unlike Transformersthat only use self-attention, Transformers with co-attention require toconsider multiple attention maps in parallel in order to highlight theinformation that is relevant to the prediction in the models input. In thiswork, we propose the first method to explain prediction by anyTransformer-based architecture, including bi-modal Transformers andTransformers with co-attentions. We provide generic solutions and apply theseto the three most commonly used of these architectures: (i) pureself-attention, (ii) self-attention combined with co-attention, and (iii)encoder-decoder attention. We show that our method is superior to all existingmethods which are adapted from single modality explainability. ", "id2": "207", "id3": "None"}
{"id": "209", "content": "Neural Architecture Search (NAS) has shown great potentials in automaticallydesigning scalable network architectures for dense image predictions. However,existing NAS algorithms usually compromise on restricted search space andsearch on proxy task to meet the achievable computational demands. To allow aswide as possible network architectures and avoid the gap between target andproxy dataset, we propose a Densely Connected NAS (DCNAS) framework, whichdirectly searches the optimal network structures for the multi-scalerepresentations of visual information, over a large-scale target dataset.Specifically, by connecting cells with each other using learnable weights, weintroduce a densely connected search space to cover an abundance of mainstreamnetwork designs. Moreover, by combining both path-level and channel-levelsampling strategies, we design a fusion module to reduce the memory consumptionof ample search space. We demonstrate that the architecture obtained from ourDCNAS algorithm achieves state-of-the-art performances on public semantic imagesegmentation benchmarks, including 84.3% on Cityscapes, and 86.9% on PASCAL VOC2012. We also retain leading performances when evaluating the architecture onthe more challenging ADE20K and Pascal Context dataset. ", "id2": "208", "id3": "None"}
{"id": "210", "content": "Semantic and instance segmentation algorithms are two general yet distinctimage segmentation solutions powered by Convolution Neural Network. Whilesemantic segmentation benefits extensively from the end-to-end trainingstrategy, instance segmentation is frequently framed as a multi-stage task,supported by learning-based discrimination and post-process clustering.Independent optimizations on substages instigate the accumulation ofsegmentation errors. In this work, we propose to embed prior clusteringinformation into an embedding learning framework FCRNet, stimulating theone-stage instance segmentation. FCRNet relieves the complexity of post processby incorporating the number of clustering groups into the embedding space. Thesuperior performance of FCRNet is verified and compared with other methods onthe nucleus dataset BBBC006. ", "id2": "209", "id3": "None"}
{"id": "211", "content": "Large, fine-grained image segmentation datasets, annotated at pixel-level,are difficult to obtain, particularly in medical imaging, where annotationsalso require expert knowledge. Weakly-supervised learning can train models byrelying on weaker forms of annotation, such as scribbles. Here, we learn tosegment using scribble annotations in an adversarial game. With unpairedsegmentation masks, we train a multi-scale GAN to generate realisticsegmentation masks at multiple resolutions, while we use scribbles to learntheir correct position in the image. Central to the models success is a novelattention gating mechanism, which we condition with adversarial signals to actas a shape prior, resulting in better object localization at multiple scales.Subject to adversarial conditioning, the segmentor learns attention maps thatare semantic, suppress the noisy activations outside the objects, and reducethe vanishing gradient problem in the deeper layers of the segmentor. Weevaluated our model on several medical (ACDC, LVSC, CHAOS) and non-medical(PPSS) datasets, and we report performance levels matching those achieved bymodels trained with fully annotated segmentation masks. We also demonstrateextensions in a variety of settings: semi-supervised learning; combiningmultiple scribble sources (a crowdsourcing scenario) and multi-task learning(combining scribble and mask supervision). We release expert-made scribbleannotations for the ACDC dataset, and the code used for the experiments, athttps://vios-s.github.io/multiscale-adversarial-attention-gates ", "id2": "210", "id3": "None"}
{"id": "212", "content": "Conventional deformable registration methods aim at solving an optimizationmodel carefully designed on image pairs and their computational costs areexceptionally high. In contrast, recent deep learning based approaches canprovide fast deformation estimation. These heuristic network architectures arefully data-driven and thus lack explicit geometric constraints, e.g.,topology-preserving, which are indispensable to generate plausibledeformations. We design a new deep learning based framework to optimize adiffeomorphic model via multi-scale propagation in order to integrateadvantages and avoid limitations of these two categories of approaches.Specifically, we introduce a generic optimization model to formulatediffeomorphic registration and develop a series of learnable architectures toobtain propagative updating in the coarse-to-fine feature space. Moreover, wepropose a novel bilevel self-tuned training strategy, allowing efficient searchof task-specific hyper-parameters. This training strategy increases theflexibility to various types of data while reduces computational and humanburdens. We conduct two groups of image registration experiments on 3D volumedatasets including image-to-atlas registration on brain MRI data andimage-to-image registration on liver CT data. Extensive results demonstrate thestate-of-the-art performance of the proposed method with diffeomorphicguarantee and extreme efficiency. We also apply our framework to challengingmulti-modal image registration, and investigate how our registration to supportthe down-streaming tasks for medical image analysis including multi-modalfusion and image segmentation. ", "id2": "211", "id3": "None"}
{"id2": 1021, "id3": "211", "content": "Conventional deformable registration methods aim at solving an optimizationmodel carefully designed on image pairs and their computational costs areexceptionally high. In contrast, recent deep learning based approaches canprovide fast deformation estimation. These heuristic network architectures arefully data-driven and thus lack explicit geometric constraints, e.g.,topology-preserving, which are indispensable to generate plausibledeformations. We design a new deep learning based framework to optimize adiffeomorphic model via multi-scale propagation in order to integrateadvantages and avoid limitations of these two categories of approaches.Specifically, we introduce a generic optimization model to formulatediffeomorphic registration and develop a series of learnable architectures toobtain propagative updating in the coarse-to-fine feature space. Moreover, wepropose a novel bilevel self-tuned training strategy, allowing efficient searchof task-specific hyper-parameters. This training strategy increases theflexibility to various types of data while reduces computational and humanburdens. We conduct two groups of image registration experiments on 3D volumedatasets including image-to-atlas registration on brain MRI data andimage-to-image registration on liver CT data. Extensive results demonstrate thestate-of-the-art performance of the proposed method with diffeomorphicguarantee and extreme efficiency. We also apply our framework to challengingmulti-modal image registration, and investigate how our registration to supportthe down-streaming tasks for medical image analysis including multi-modalfusion and image segmentation."}
{"id": "213", "content": "Registration is a fundamental task in medical robotics and is often a crucialstep for many downstream tasks such as motion analysis, intra-operativetracking and image segmentation. Popular registration methods such as ANTs andNiftyReg optimize objective functions for each pair of images from scratch,which are time-consuming for 3D and sequential images with complexdeformations. Recently, deep learning-based registration approaches such asVoxelMorph have been emerging and achieve competitive performance. In thiswork, we construct a test-time training for deep deformable image registrationto improve the generalization ability of conventional learning-basedregistration model. We design multi-scale deep networks to consecutively modelthe residual deformations, which is effective for high variationaldeformations. Extensive experiments validate the effectiveness of multi-scaledeep registration with test-time training based on Dice coefficient for imagesegmentation and mean square error (MSE), normalized local cross-correlation(NLCC) for tissue dense tracking tasks. Two videos are inhttps://www.youtube.com/watch?v=NvLrCaqCiAE andhttps://www.youtube.com/watch?v=pEA6ZmtTNuQ ", "id2": "212", "id3": "None"}
{"id": "214", "content": "This work explores how to design a single neural network that is capable ofadapting to multiple heterogeneous tasks of computer vision, such as imagesegmentation, 3D detection, and video recognition. This goal is challengingbecause network architecture designs in different tasks are inconsistent. Wesolve this challenge by proposing Network Coding Propagation (NCP), a novelneural predictor, which is able to predict an architectures performance inmultiple datasets and tasks. Unlike prior arts of neural architecture search(NAS) that typically focus on a single task, NCP has several unique benefits.(1) NCP can be trained on different NAS benchmarks, such as NAS-Bench-201 andNAS-Bench-MR, which contains a novel network space designed by us for jointlysearching an architecture among multiple tasks, including ImageNet, Cityscapes,KITTI, and HMDB51. (2) NCP learns from network codes but not original data,enabling it to update the architecture efficiently across datasets. (3)Extensive experiments evaluate NCP on object classification, detection,segmentation, and video recognition. For example, with 17 % fewer FLOPs, asingle architecture returned by NCP achieves 86 % and 77.16 % onImageNet-50-1000 and Cityscapes respectively, outperforming its counterparts.More interestingly, NCP enables a single architecture applicable to both imagesegmentation and video recognition, which achieves competitive performance onboth HMDB51 and ADE20K compared to the singular counterparts. Code is availableat https://github.com/dingmyu/NCP  https://github.com/dingmyu/NCP. ", "id2": "213", "id3": "None"}
{"id": "215", "content": "Fluorescence microscopy images play the critical role of capturing spatial orspatiotemporal information of biomedical processes in life sciences. Theirsimple structures and semantics provide unique advantages in elucidatinglearning behavior of deep neural networks (DNNs). It is generally assumed thataccurate image annotation is required to train DNNs for accurate imagesegmentation. In this study, however, we find that DNNs trained by label imagesin which nearly half (49%) of the binary pixel labels are randomly flippedprovide largely the same segmentation performance. This suggests that DNNslearn high-level structures rather than pixel-level labels per se to segmentfluorescence microscopy images. We refer to these structures asmeta-structures. In support of the existence of the meta-structures, when DNNsare trained by a series of label images with progressively less meta-structureinformation, we find progressive degradation in their segmentation performance.Motivated by the learning behavior of DNNs trained by random labels and thecharacteristics of meta-structures, we propose an unsupervised segmentationmodel. Experiments show that it achieves remarkably competitive performance incomparison to supervised segmentation models. ", "id2": "214", "id3": "None"}
{"id": "216", "content": "Deep Metric Learning (DML) is helpful in computer vision tasks. In thispaper, we firstly introduce DML into image co-segmentation. We propose a novelTriplet loss for Image Segmentation, called IS-Triplet loss for short, andcombine it with traditional image segmentation loss. Different from the generalDML task which learns the metric between pictures, we treat each pixel as asample, and use their embedded features in high-dimensional space to formtriples, then we tend to force the distance between pixels of differentcategories greater than of the same category by optimizing IS-Triplet loss sothat the pixels from different categories are easier to be distinguished in thehigh-dimensional feature space. We further present an efficient triple samplingstrategy to make a feasible computation of IS-Triplet loss. Finally, theIS-Triplet loss is combined with 3 traditional image segmentation losses toperform image segmentation. We apply the proposed approach to imageco-segmentation and test it on the SBCoseg dataset and the Internet dataset.The experimental result shows that our approach can effectively improve thediscrimination of pixels categories in high-dimensional space and thus helptraditional loss achieve better performance of image segmentation with fewertraining epochs. ", "id2": "215", "id3": "None"}
{"id": "217", "content": "Confused about renovating your space? Choosing the perfect color for yourwalls is always a challenging task. One does rounds of color consultation andseveral patch tests. This paper proposes an AI tool to pitch paint based onattributes of your room and other furniture, and visualize it on your walls. Itmakes the color selection process easy. It takes in images of a room, detectsfurniture objects using YOLO object detection. Once these objects have beendetected, the tool picks out color of the object. Later this object specificinformation gets appended to the room attributes (room_type, room_size,preferred_tone, etc) and a deep neural net is trained to make predictions forcolor/texture/wallpaper for the walls. Finally, these predictions arevisualized on the walls from the images provided. The idea is to take theknowledge of a color consultant and pitch colors that suit the walls andprovide a good contrast with the furniture and harmonize with different colorsin the room. Transfer learning for YOLO object detection from the COCO datasetwas used as a starting point and the weights were later fine-tuned by trainingon additional images. The model was trained on 1000 records listing the roomand furniture attributes, to predict colors. Given the room image, this methodfinds the best color scheme for the walls. These predictions are thenvisualized on the walls in the image using image segmentation. The results arevisually appealing and automatically enhance the color look-and-feel. ", "id2": "216", "id3": "None"}
{"id": "218", "content": "Despite the tremendous success of deep neural networks in medical imagesegmentation, they typically require a large amount of costly, expert-levelannotated data. Few-shot segmentation approaches address this issue by learningto transfer knowledge from limited quantities of labeled examples.Incorporating appropriate prior knowledge is critical in designinghigh-performance few-shot segmentation algorithms. Since strong spatial priorsexist in many medical imaging modalities, we propose a prototype-based method-- namely, the location-sensitive local prototype network -- that leveragesspatial priors to perform few-shot medical image segmentation. Our approachdivides the difficult problem of segmenting the entire image with globalprototypes into easily solvable subproblems of local region segmentation withlocal prototypes. For organ segmentation experiments on the VISCERAL CT imagedataset, our method outperforms the state-of-the-art approaches by 10% in themean Dice coefficient. Extensive ablation studies demonstrate the substantialbenefits of incorporating spatial information and confirm the effectiveness ofour approach. ", "id2": "217", "id3": "None"}
{"id": "219", "content": "In the segmentation of fine-scale structures from natural and biomedicalimages, per-pixel accuracy is not the only metric of concern. Topologicalcorrectness, such as vessel connectivity and membrane closure, is crucial fordownstream analysis tasks. In this paper, we propose a new approach to traindeep image segmentation networks for better topological accuracy. Inparticular, leveraging the power of discrete Morse theory (DMT), we identifyglobal structures, including 1D skeletons and 2D patches, which are importantfor topological accuracy. Trained with a novel loss based on these globalstructures, the network performance is significantly improved especially neartopologically challenging locations (such as weak spots of connections andmembranes). On diverse datasets, our method achieves superior performance onboth the DICE score and topological metrics. ", "id2": "218", "id3": "None"}
{"id": "220", "content": "Medical image segmentation is a relevant task as it serves as the first stepfor several diagnosis processes, thus it is indispensable in clinical usage.Whilst major success has been reported using supervised techniques, they assumea large and well-representative labelled set. This is a strong assumption inthe medical domain where annotations are expensive, time-consuming, andinherent to human bias. To address this problem, unsupervised techniques havebeen proposed in the literature yet it is still an open problem due to thedifficulty of learning any transformation pattern. In this work, we present anovel optimisation model framed into a new CNN-based contrastive registrationarchitecture for unsupervised medical image segmentation. The core of ourapproach is to exploit image-level registration and feature-level from acontrastive learning mechanism, to perform registration-based segmentation.Firstly, we propose an architecture to capture the image-to-imagetransformation pattern via registration for unsupervised medical imagesegmentation. Secondly, we embed a contrastive learning mechanism into theregistration architecture to enhance the discriminating capacity of the networkin the feature-level. We show that our proposed technique mitigates the majordrawbacks of existing unsupervised techniques. We demonstrate, throughnumerical and visual experiments, that our technique substantially outperformsthe current state-of-the-art unsupervised segmentation methods on two majormedical image datasets. ", "id2": "219", "id3": "None"}
{"id": "221", "content": "Recent advances in appearance-based models have shown improved eye trackingperformance in difficult scenarios like occlusion due to eyelashes, eyelids orcamera placement, and environmental reflections on the cornea and glasses. Thekey reason for the improvement is the accurate and robust identification of eyeparts (pupil, iris, and sclera regions). The improved accuracy often comes atthe cost of labeling an enormous dataset, which is complex and time-consuming.This work presents two semi-supervised learning frameworks to identifyeye-parts by taking advantage of unlabeled images where labeled datasets arescarce. With these frameworks, leveraging the domain-specific augmentation andnovel spatially varying transformations for image segmentation, we showimproved performance on various test cases. For instance, for a model trainedon just 48 labeled images, these frameworks achieved an improvement of 0.38%and 0.65% in segmentation performance over the baseline model, which is trainedonly with the labeled dataset. ", "id2": "220", "id3": "None"}
{"id": "222", "content": "To bridge the gap between the source and target domains in unsuperviseddomain adaptation (UDA), the most common strategy puts focus on matching themarginal distributions in the feature space through adversarial learning.However, such category-agnostic global alignment lacks of exploiting theclass-level joint distributions, causing the aligned distribution lessdiscriminative. To address this issue, we propose in this paper a novel marginpreserving self-paced contrastive Learning (MPSCL) model for cross-modalmedical image segmentation. Unlike the conventional construction of contrastivepairs in contrastive learning, the domain-adaptive category prototypes areutilized to constitute the positive and negative sample pairs. With theguidance of progressively refined semantic prototypes, a novel marginpreserving contrastive loss is proposed to boost the discriminability ofembedded representation space. To enhance the supervision for contrastivelearning, more informative pseudo-labels are generated in target domain in aself-paced way, thus benefiting the category-aware distribution alignment forUDA. Furthermore, the domain-invariant representations are learned throughjoint contrastive learning between the two domains. Extensive experiments oncross-modal cardiac segmentation tasks demonstrate that MPSCL significantlyimproves semantic segmentation performance, and outperforms a wide variety ofstate-of-the-art methods by a large margin. ", "id2": "221", "id3": "None"}
{"id": "223", "content": "Deep learning models are sensitive to domain shift phenomena. A model trainedon images from one domain cannot generalise well when tested on images from adifferent domain, despite capturing similar anatomical structures. It is mainlybecause the data distribution between the two domains is different. Moreover,creating annotation for every new modality is a tedious and time-consumingtask, which also suffers from high inter- and intra- observer variability.Unsupervised domain adaptation (UDA) methods intend to reduce the gap betweensource and target domains by leveraging source domain labelled data to generatelabels for the target domain. However, current state-of-the-art (SOTA) UDAmethods demonstrate degraded performance when there is insufficient data insource and target domains. In this paper, we present a novel UDA method formulti-modal cardiac image segmentation. The proposed method is based onadversarial learning and adapts network features between source and targetdomain in different spaces. The paper introduces an end-to-end framework thatintegrates: a) entropy minimisation, b) output feature space alignment and c) anovel point-cloud shape adaptation based on the latent features learned by thesegmentation model. We validated our method on two cardiac datasets by adaptingfrom the annotated source domain, bSSFP-MRI (balanced Steady-State FreeProcession-MRI), to the unannotated target domain, LGE-MRI (Late-gadoliniumenhance-MRI), for the multi-sequence dataset; and from MRI (source) to CT(target) for the cross-modality dataset. The results highlighted that byenforcing adversarial learning in different parts of the network, the proposedmethod delivered promising performance, compared to other SOTA methods. ", "id2": "222", "id3": "None"}
{"id": "224", "content": "In this paper, we present new image segmentation methods based on hiddenMarkov random fields (HMRFs) and cuckoo search (CS) variants. HMRFs model thesegmentation problem as a minimization of an energy function. CS algorithm isone of the recent powerful optimization techniques. Therefore, five variants ofthe CS algorithm are used to compute a solution. Through tests, we conduct astudy to choose the CS variant with parameters that give good results(execution time and quality of segmentation). CS variants are evaluated andcompared with non-destructive testing (NDT) images using a misclassificationerror (ME) criterion. ", "id2": "223", "id3": "None"}
{"id": "225", "content": "We aim to estimate food portion size, a property that is strongly related tothe presence of food object in 3D space, from single monocular images underreal life setting. Specifically, we are interested in end-to-end estimation offood portion size, which has great potential in the field of personal healthmanagement. Unlike image segmentation or object recognition where annotationcan be obtained through large scale crowd sourcing, it is much more challengingto collect datasets for portion size estimation since human cannot accuratelyestimate the size of an object in an arbitrary 2D image without expertknowledge. To address such challenge, we introduce a real life food imagedataset collected from a nutrition study where the groundtruth food energy(calorie) is provided by registered dietitians, and will be made available tothe research community. We propose a deep regression process for portion sizeestimation by combining features estimated from both RGB and learned energydistribution domains. Our estimates of food energy achieved state-of-the-artwith a MAPE of 11.47%, significantly outperforms non-expert human estimates by27.56%. ", "id2": "224", "id3": "None"}
{"id": "226", "content": "We generalize a graph-based multiclass semi-supervised classificationtechnique based on diffuse interface methods to multilayer graphs. Besides thetreatment of various applications with an inherent multilayer structure, wepresent a very flexible approach that interprets high-dimensional data in alow-dimensional multilayer graph representation. Highly efficient numericalmethods involving the spectral decomposition of the corresponding differentialgraph operators as well as fast matrix-vector products based on thenonequispaced fast Fourier transform (NFFT) enable the rapid treatment of largeand high-dimensional data sets. We perform various numerical tests putting aspecial focus on image segmentation. In particular, we test the performance ofour method on data sets with up to 10 million nodes per layer as well as up to104 dimensions resulting in graphs with up to 52 layers. While all presentednumerical experiments can be run on an average laptop computer, the lineardependence per iteration step of the runtime on the network size in all stagesof our algorithm makes it scalable to even larger and higher-dimensionalproblems. ", "id2": "225", "id3": "None"}
{"id": "227", "content": "Aerial Image Segmentation is a particular semantic segmentation problem andhas several challenging characteristics that general semantic segmentation doesnot have. There are two critical issues: The one is an extremelyforeground-background imbalanced distribution, and the other is multiple smallobjects along with the complex background. Such problems make the recent denseaffinity context modeling perform poorly even compared with baselines due toover-introduced background context. To handle these problems, we propose apoint-wise affinity propagation module based on the Feature Pyramid Network(FPN) framework, named PointFlow. Rather than dense affinity learning, a sparseaffinity map is generated upon selected points between the adjacent features,which reduces the noise introduced by the background while keeping efficiency.In particular, we design a dual point matcher to select points from the salientarea and object boundaries, respectively. Experimental results on threedifferent aerial segmentation datasets suggest that the proposed method is moreeffective and efficient than state-of-the-art general semantic segmentationmethods. Especially, our methods achieve the best speed and accuracy trade-offon three aerial benchmarks. Further experiments on three general semanticsegmentation datasets prove the generality of our method. Code will be providedin (https: //github.com/lxtGH/PFSegNets). ", "id2": "226", "id3": "None"}
{"id": "228", "content": "Image segmentation, one of the most critical vision tasks, has been studiedfor many years. Most of the early algorithms are unsupervised methods, whichuse hand-crafted features to divide the image into many regions. Recently,owing to the great success of deep learning technology, CNNs based methods showsuperior performance in image segmentation. However, these methods rely on alarge number of human annotations, which are expensive to collect. In thispaper, we propose a deep unsupervised method for image segmentation, whichcontains the following two stages. First, a Superpixelwise Autoencoder(SuperAE) is designed to learn the deep embedding and reconstruct a smoothedimage, then the smoothed image is passed to generate superpixels. Second, wepresent a novel clustering algorithm called Deep Superpixel Cut (DSC), whichmeasures the deep similarity between superpixels and formulates imagesegmentation as a soft partitioning problem. Via backpropagation, DSCadaptively partitions the superpixels into perceptual regions. Experimentalresults on the BSDS500 dataset demonstrate the effectiveness of the proposedmethod. ", "id2": "227", "id3": "None"}
{"id": "229", "content": "Federated learning allows distributed medical institutions to collaborativelylearn a shared prediction model with privacy protection. While at clinicaldeployment, the models trained in federated learning can still suffer fromperformance drop when applied to completely unseen hospitals outside thefederation. In this paper, we point out and solve a novel problem setting offederated domain generalization (FedDG), which aims to learn a federated modelfrom multiple distributed source domains such that it can directly generalizeto unseen target domains. We present a novel approach, named as EpisodicLearning in Continuous Frequency Space (ELCFS), for this problem by enablingeach client to exploit multi-source data distributions under the challengingconstraint of data decentralization. Our approach transmits the distributioninformation across clients in a privacy-protecting way through an effectivecontinuous frequency space interpolation mechanism. With the transferredmulti-source distributions, we further carefully design a boundary-orientedepisodic learning paradigm to expose the local learning to domain distributionshifts and particularly meet the challenges of model generalization in medicalimage segmentation scenario. The effectiveness of our method is demonstratedwith superior performance over state-of-the-arts and in-depth ablationexperiments on two medical image segmentation tasks. The code is available athttps://github.com/liuquande/FedDG-ELCFS. ", "id2": "228", "id3": "None"}
{"id": "230", "content": "The in vitro clonogenic assay is a technique to study the ability of a cellto form a colony in a culture dish. By optical imaging, dishes with stainedcolonies can be scanned and assessed digitally. Identification, segmentationand counting of stained colonies play a vital part in high-throughput screeningand quantitative assessment of biological assays. Image processing of suchpictured/scanned assays can be affected by image/scan acquisition artifactslike background noise and spatially varying illumination, and contaminants inthe suspension medium. Although existing approaches tackle these issues, thesegmentation quality requires further improvement, particularly on noisy andlow contrast images. In this work, we present an objective and versatilemachine learning procedure to amend these issues by characterizing, extractingand segmenting inquired colonies using principal component analysis, k-meansclustering and a modified watershed segmentation algorithm. The intention is toautomatically identify visible colonies through spatial texture assessment andaccordingly discriminate them from background in preparation for successivesegmentation. The proposed segmentation algorithm yielded a similar quality asmanual counting by human observers. High F1 scores (>0.9) and lowroot-mean-square errors (around 14%) underlined good agreement with groundtruth data. Moreover, it outperformed a recent state-of-the-art method. Themethodology will be an important tool in future cancer research applications. ", "id2": "229", "id3": "None"}
{"id": "231", "content": "Convolutional Networks (ConvNets) excel at semantic segmentation and havebecome a vital component for perception in autonomous driving. Enabling anall-encompassing view of street-scenes, omnidirectional cameras presentthemselves as a perfect fit in such systems. Most segmentation models forparsing urban environments operate on common, narrow Field of View (FoV)images. Transferring these models from the domain they were designed for to360-degree perception, their performance drops dramatically, e.g., by anabsolute 30.0% (mIoU) on established test-beds. To bridge the gap in terms ofFoV and structural distribution between the imaging domains, we introduceEfficient Concurrent Attention Networks (ECANets), directly capturing theinherent long-range dependencies in omnidirectional imagery. In addition to thelearned attention-based contextual priors that can stretch across 360-degreeimages, we upgrade model training by leveraging multi-source andomni-supervised learning, taking advantage of both: Densely labeled andunlabeled data originating from multiple datasets. To foster progress inpanoramic image segmentation, we put forward and extensively evaluate models onWild PAnoramic Semantic Segmentation (WildPASS), a dataset designed to capturediverse scenes from all around the globe. Our novel model, training regimen andmulti-source prediction fusion elevate the performance (mIoU) to newstate-of-the-art results on the public PASS (60.2%) and the fresh WildPASS(69.0%) benchmarks. ", "id2": "230", "id3": "None"}
{"id": "232", "content": "Synthetic Aperture Sonar (SAS) surveys produce imagery with large regions oftransition between seabed types. Due to these regions, it is difficult to labeland segment the imagery and, furthermore, challenging to score the imagesegmentations appropriately. While there are many approaches to quantifyperformance in standard crisp segmentation schemes, drawing hard boundaries inremote sensing imagery where gradients and regions of uncertainty exist isinappropriate. These cases warrant weak labels and an associated appropriatescoring approach. In this paper, a labeling approach and associated modifiedversion of the Rand index for weakly-labeled data is introduced to addressthese issues. Results are evaluated with the new index and compared totraditional segmentation evaluation methods. Experimental results on a SAS dataset containing must-link and cannot-link labels show that our Weakly-LabeledRand index scores segmentations appropriately in reference to qualitativeperformance and is more suitable than traditional quantitative metrics forscoring weakly-labeled data. ", "id2": "231", "id3": "None"}
{"id": "233", "content": "Deep learning-based semi-supervised learning (SSL) algorithms have led topromising results in medical images segmentation and can alleviate doctorsexpensive annotations by leveraging unlabeled data. However, most of theexisting SSL algorithms in literature tend to regularize the model training byperturbing networks and/or data. Observing that multi/dual-task learningattends to various levels of information which have inherent predictionperturbation, we ask the question in this work: can we explicitly buildtask-level regularization rather than implicitly constructing networks- and/ordata-level perturbation-and-transformation for SSL? To answer this question, wepropose a novel dual-task-consistency semi-supervised framework for the firsttime. Concretely, we use a dual-task deep network that jointly predicts apixel-wise segmentation map and a geometry-aware level set representation ofthe target. The level set representation is converted to an approximatedsegmentation map through a differentiable task transform layer. Simultaneously,we introduce a dual-task consistency regularization between the levelset-derived segmentation maps and directly predicted segmentation maps for bothlabeled and unlabeled data. Extensive experiments on two public datasets showthat our method can largely improve the performance by incorporating theunlabeled data. Meanwhile, our framework outperforms the state-of-the-artsemi-supervised medical image segmentation methods. Code is available at:https://github.com/Luoxd1996/DTC ", "id2": "232", "id3": "None"}
{"id": "234", "content": "The shapes and morphology of the organs and tissues are important priorknowledge in medical imaging recognition and segmentation. The morphologicaloperation is a well-known method for morphological feature extraction. As themorphological operation is performed well in hand-crafted image segmentationtechniques, it is also promising to design an approach to approximatemorphological operation in the convolutional networks. However, using thetraditional convolutional neural network as a black-box is usually hard tospecify the morphological operation action. Here, we introduced a 3Dmorphological operation residual block to extract morphological features inend-to-end deep learning models for semantic segmentation. This study proposeda novel network block architecture that embedded the morphological operation asan infinitely strong prior in the convolutional neural network. Several 3D deeplearning models with the proposed morphological operation block were built andcompared in different medical imaging segmentation tasks. Experimental resultsshowed the proposed network achieved a relatively higher performance in thesegmentation tasks comparing with the conventional approach. In conclusion, thenovel network block could be easily embedded in traditional networks andefficiently reinforce the deep learning models for medical imagingsegmentation. ", "id2": "233", "id3": "None"}
{"id": "235", "content": "Convolutional neural networks (CNNs) have been the de facto standard fornowadays 3D medical image segmentation. The convolutional operations used inthese networks, however, inevitably have limitations in modeling the long-rangedependency due to their inductive bias of locality and weight sharing. AlthoughTransformer was born to address this issue, it suffers from extremecomputational and spatial complexities in processing high-resolution 3D featuremaps. In this paper, we propose a novel framework that efficiently bridges a  bf Co nvolutional neural network and a   bf Tr ansformer   bf (CoTr)  foraccurate 3D medical image segmentation. Under this framework, the CNN isconstructed to extract feature representations and an efficient deformableTransformer (DeTrans) is built to model the long-range dependency on theextracted feature maps. Different from the vanilla Transformer which treats allimage positions equally, our DeTrans pays attention only to a small set of keypositions by introducing the deformable self-attention mechanism. Thus, thecomputational and spatial complexities of DeTrans have been greatly reduced,making it possible to process the multi-scale and high-resolution feature maps,which are usually of paramount importance for image segmentation. We conduct anextensive evaluation on the Multi-Atlas Labeling Beyond the Cranial Vault (BCV)dataset that covers 11 major human organs. The results indicate that our CoTrleads to a substantial performance improvement over other CNN-based,transformer-based, and hybrid methods on the 3D multi-organ segmentation task.Code is available at  def UrlFont  rm small ttfamily  url https://github.com/YtongXie/CoTr  ", "id2": "234", "id3": "None"}
{"id": "236", "content": "The hatching process also influences the success of hatching eggs beside theinitial egg factor. So that the results have a large percentage of hatching, itis necessary to check the development of the embryo at the beginning of thehatching. This process aims to sort eggs that have embryos to remain hatcheduntil the end. Maximum checking is done the first week in the hatching period.This study aims to detect the presence of embryos in eggs. Detection of theexistence of embryos is processed using segmentation. Egg images are segmentedusing the K-means algorithm based on Lab color images. The results of theimages acquisition are converted into Lab color space images. The results ofLab color space images are processed using K-means for each color. The K-meansprocess uses cluster k=3, where this cluster divided the image into threeparts, namely background, eggs, and yolk eggs. Yolk eggs are part of eggs thathave embryonic characteristics. This study applies the concept of color in theinitial segmentation and grayscale in the final stages. The results of theinitial phase show that the image segmentation results using k-means clusteringbased on Lab color space provide a grouping of three parts. At the grayscaleimage processing stage, the results of color image segmentation are processedwith grayscaling, image enhancement, and morphology. Thus, it seems clear thatthe yolk segmented shows the presence of egg embryos. Based on this process andresults, K-means segmentation based on Lab color space can be used for theinitial stages of the embryo detection process. The evaluation uses MSE andMSSIM, with values of 0.0486 and 0.9979; this can be used as a reference thatthe results obtained can indicate the detection of embryos in egg yolk. ", "id2": "235", "id3": "None"}
{"id": "237", "content": "With the widespread success of deep learning in biomedical imagesegmentation, domain shift becomes a critical and challenging problem, as thegap between two domains can severely affect model performance when deployed tounseen data with heterogeneous features. To alleviate this problem, we presenta novel unsupervised domain adaptation network, for generalizing models learnedfrom the labeled source domain to the unlabeled target domain forcross-modality biomedical image segmentation. Specifically, our approachconsists of two key modules, a conditional domain discriminator~(CDD) and acategory-centric prototype aligner~(CCPA). The CDD, extended from conditionaldomain adversarial networks in classifier tasks, is effective and robust inhandling complex cross-modality biomedical images. The CCPA, improved from thegraph-induced prototype alignment mechanism in cross-domain object detection,can exploit precise instance-level features through an elaborate prototyperepresentation. In addition, it can address the negative effect of classimbalance via entropy-based loss. Extensive experiments on a public benchmarkfor the cardiac substructure segmentation task demonstrate that our methodsignificantly improves performance on the target domain. ", "id2": "236", "id3": "None"}
{"id": "238", "content": "Deep convolutional neural networks have shown outstanding performance inmedical image segmentation tasks. The usual problem when training superviseddeep learning methods is the lack of labeled data which is time-consuming andcostly to obtain. In this paper, we propose a novel uncertainty-guidedsemi-supervised learning based on a student-teacher approach for training thesegmentation network using limited labeled samples and a large number ofunlabeled images. First, a teacher segmentation model is trained from thelabeled samples using Bayesian deep learning. The trained model is used togenerate soft segmentation labels and uncertainty maps for the unlabeled set.The student model is then updated using the softly segmented samples and thecorresponding pixel-wise confidence of the segmentation quality estimated fromthe uncertainty of the teacher model using a newly designed loss function.Experimental results on a retinal layer segmentation task show that theproposed method improves the segmentation performance in comparison to thefully supervised approach and is on par with the expert annotator. The proposedsemi-supervised segmentation framework is a key contribution and applicable forbiomedical image segmentation across various imaging modalities where access toannotated medical images is challenging ", "id2": "237", "id3": "None"}
{"id": "239", "content": "In medical image diagnosis, pathology image analysis using semanticsegmentation becomes important for efficient screening as a field of digitalpathology. The spatial augmentation is ordinary used for semantic segmentation.Tumor images under malignant are rare and to annotate the labels of nucleiregion takes much time-consuming. We require an effective use of dataset tomaximize the segmentation accuracy. It is expected that some augmentation totransform generalized images influence the segmentation performance. We proposea synthetic augmentation using label-to-image translation, mapping from asemantic label with the edge structure to a real image. Exactly this paper dealwith stain slides of nuclei in tumor. Actually, we demonstrate severalsegmentation algorithms applied to the initial dataset that contains realimages and labels using synthetic augmentation in order to add theirgeneralized images. We computes and reports that a proposed syntheticaugmentation procedure improve their accuracy. ", "id2": "238", "id3": "None"}
{"id": "240", "content": "Time-series remote sensing data offer a rich source of information that canbe used in a wide range of applications, from monitoring changes in land coverto surveilling crops, coastal changes, flood risk assessment, and urban sprawl.This paper addresses the challenge of using time-series satellite images topredict urban expansion. Building upon previous work, we propose a noveltwo-step approach based on semantic image segmentation in order to predicturban expansion. The first step aims to extract information about urban regionsat different time scales and prepare them for use in the training step. Thesecond step combines Convolutional Neural Networks (CNN) with Long Short TermMemory (LSTM) methods in order to learn temporal features and thus predicturban expansion. In this paper, experimental results are conducted usingseveral multi-date satellite images representing the three largest cities inSaudi Arabia, namely: Riyadh, Jeddah, and Dammam. We empirically evaluated ourproposed technique, and examined its results by comparing them withstate-of-the-art approaches. Following this evaluation, we determined that ourresults reveal improved performance for the new-coupled CNN-LSTM approach,particularly in terms of assessments based on Mean Square Error, Root MeanSquare Error, Peak Signal to Noise Ratio, Structural Similarity Index, andoverall classification accuracy. ", "id2": "239", "id3": "None"}
{"id": "241", "content": "Despite the remarkable performance of deep learning methods on various tasks,most cutting-edge models rely heavily on large-scale annotated trainingexamples, which are often unavailable for clinical and health care tasks. Thelabeling costs for medical images are very high, especially in medical imagesegmentation, which typically requires intensive pixel/voxel-wise labeling.Therefore, the strong capability of learning and generalizing from limitedsupervision, including a limited amount of annotations, sparse annotations, andinaccurate annotations, is crucial for the successful application of deeplearning models in medical image segmentation. However, due to its intrinsicdifficulty, segmentation with limited supervision is challenging and specificmodel design and/or learning strategies are needed. In this paper, we provide asystematic and up-to-date review of the solutions above, with summaries andcomments about the methodologies. We also highlight several problems in thisfield, discussed future directions observing further investigations. ", "id2": "240", "id3": "None"}
{"id": "242", "content": "Inspired by the recent development of deep network-based methods in semanticimage segmentation, we introduce an end-to-end trainable model for face maskextraction in video sequence. Comparing to landmark-based sparse face shaperepresentation, our method can produce the segmentation masks of individualfacial components, which can better reflect their detailed shape variations. Byintegrating Convolutional LSTM (ConvLSTM) algorithm with Fully ConvolutionalNetworks (FCN), our new ConvLSTM-FCN model works on a per-sequence basis andtakes advantage of the temporal correlation in video clips. In addition, wealso propose a novel loss function, called Segmentation Loss, to directlyoptimise the Intersection over Union (IoU) performances. In practice, tofurther increase segmentation accuracy, one primary model and two additionalmodels were trained to focus on the face, eyes, and mouth regions,respectively. Our experiment shows the proposed method has achieved a 16.99%relative improvement (from 54.50% to 63.76% mean IoU) over the baseline FCNmodel on the 300 Videos in the Wild (300VW) dataset. ", "id2": "241", "id3": "None"}
{"id": "243", "content": "Considering the scarcity of medical data, most datasets in medical imageanalysis are an order of magnitude smaller than those of natural images.However, most Network Architecture Search (NAS) approaches in medical imagesfocused on specific datasets and did not take into account the generalizationability of the learned architectures on unseen datasets as well as differentdomains. In this paper, we address this point by proposing to search forgeneralizable U-shape architectures on a composited dataset that mixes medicalimages from multiple segmentation tasks and domains creatively, which is namedMixSearch. Specifically, we propose a novel approach to mix multiplesmall-scale datasets from multiple domains and segmentation tasks to produce alarge-scale dataset. Then, a novel weaved encoder-decoder structure is designedto search for a generalized segmentation network in both cell-level andnetwork-level. The network produced by the proposed MixSearch frameworkachieves state-of-the-art results compared with advanced encoder-decodernetworks across various datasets. ", "id2": "242", "id3": "None"}
{"id": "244", "content": "One-shot semantic image segmentation aims to segment the object regions forthe novel class with only one annotated image. Recent works adopt the episodictraining strategy to mimic the expected situation at testing time. However,these existing approaches simulate the test conditions too strictly during thetraining process, and thus cannot make full use of the given label information.Besides, these approaches mainly focus on the foreground-background targetclass segmentation setting. They only utilize binary mask labels for training.In this paper, we propose to leverage the multi-class label information duringthe episodic training. It will encourage the network to generate moresemantically meaningful features for each category. After integrating thetarget class cues into the query features, we then propose a pyramid featurefusion module to mine the fused features for the final classifier. Furthermore,to take more advantage of the support image-mask pair, we propose aself-prototype guidance branch to support image segmentation. It can constrainthe network for generating more compact features and a robust prototype foreach semantic class. For inference, we propose a fused prototype guidancebranch for the segmentation of the query image. Specifically, we leverage theprediction of the query image to extract the pseudo-prototype and combine itwith the initial prototype. Then we utilize the fused prototype to guide thefinal segmentation of the query image. Extensive experiments demonstrate thesuperiority of our proposed approach. ", "id2": "243", "id3": "None"}
{"id": "245", "content": "Environmental Microorganism Data Set Fifth Version (EMDS-5) is a microscopicimage dataset including original Environmental Microorganism (EM) images andtwo sets of Ground Truth (GT) images. The GT image sets include a single-objectGT image set and a multi-object GT image set. The EMDS-5 dataset has 21 typesof EMs, each of which contains 20 original EM images, 20 single-object GTimages and 20 multi-object GT images. EMDS-5 can realize to evaluate imagepreprocessing, image segmentation, feature extraction, image classification andimage retrieval functions. In order to prove the effectiveness of EMDS-5, foreach function, we select the most representative algorithms and priceindicators for testing and evaluation. The image preprocessing functionscontain two parts: image denoising and image edge detection. Image denoisinguses nine kinds of filters to denoise 13 kinds of noises, respectively. In theaspect of edge detection, six edge detection operators are used to detect theedges of the images, and two evaluation indicators, peak-signal to noise ratioand mean structural similarity, are used for evaluation. Image segmentationincludes single-object image segmentation and multi-object image segmentation.Six methods are used for single-object image segmentation, while k-means andU-net are used for multi-object segmentation.We extract nine features from theimages in EMDS-5 and use the Support Vector Machine classifier for testing. Interms of image classification, we select the VGG16 feature to test differentclassifiers. We test two types of retrieval approaches: texture featureretrieval and deep learning feature retrieval. We select the last layer offeatures of these two deep learning networks as feature vectors. We use meanaverage precision as the evaluation index for retrieval. ", "id2": "244", "id3": "None"}
{"id": "246", "content": "Class imbalance poses a challenge for developing unbiased, accuratepredictive models. In particular, in image segmentation neural networks mayoverfit to the foreground samples from small structures, which are oftenheavily under-represented in the training set, leading to poor generalization.In this study, we provide new insights on the problem of overfitting underclass imbalance by inspecting the network behavior. We find empirically thatwhen training with limited data and strong class imbalance, at test time thedistribution of logit activations may shift across the decision boundary, whilesamples of the well-represented class seem unaffected. This bias leads to asystematic under-segmentation of small structures. This phenomenon isconsistently observed for different databases, tasks and network architectures.To tackle this problem, we introduce new asymmetric variants of popular lossfunctions and regularization techniques including a large margin loss, focalloss, adversarial training, mixup and data augmentation, which are explicitlydesigned to counter logit shift of the under-represented classes. Extensiveexperiments are conducted on several challenging segmentation tasks. Ourresults demonstrate that the proposed modifications to the objective functioncan lead to significantly improved segmentation accuracy compared to baselinesand alternative approaches. ", "id2": "245", "id3": "None"}
{"id": "247", "content": "Preserving contour topology during image segmentation is useful in manypractical scenarios. By keeping the contours isomorphic, it is possible toprevent over-segmentation and under-segmentation, as well as to adhere to giventopologies. The Self-repelling Snake model (SR) is a variational model thatpreserves contour topology by combining a non-local repulsion term with thegeodesic active contour model (GAC). The SR is traditionally solved using theadditive operator splitting (AOS) scheme. In our paper, we propose analternative solution to the SR using the Split Bregman method. Our algorithmbreaks the problem down into simpler sub-problems to use lower-order evolutionequations and a simple projection scheme rather than re-initialization. Thesub-problems can be solved via fast Fourier transform (FFT) or an approximatesoft thresholding formula which maintains stability, shortening the convergencetime, and reduces the memory requirement. The Split Bregman and AOS algorithmsare compared theoretically and experimentally. ", "id2": "246", "id3": "None"}
{"id": "248", "content": "Producing manual, pixel-accurate, image segmentation labels is tedious andtime-consuming. This is often a rate-limiting factor when large amounts oflabeled images are required, such as for training deep convolutional networksfor instrument-background segmentation in surgical scenes. No large datasetscomparable to industry standards in the computer vision community are availablefor this task. To circumvent this problem, we propose to automate the creationof a realistic training dataset by exploiting techniques stemming from specialeffects and harnessing them to target training performance rather than visualappeal. Foreground data is captured by placing sample surgical instruments overa chroma key (a.k.a. green screen) in a controlled environment, thereby makingextraction of the relevant image segment straightforward. Multiple lightingconditions and viewpoints can be captured and introduced in the simulation bymoving the instruments and camera and modulating the light source. Backgrounddata is captured by collecting videos that do not contain instruments. In theabsence of pre-existing instrument-free background videos, minimal labelingeffort is required, just to select frames that do not contain surgicalinstruments from videos of surgical interventions freely available online. Wecompare different methods to blend instruments over tissue and propose a noveldata augmentation approach that takes advantage of the plurality of options. Weshow that by training a vanilla U-Net on semi-synthetic data only and applyinga simple post-processing, we are able to match the results of the same networktrained on a publicly available manually labeled real dataset. ", "id2": "247", "id3": "None"}
{"id": "249", "content": "The success of modern deep learning algorithms for image segmentation heavilydepends on the availability of large datasets with clean pixel-levelannotations (masks), where the objects of interest are accurately delineated.Lack of time and expertise during data annotation leads to incorrect boundariesand label noise. It is known that deep convolutional neural networks (DCNNs)can memorize even completely random labels, resulting in poor accuracy. Wepropose a framework to train binary segmentation DCNNs using sets of unreliablepixel-level annotations. Erroneously labeled pixels are identified based on theestimated aleatoric uncertainty of the segmentation and are relabeled to thetrue value. ", "id2": "248", "id3": "None"}
{"id": "250", "content": "Advances in image-based dietary assessment methods have allowed nutritionprofessionals and researchers to improve the accuracy of dietary assessment,where images of food consumed are captured using smartphones or wearabledevices. These images are then analyzed using computer vision methods toestimate energy and nutrition content of the foods. Food image segmentation,which determines the regions in an image where foods are located, plays animportant role in this process. Current methods are data dependent, thus cannotgeneralize well for different food types. To address this problem, we propose aclass-agnostic food image segmentation method. Our method uses a pair of eatingscene images, one before start eating and one after eating is completed. Usinginformation from both the before and after eating images, we can segment foodimages by finding the salient missing objects without any prior informationabout the food class. We model a paradigm of top down saliency which guides theattention of the human visual system (HVS) based on a task to find the salientmissing objects in a pair of images. Our method is validated on food imagescollected from a dietary study which showed promising results. ", "id2": "249", "id3": "None"}
{"id": "251", "content": "Coarse-to-fine models and cascade segmentation architectures are widelyadopted to solve the problem of large scale variations in medical imagesegmentation. However, those methods have two primary limitations: thefirst-stage segmentation becomes a performance bottleneck; the lack of overalldifferentiability makes the training process of two stages asynchronous andinconsistent. In this paper, we propose a differentiable two-stage networkarchitecture to tackle these problems. In the first stage, a localizationnetwork (L-Net) locates Regions of Interest (RoIs) in a detection fashion; inthe second stage, a segmentation network (S-Net) performs fine segmentation onthe recalibrated RoIs; a RoI recalibration module between L-Net and S-Neteliminating the inconsistencies. Experimental results on the public datasetshow that our method outperforms state-of-the-art coarse-to-fine models withnegligible computation overheads. ", "id2": "250", "id3": "None"}
{"id": "252", "content": "Deep co-training has recently been proposed as an effective approach forimage segmentation when annotated data is scarce. In this paper, we improveexisting approaches for semi-supervised segmentation with a self-paced andself-consistent co-training method. To help distillate information fromunlabeled images, we first design a self-paced learning strategy forco-training that lets jointly-trained neural networks focus oneasier-to-segment regions first, and then gradually consider harder ones.Thisis achieved via an end-to-end differentiable loss inthe form of a generalizedJensen Shannon Divergence(JSD). Moreover, to encourage predictions fromdifferent networks to be both consistent and confident, we enhance thisgeneralized JSD loss with an uncertainty regularizer based on entropy. Therobustness of individual models is further improved using a self-ensemblingloss that enforces their prediction to be consistent across different trainingiterations. We demonstrate the potential of our method on three challengingimage segmentation problems with different image modalities, using smallfraction of labeled data. Results show clear advantages in terms of performancecompared to the standard co-training baselines and recently proposedstate-of-the-art approaches for semi-supervised segmentation ", "id2": "251", "id3": "None"}
{"id2": 1022, "id3": "251", "content": "Deep co-training has recently been proposed as an effective approach forimage segmentation when annotated data is scarce. In this paper, we improveexisting approaches for semi-supervised segmentation with a self-paced andself-consistent co-training method. To help distillate information fromunlabeled images, we first design a self-paced learning strategy forco-training that lets jointly-trained neural networks focus oneasier-to-segment regions first, and then gradually consider harder ones.Thisis achieved via an end-to-end differentiable loss inthe form of a generalizedJensen Shannon Divergence(JSD). Moreover, to encourage predictions fromdifferent networks to be both consistent and confident, we enhance thisgeneralized JSD loss with an uncertainty regularizer based on entropy. Therobustness of individual models is further improved using a self-ensemblingloss that enforces their prediction to be consistent across different trainingiterations. We demonstrate the potential of our method on three challengingimage segmentation problems with different image modalities, using smallfraction of labeled data. Results show clear advantages in terms of performancecompared to the standard co-training baselines and recently proposedstate-of-the-art approaches for semi-supervised segmentation"}
{"id": "253", "content": "Medical image segmentation is an essential prerequisite for developinghealthcare systems, especially for disease diagnosis and treatment planning. Onvarious medical image segmentation tasks, the u-shaped architecture, also knownas U-Net, has become the de-facto standard and achieved tremendous success.However, due to the intrinsic locality of convolution operations, U-Netgenerally demonstrates limitations in explicitly modeling long-rangedependency. Transformers, designed for sequence-to-sequence prediction, haveemerged as alternative architectures with innate global self-attentionmechanisms, but can result in limited localization abilities due toinsufficient low-level details. In this paper, we propose TransUNet, whichmerits both Transformers and U-Net, as a strong alternative for medical imagesegmentation. On one hand, the Transformer encodes tokenized image patches froma convolution neural network (CNN) feature map as the input sequence forextracting global contexts. On the other hand, the decoder upsamples theencoded features which are then combined with the high-resolution CNN featuremaps to enable precise localization.  We argue that Transformers can serve as strong encoders for medical imagesegmentation tasks, with the combination of U-Net to enhance finer details byrecovering localized spatial information. TransUNet achieves superiorperformances to various competing methods on different medical applicationsincluding multi-organ segmentation and cardiac segmentation. Code and modelsare available at https://github.com/Beckschen/TransUNet. ", "id2": "252", "id3": "None"}
{"id": "254", "content": "Segmentation and analysis of individual pores and grains of mudrocks fromscanning electron microscope images is non-trivial because of noise, imagingartifacts, variation in pixel grayscale values across images, and overlaps ingrayscale values among different physical features such as silt grains, claygrains, and pores in an image, which make their identification difficult.Moreover, because grains and pores often have overlapping grayscale values,direct application of threshold-based segmentation techniques is notsufficient. Recent advances in the field of computer vision have made it easierand faster to segment images and identify multiple occurrences of such featuresin an image, provided that ground-truth data for training the algorithm isavailable. Here, we propose a deep learning SEM image segmentation model,MudrockNet based on Googles DeepLab-v3+ architecture implemented with theTensorFlow library. The ground-truth data was obtained from an image-processingworkflow applied to scanning electron microscope images of uncemented muds fromthe Kumano Basin offshore Japan at depths < 1.1 km. The trained deep learningmodel obtained a pixel-accuracy about 90%, and predictions for the test dataobtained a mean intersection over union (IoU) of 0.6591 for silt grains and0.6642 for pores. We also compared our model with the random forest classifierusing trainable Weka segmentation in ImageJ, and it was observed thatMudrockNet gave better predictions for both silt grains and pores. The size,concentration, and spatial arrangement of the silt and clay grains can affectthe petrophysical properties of a mudrock, and an automated method toaccurately identify the different grains and pores in mudrocks can help improvereservoir and seal characterization for petroleum exploration and anthropogenicwaste sequestration. ", "id2": "253", "id3": "None"}
{"id": "255", "content": "Supervised deep learning performance is heavily tied to the availability ofhigh-quality labels for training. Neural networks can gradually overfitcorrupted labels if directly trained on noisy datasets, leading to severeperformance degradation at test time. In this paper, we propose a novel deeplearning framework, namely Co-Seg, to collaboratively train segmentationnetworks on datasets which include low-quality noisy labels. Our approach firsttrains two networks simultaneously to sift through all samples and obtain asubset with reliable labels. Then, an efficient yet easily-implemented labelcorrection strategy is applied to enrich the reliable subset. Finally, usingthe updated dataset, we retrain the segmentation network to finalize itsparameters. Experiments in two noisy labels scenarios demonstrate that ourproposed model can achieve results comparable to those obtained from supervisedlearning trained on the noise-free labels. In addition, our framework can beeasily implemented in any segmentation algorithm to increase its robustness tonoisy labels. ", "id2": "254", "id3": "None"}
{"id": "256", "content": "We apply generative adversarial convolutional neural networks to the problemof style transfer to underdrawings and ghost-images in x-rays of fine artpaintings with a special focus on enhancing their spatial resolution. We buildupon a neural architecture developed for the related problem of synthesizinghigh-resolution photo-realistic image from semantic label maps. Our neuralarchitecture achieves high resolution through a hierarchy of generators anddiscriminator sub-networks, working throughout a range of spatial resolutions.This coarse-to-fine generator architecture can increase the effectiveresolution by a factor of eight in each spatial direction, or an overallincrease in number of pixels by a factor of 64. We also show that even just afew examples of human-generated image segmentations can greatly improve --qualitatively and quantitatively -- the generated images. We demonstrate ourmethod on works such as Leonardos Madonna of the carnation and theunderdrawing in his Virgin of the rocks, which pose several special problems instyle transfer, including the paucity of representative works from which tolearn and transfer style information. ", "id2": "255", "id3": "None"}
{"id": "257", "content": "Fluorescence microscopy images contain several channels, each indicating amarker staining the sample. Since many different marker combinations areutilized in practice, it has been challenging to apply deep learning basedsegmentation models, which expect a predefined channel combination for alltraining samples as well as at inference for future application. Recent workcircumvents this problem using a modality attention approach to be effectiveacross any possible marker combination. However, for combinations that do notexist in a labeled training dataset, one cannot have any estimation ofpotential segmentation quality if that combination is encountered duringinference. Without this, not only one lacks quality assurance but one also doesnot know where to put any additional imaging and labeling effort. We hereinpropose a method to estimate segmentation quality on unlabeled images by (i)estimating both aleatoric and epistemic uncertainties of convolutional neuralnetworks for image segmentation, and (ii) training a Random Forest model forthe interpretation of uncertainty features via regression to theircorresponding segmentation metrics. Additionally, we demonstrate that includingthese uncertainty measures during training can provide an improvement onsegmentation performance. ", "id2": "256", "id3": "None"}
{"id": "258", "content": "Semantic segmentation using convolutional neural networks (CNN) is a crucialcomponent in image analysis. Training a CNN to perform semantic segmentationrequires a large amount of labeled data, where the production of such labeleddata is both costly and labor intensive. Semi-supervised learning algorithmsaddress this issue by utilizing unlabeled data and so reduce the amount oflabeled data needed for training. In particular, data augmentation techniquessuch as CutMix and ClassMix generate additional training data from existinglabeled data. In this paper we propose a new approach for data augmentation,termed ComplexMix, which incorporates aspects of CutMix and ClassMix withimproved performance. The proposed approach has the ability to control thecomplexity of the augmented data while attempting to be semantically-correctand address the tradeoff between complexity and correctness. The proposedComplexMix approach is evaluated on a standard dataset for semanticsegmentation and compared to other state-of-the-art techniques. Experimentalresults show that our method yields improvement over state-of-the-art methodson standard datasets for semantic image segmentation. ", "id2": "257", "id3": "None"}
{"id": "259", "content": "Conventional transfer learning leverages weights of pre-trained networks, butmandates the need for similar neural architectures. Alternatively, knowledgedistillation can transfer knowledge between heterogeneous networks but oftenrequires access to the original training data or additional generativenetworks. Knowledge transfer between networks can be improved by being agnosticto the choice of network architecture and reducing the dependence on originaltraining data. We propose a knowledge transfer approach from a teacher to astudent network wherein we train the student on an independent transferaldataset, whose annotations are generated by the teacher. Experiments wereconducted on five state-of-the-art networks for semantic segmentation and sevendatasets across three imaging modalities. We studied knowledge transfer from asingle teacher, combination of knowledge transfer and fine-tuning, andknowledge transfer from multiple teachers. The student model with a singleteacher achieved similar performance as the teacher; and the student model withmultiple teachers achieved better performance than the teachers. The salientfeatures of our algorithm include: 1)no need for original training data orgenerative networks, 2) knowledge transfer between different architectures, 3)ease of implementation for downstream tasks by using the downstream taskdataset as the transferal dataset, 4) knowledge transfer of an ensemble ofmodels, trained independently, into one student model. Extensive experimentsdemonstrate that the proposed algorithm is effective for knowledge transfer andeasily tunable. ", "id2": "258", "id3": "None"}
{"id": "260", "content": "Image segmentation is one of the most essential biomedical image processingproblems for different imaging modalities, including microscopy and X-ray inthe Internet-of-Medical-Things (IoMT) domain. However, annotating biomedicalimages is knowledge-driven, time-consuming, and labor-intensive, making itdifficult to obtain abundant labels with limited costs. Active learningstrategies come into ease the burden of human annotation, which queries only asubset of training data for annotation. Despite receiving attention, most ofactive learning methods generally still require huge computational costs andutilize unlabeled data inefficiently. They also tend to ignore the intermediateknowledge within networks. In this work, we propose a deep activesemi-supervised learning framework, DSAL, combining active learning andsemi-supervised learning strategies. In DSAL, a new criterion based on deepsupervision mechanism is proposed to select informative samples with highuncertainties and low uncertainties for strong labelers and weak labelersrespectively. The internal criterion leverages the disagreement of intermediatefeatures within the deep learning network for active sample selection, whichsubsequently reduces the computational costs. We use the proposed criteria toselect samples for strong and weak labelers to produce oracle labels and pseudolabels simultaneously at each active learning iteration in an ensemble learningmanner, which can be examined with IoMT Platform. Extensive experiments onmultiple medical image datasets demonstrate the superiority of the proposedmethod over state-of-the-art active learning methods. ", "id2": "259", "id3": "None"}
{"id": "261", "content": "As a natural way for human-computer interaction, fixation provides apromising solution for interactive image segmentation. In this paper, we focuson Personal Fixations-based Object Segmentation (PFOS) to address issues inprevious studies, such as the lack of appropriate dataset and the ambiguity infixations-based interaction. In particular, we first construct a new PFOSdataset by carefully collecting pixel-level binary annotation data over anexisting fixation prediction dataset, such dataset is expected to greatlyfacilitate the study along the line. Then, considering characteristics ofpersonal fixations, we propose a novel network based on Object Localization andBoundary Preservation (OLBP) to segment the gazed objects. Specifically, theOLBP network utilizes an Object Localization Module (OLM) to analyze personalfixations and locates the gazed objects based on the interpretation. Then, aBoundary Preservation Module (BPM) is designed to introduce additional boundaryinformation to guard the completeness of the gazed objects. Moreover, OLBP isorganized in the mixed bottom-up and top-down manner with multiple types ofdeep supervision. Extensive experiments on the constructed PFOS dataset showthe superiority of the proposed OLBP network over 17 state-of-the-art methods,and demonstrate the effectiveness of the proposed OLM and BPM components. Theconstructed PFOS dataset and the proposed OLBP network are available athttps://github.com/MathLee/OLBPNet4PFOS. ", "id2": "260", "id3": "None"}
{"id": "262", "content": "Finger vein recognition has drawn increasing attention as one of the mostpopular and promising biometrics due to its high distinguishes ability,security and non-invasive procedure. The main idea of traditional schemes is todirectly extract features from finger vein images or patterns and then comparefeatures to find the best match. However, the features extracted from imagescontain much redundant data, while the features extracted from patterns aregreatly influenced by image segmentation methods. To tack these problems, thispaper proposes a new finger vein recognition by generating code. The proposedmethod does not require an image segmentation algorithm, is simple to calculateand has a small amount of data. Firstly, the finger vein images were dividedinto blocks to calculate the mean value. Then the centrosymmetric coding isperformed by using the generated eigenmatrix. The obtained codewords areconcatenated as the feature codewords of the image. The similarity between veincodes is measured by the ratio of minimum Hamming distance to codeword length.Extensive experiments on two public finger vein databases verify theeffectiveness of the proposed method. The results indicate that our methodoutperforms the state-of-theart methods and has competitive potential inperforming the matching task. ", "id2": "261", "id3": "None"}
{"id": "263", "content": "Image segmentation aims at identifying regions of interest within an image,by grouping pixels according to their properties. This task resembles thestatistical one of clustering, yet many standard clustering methods fail tomeet the basic requirements of image segmentation: segment shapes are oftenbiased toward predetermined shapes and their number is rarely determinedautomatically. Nonparametric clustering is, in principle, free from theselimitations and turns out to be particularly suitable for the task of imagesegmentation. This is also witnessed by several operational analogies, as, forinstance, the resort to topological data analysis and spatial tessellation inboth the frameworks. We discuss the application of nonparametric clustering toimage segmentation and provide an algorithm specific for this task. Pixelsimilarity is evaluated in terms of density of the color representation and theadjacency structure of the pixels is exploited to introduce a simple, yeteffective method to identify image segments as disconnected high-densityregions. The proposed method works both to segment an image and to detect itsboundaries and can be seen as a generalization to color images of the class ofthresholding methods. ", "id2": "262", "id3": "None"}
{"id": "264", "content": "Deep Neural Networks (DNNs) are widely used for decision making in a myriadof critical applications, ranging from medical to societal and even judicial.Given the importance of these decisions, it is crucial for us to be able tointerpret these models. We introduce a new method for interpreting imagesegmentation models by learning regions of images in which noise can be appliedwithout hindering downstream model performance. We apply this method tosegmentation of the pancreas in CT scans, and qualitatively compare the qualityof the method to existing explainability techniques, such as Grad-CAM andocclusion sensitivity. Additionally we show that, unlike other methods, ourinterpretability model can be quantitatively evaluated based on the downstreamperformance over obscured images. ", "id2": "263", "id3": "None"}
{"id": "265", "content": "Image Segmentation plays an essential role in computer vision and imageprocessing with various applications from medical diagnosis to autonomous cardriving. A lot of segmentation algorithms have been proposed for addressingspecific problems. In recent years, the success of deep learning techniques hastremendously influenced a wide range of computer vision areas, and the modernapproaches of image segmentation based on deep learning are becoming prevalent.In this article, we introduce a high-efficient development toolkit for imagesegmentation, named PaddleSeg. The toolkit aims to help both developers andresearchers in the whole process of designing segmentation models, trainingmodels, optimizing performance and inference speed, and deploying models.Currently, PaddleSeg supports around 20 popular segmentation models and morethan 50 pre-trained models from real-time and high-accuracy levels. Withmodular components and backbone networks, users can easily build over onehundred models for different requirements. Furthermore, we providecomprehensive benchmarks and evaluations to show that these segmentationalgorithms trained on our toolkit have more competitive accuracy. Also, weprovide various real industrial applications and practical cases based onPaddleSeg. All codes and examples of PaddleSeg are available athttps://github.com/PaddlePaddle/PaddleSeg. ", "id2": "264", "id3": "None"}
{"id": "266", "content": "This paper proposes an affinity fusion graph framework to effectively connectdifferent graphs with highly discriminating power and nonlinearity for naturalimage segmentation. The proposed framework combines adjacency-graphs and kernelspectral clustering based graphs (KSC-graphs) according to a new definitionnamed affinity nodes of multi-scale superpixels. These affinity nodes areselected based on a better affiliation of superpixels, namelysubspace-preserving representation which is generated by sparse subspaceclustering based on subspace pursuit. Then a KSC-graph is built via a novelkernel spectral clustering to explore the nonlinear relationships among theseaffinity nodes. Moreover, an adjacency-graph at each scale is constructed,which is further used to update the proposed KSC-graph at affinity nodes. Thefusion graph is built across different scales, and it is partitioned to obtainfinal segmentation result. Experimental results on the Berkeley segmentationdataset and Microsoft Research Cambridge dataset show the superiority of ourframework in comparison with the state-of-the-art methods. The code isavailable at https://github.com/Yangzhangcst/AF-graph. ", "id2": "265", "id3": "None"}
{"id": "267", "content": "Despite the progress of interactive image segmentation methods, high-qualitypixel-level annotation is still time-consuming and laborious -- a bottleneckfor several deep learning applications. We take a step back to proposeinteractive and simultaneous segment annotation from multiple images guided byfeature space projection and optimized by metric learning as the labelingprogresses. This strategy is in stark contrast to existing interactivesegmentation methodologies, which perform annotation in the image domain. Weshow that our approach can surpass the accuracy of state-of-the-art methods inforeground segmentation datasets: iCoSeg, DAVIS, and Rooftop. Moreover, itachieves 91.5 % accuracy in a known semantic segmentation dataset, Cityscapes,being 74.75 times faster than the original annotation procedure. The appendixpresents additional qualitative results. Code and video demonstration will bereleased upon publication. ", "id2": "266", "id3": "None"}
{"id": "268", "content": "We address the problem of semantic nighttime image segmentation and improvethe state-of-the-art, by adapting daytime models to nighttime without usingnighttime annotations. Moreover, we design a new evaluation framework toaddress the substantial uncertainty of semantics in nighttime images. Ourcentral contributions are: 1) a curriculum framework to gradually adaptsemantic segmentation models from day to night through progressively darkertimes of day, exploiting cross-time-of-day correspondences between daytimeimages from a reference map and dark images to guide the label inference in thedark domains; 2) a novel uncertainty-aware annotation and evaluation frameworkand metric for semantic segmentation, including image regions beyond humanrecognition capability in the evaluation in a principled fashion; 3) the DarkZurich dataset, comprising 2416 unlabeled nighttime and 2920 unlabeled twilightimages with correspondences to their daytime counterparts plus a set of 201nighttime images with fine pixel-level annotations created with our protocol,which serves as a first benchmark for our novel evaluation. Experiments showthat our map-guided curriculum adaptation significantly outperformsstate-of-the-art methods on nighttime sets both for standard metrics and ouruncertainty-aware metric. Furthermore, our uncertainty-aware evaluation revealsthat selective invalidation of predictions can improve results on data withambiguous content such as our benchmark and profit safety-oriented applicationsinvolving invalid inputs. ", "id2": "267", "id3": "None"}
{"id": "269", "content": "Boundary information plays a significant role in 2D image segmentation, whileusually being ignored in 3D point cloud segmentation where ambiguous featuresmight be generated in feature extraction, leading to misclassification in thetransition area between two objects. In this paper, firstly, we propose aBoundary Prediction Module (BPM) to predict boundary points. Based on thepredicted boundary, a boundary-aware Geometric Encoding Module (GEM) isdesigned to encode geometric information and aggregate features withdiscrimination in a neighborhood, so that the local features belonging todifferent categories will not be polluted by each other. To provide extrageometric information for boundary-aware GEM, we also propose a light-weightGeometric Convolution Operation (GCO), making the extracted features moredistinguishing. Built upon the boundary-aware GEM, we build our network andtest it on benchmarks like ScanNet v2, S3DIS. Results show our methods cansignificantly improve the baseline and achieve state-of-the-art performance.Code is available at https://github.com/JchenXu/BoundaryAwareGEM. ", "id2": "268", "id3": "None"}
{"id": "270", "content": "Active learning is a unique abstraction of machine learning techniques wherethe model/algorithm could guide users for annotation of a set of data pointsthat would be beneficial to the model, unlike passive machine learning. Theprimary advantage being that active learning frameworks select data points thatcan accelerate the learning process of a model and can reduce the amount ofdata needed to achieve full accuracy as compared to a model trained on arandomly acquired data set. Multiple frameworks for active learning combinedwith deep learning have been proposed, and the majority of them are dedicatedto classification tasks. Herein, we explore active learning for the task ofsegmentation of medical imaging data sets. We investigate our proposedframework using two datasets: 1.) MRI scans of the hippocampus, 2.) CT scans ofpancreas and tumors. This work presents a query-by-committee approach foractive learning where a joint optimizer is used for the committee. At the sametime, we propose three new strategies for active learning: 1.) increasingfrequency of uncertain data to bias the training data set; 2.) Using mutualinformation among the input images as a regularizer for acquisition to ensurediversity in the training dataset; 3.) adaptation of Dice log-likelihood forStein variational gradient descent (SVGD). The results indicate an improvementin terms of data reduction by achieving full accuracy while only using 22.69 %and 48.85 % of the available data for each dataset, respectively. ", "id2": "269", "id3": "None"}
{"id": "271", "content": "Multi-modal medical image segmentation plays an essential role in clinicaldiagnosis. It remains challenging as the input modalities are often notwell-aligned spatially. Existing learning-based methods mainly consider sharingtrainable layers across modalities and minimizing visual feature discrepancies.While the problem is often formulated as joint supervised feature learning,multiple-scale features and class-specific representation have not yet beenexplored. In this paper, we propose an affinity-guided fully convolutionalnetwork for multimodal image segmentation. To learn effective representations,we design class-specific affinity matrices to encode the knowledge ofhierarchical feature reasoning, together with the shared convolutional layersto ensure the cross-modality generalization. Our affinity matrix does notdepend on spatial alignments of the visual features and thus allows us to trainwith unpaired, multimodal inputs. We extensively evaluated our method on twopublic multimodal benchmark datasets and outperform state-of-the-art methods. ", "id2": "270", "id3": "None"}
{"id": "272", "content": "We present a fully convolutional neural network (ConvNet), named RatLesNetv2,for segmenting lesions in rodent magnetic resonance (MR) brain images.RatLesNetv2 architecture resembles an autoencoder and it incorporates residualblocks that facilitate its optimization. RatLesNetv2 is trained end to end onthree-dimensional images and it requires no preprocessing. We evaluatedRatLesNetv2 on an exceptionally large dataset composed of 916 T2-weighted ratbrain MRI scans of 671 rats at nine different lesion stages that were used tostudy focal cerebral ischemia for drug development. In addition, we comparedits performance with three other ConvNets specifically designed for medicalimage segmentation. RatLesNetv2 obtained similar to higher Dice coefficientvalues than the other ConvNets and it produced much more realistic and compactsegmentations with notably fewer holes and lower Hausdorff distance. The Dicescores of RatLesNetv2 segmentations also exceeded inter-rater agreement ofmanual segmentations. In conclusion, RatLesNetv2 could be used for automatedlesion segmentation, reducing human workload and improving reproducibility.RatLesNetv2 is publicly available at https://github.com/jmlipman/RatLesNetv2. ", "id2": "271", "id3": "None"}
{"id2": 1023, "id3": "271", "content": "We present a fully convolutional neural network (ConvNet), named RatLesNetv2,for segmenting lesions in rodent magnetic resonance (MR) brain images.RatLesNetv2 architecture resembles an autoencoder and it incorporates residualblocks that facilitate its optimization. RatLesNetv2 is trained end to end onthree-dimensional images and it requires no preprocessing. We evaluatedRatLesNetv2 on an exceptionally large dataset composed of 916 T2-weighted ratbrain MRI scans of 671 rats at nine different lesion stages that were used tostudy focal cerebral ischemia for drug development. In addition, we comparedits performance with three other ConvNets specifically designed for medicalimage segmentation. RatLesNetv2 obtained similar to higher Dice coefficientvalues than the other ConvNets and it produced much more realistic and compactsegmentations with notably fewer holes and lower Hausdorff distance. The Dicescores of RatLesNetv2 segmentations also exceeded inter-rater agreement ofmanual segmentations. In conclusion, RatLesNetv2 could be used for automatedlesion segmentation, reducing human workload and improving reproducibility.RatLesNetv2 is publicly available at https://github.com/jmlipman/RatLesNetv2."}
{"id": "273", "content": "Unsupervised domain adaptation (UDA) for cross-modality medical imagesegmentation has shown great progress by domain-invariant feature learning orimage appearance translation. Adapted feature learning usually cannot detectdomain shifts at the pixel level and is not able to achieve good results indense semantic segmentation tasks. Image appearance translation, e.g. CycleGAN,translates images into different styles with good appearance, despite itspopulation, its semantic consistency is hardly to maintain and results in poorcross-modality segmentation. In this paper, we propose intra- andcross-modality semantic consistency (ICMSC) for UDA and our key insight is thatthe segmentation of synthesised images in different styles should beconsistent. Specifically, our model consists of an image translation module anda domain-specific segmentation module. The image translation module is astandard CycleGAN, while the segmentation module contains two domain-specificsegmentation networks. The intra-modality semantic consistency (IMSC) forcesthe reconstructed image after a cycle to be segmented in the same way as theoriginal input image, while the cross-modality semantic consistency (CMSC)encourages the synthesized images after translation to be segmented exactly thesame as before translation. Comprehensive experimental results oncross-modality hip joint bone segmentation show the effectiveness of ourproposed method, which achieves an average DICE of 81.61% on the acetabulum and88.16% on the proximal femur, outperforming other state-of-the-art methods. Itis worth to note that without UDA, a model trained on CT for hip joint bonesegmentation is non-transferable to MRI and has almost zero-DICE segmentation. ", "id2": "272", "id3": "None"}
{"id2": 1024, "id3": "272", "content": "Unsupervised domain adaptation (UDA) for cross-modality medical imagesegmentation has shown great progress by domain-invariant feature learning orimage appearance translation. Adapted feature learning usually cannot detectdomain shifts at the pixel level and is not able to achieve good results indense semantic segmentation tasks. Image appearance translation, e.g. CycleGAN,translates images into different styles with good appearance, despite itspopulation, its semantic consistency is hardly to maintain and results in poorcross-modality segmentation. In this paper, we propose intra- andcross-modality semantic consistency (ICMSC) for UDA and our key insight is thatthe segmentation of synthesised images in different styles should beconsistent. Specifically, our model consists of an image translation module anda domain-specific segmentation module. The image translation module is astandard CycleGAN, while the segmentation module contains two domain-specificsegmentation networks. The intra-modality semantic consistency (IMSC) forcesthe reconstructed image after a cycle to be segmented in the same way as theoriginal input image, while the cross-modality semantic consistency (CMSC)encourages the synthesized images after translation to be segmented exactly thesame as before translation. Comprehensive experimental results oncross-modality hip joint bone segmentation show the effectiveness of ourproposed method, which achieves an average DICE of 81.61% on the acetabulum and88.16% on the proximal femur, outperforming other state-of-the-art methods. Itis worth to note that without UDA, a model trained on CT for hip joint bonesegmentation is non-transferable to MRI and has almost zero-DICE segmentation."}
{"id": "274", "content": "In image segmentation, there is often more than one plausible solution for agiven input. In medical imaging, for example, experts will often disagree aboutthe exact location of object boundaries. Estimating this inherent uncertaintyand predicting multiple plausible hypotheses is of great interest in manyapplications, yet this ability is lacking in most current deep learningmethods. In this paper, we introduce stochastic segmentation networks (SSNs),an efficient probabilistic method for modelling aleatoric uncertainty with anyimage segmentation network architecture. In contrast to approaches that producepixel-wise estimates, SSNs model joint distributions over entire label maps andthus can generate multiple spatially coherent hypotheses for a single image. Byusing a low-rank multivariate normal distribution over the logit space to modelthe probability of the label map given the image, we obtain a spatiallyconsistent probability distribution that can be efficiently computed by aneural network without any changes to the underlying architecture. We testedour method on the segmentation of real-world medical data, including lungnodules in 2D CT and brain tumours in 3D multimodal MRI scans. SSNs outperformstate-of-the-art for modelling correlated uncertainty in ambiguous images whilebeing much simpler, more flexible, and more efficient. ", "id2": "273", "id3": "None"}
{"id": "275", "content": "Superpixel algorithms, which group pixels similar in color and otherlow-level properties, are increasingly used for pre-processing in imagesegmentation. Commonly important criteria for the computation of superpixelsare boundary adherence, speed, and regularity.  Boundary adherence and regularity are typically contradictory goals. Mostrecent algorithms have focused on improving boundary adherence. Motivated byimproving superpixel regularity, we propose a diagram-based superpixelgeneration method called Power-SLIC.  On the BSDS500 data set, Power-SLIC outperforms other state-of-the-artalgorithms in terms of compactness and boundary precision, and its boundaryadherence is the most robust against varying levels of Gaussian noise. In termsof speed, Power-SLIC is competitive with SLIC. ", "id2": "274", "id3": "None"}
{"id2": 1025, "id3": "274", "content": "Superpixel algorithms, which group pixels similar in color and otherlow-level properties, are increasingly used for pre-processing in imagesegmentation. Commonly important criteria for the computation of superpixelsare boundary adherence, speed, and regularity. Boundary adherence and regularity are typically contradictory goals. Mostrecent algorithms have focused on improving boundary adherence. Motivated byimproving superpixel regularity, we propose a diagram-based superpixelgeneration method called Power-SLIC. On the BSDS500 data set, Power-SLIC outperforms other state-of-the-artalgorithms in terms of compactness and boundary precision, and its boundaryadherence is the most robust against varying levels of Gaussian noise. In termsof speed, Power-SLIC is competitive with SLIC."}
{"id": "276", "content": "Nowcasting is a field of meteorology which aims at forecasting weather on ashort term of up to a few hours. In the meteorology landscape, this field israther specific as it requires particular techniques, such as dataextrapolation, where conventional meteorology is generally based on physicalmodeling. In this paper, we focus on cloud cover nowcasting, which has variousapplication areas such as satellite shots optimisation and photovoltaic energyproduction forecast.  Following recent deep learning successes on multiple imagery tasks, weapplied deep convolutionnal neural networks on Meteosat satellite images forcloud cover nowcasting. We present the results of several architecturesspecialized in image segmentation and time series prediction. We selected thebest models according to machine learning metrics as well as meteorologicalmetrics. All selected architectures showed significant improvements overpersistence and the well-known U-Net surpasses AROME physical model. ", "id2": "275", "id3": "None"}
{"id": "277", "content": "As an important component of autonomous systems, autonomous car perceptionhas had a big leap with recent advances in parallel computing architectures.With the use of tiny but full-feature embedded supercomputers, computer stereovision has been prevalently applied in autonomous cars for depth perception.The two key aspects of computer stereo vision are speed and accuracy. They areboth desirable but conflicting properties, as the algorithms with betterdisparity accuracy usually have higher computational complexity. Therefore, themain aim of developing a computer stereo vision algorithm for resource-limitedhardware is to improve the trade-off between speed and accuracy. In thischapter, we introduce both the hardware and software aspects of computer stereovision for autonomous car systems. Then, we discuss four autonomous carperception tasks, including 1) visual feature detection, description andmatching, 2) 3D information acquisition, 3) object detection/recognition and 4)semantic image segmentation. The principles of computer stereo vision andparallel computing on multi-threading CPU and GPU architectures are thendetailed. ", "id2": "276", "id3": "None"}
{"id": "278", "content": "Automatic medical image segmentation has wide applications for diseasediagnosing. However, it is much more challenging than natural optical imagesegmentation due to the high-resolution of medical images and the correspondinghuge computation cost. The sliding window is a commonly used technique forwhole slide image (WSI) segmentation, however, for these methods based on thesliding window, the main drawback is lacking global contextual information forsupervision. In this paper, we propose a dual-inputs attention network (denotedas DA-RefineNet) for WSI segmentation, where both local fine-grainedinformation and global coarse information can be efficiently utilized.Sufficient comparative experiments are conducted to evaluate the effectivenessof the proposed method, the results prove that the proposed method can achievebetter performance on WSI segmentation compared to methods relying onsingle-input. ", "id2": "277", "id3": "None"}
{"id": "279", "content": "Automatic histopathology image segmentation is crucial to disease analysis.Limited available labeled data hinders the generalizability of trained modelsunder the fully supervised setting. Semi-supervised learning (SSL) based ongenerative methods has been proven to be effective in utilizing diverse imagecharacteristics. However, it has not been well explored what kinds of generatedimages would be more useful for model training and how to use such images. Inthis paper, we propose a new data guided generative method for histopathologyimage segmentation by leveraging the unlabeled data distributions. First, wedesign an image generation module. Image content and style are disentangled andembedded in a clustering-friendly space to utilize their distributions. Newimages are synthesized by sampling and cross-combining contents and styles.Second, we devise an effective data selection policy for judiciously samplingthe generated images: (1) to make the generated training set better cover thedataset, the clusters that are underrepresented in the original training setare covered more; (2) to make the training process more effective, we identifyand oversample the images of hard cases in the data for which annotatedtraining data may be scarce. Our method is evaluated on glands and nucleidatasets. We show that under both the inductive and transductive settings, ourSSL method consistently boosts the performance of common segmentation modelsand attains state-of-the-art results. ", "id2": "278", "id3": "None"}
{"id": "280", "content": "3D Convolution Neural Networks (CNNs) have been widely applied to 3D sceneunderstanding, such as video analysis and volumetric image recognition.However, 3D networks can easily lead to over-parameterization which incursexpensive computation cost. In this paper, we propose Channel-wise AutomaticKErnel Shrinking (CAKES), to enable efficient 3D learning by shrinking standard3D convolutions into a set of economic operations e.g., 1D, 2D convolutions.Unlike previous methods, CAKES performs channel-wise kernel shrinkage, whichenjoys the following benefits: 1) enabling operations deployed in every layerto be heterogeneous, so that they can extract diverse and complementaryinformation to benefit the learning process; and 2) allowing for an efficientand flexible replacement design, which can be generalized to bothspatial-temporal and volumetric data. Further, we propose a new search spacebased on CAKES, so that the replacement configuration can be determinedautomatically for simplifying 3D networks. CAKES shows superior performance toother methods with similar model size, and it also achieves comparableperformance to state-of-the-art with much fewer parameters and computationalcosts on tasks including 3D medical imaging segmentation and video actionrecognition. Codes and models are available athttps://github.com/yucornetto/CAKES ", "id2": "279", "id3": "None"}
{"id": "281", "content": "Unsupervised image segmentation aims at assigning the pixels with similarfeature into a same cluster without annotation, which is an important task incomputer vision. Due to lack of prior knowledge, most of existing model usuallyneed to be trained several times to obtain suitable results. To address thisproblem, we propose an unsupervised image segmentation model based on theMutual Mean-Teaching (MMT) framework to produce more stable results. Inaddition, since the labels of pixels from two model are not matched, a labelalignment algorithm based on the Hungarian algorithm is proposed to match thecluster labels. Experimental results demonstrate that the proposed model isable to segment various types of images and achieves better performance thanthe existing methods. ", "id2": "280", "id3": "None"}
{"id": "282", "content": "Object recognition advances very rapidly these days. One challenge is togeneralize existing methods to new domains, to more classes and/or to new datamodalities. In order to avoid annotating one dataset for each of these newcases, one needs to combine and reuse existing datasets that may belong todifferent domains, have partial annotations, and/or have different datamodalities. This paper treats this task as a multi-source domain adaptation andlabel unification (mDALU) problem and proposes a novel method for it. Ourmethod consists of a partially-supervised adaptation stage and afully-supervised adaptation stage. In the former, partial knowledge istransferred from multiple source domains to the target domain and fusedtherein. Negative transfer between unmatched label space is mitigated via threenew modules: domain attention, uncertainty maximization and attention-guidedadversarial alignment. In the latter, knowledge is transferred in the unifiedlabel space after a label completion process with pseudo-labels. We verify themethod on three different tasks, image classification, 2D semantic imagesegmentation, and joint 2D-3D semantic segmentation. Extensive experiments showthat our method outperforms all competing methods significantly. ", "id2": "281", "id3": "None"}
{"id": "283", "content": "Segmentation of findings in the gastrointestinal tract is a challenging butalso an important task which is an important building stone for sufficientautomatic decision support systems. In this work, we present our solution forthe Medico 2020 task, which focused on the problem of colon polyp segmentation.We present our simple but efficient idea of using an augmentation method thatuses grids in a pyramid-like manner (large to small) for segmentation. Ourresults show that the proposed methods work as indented and can also lead tocomparable results when competing with other methods. ", "id2": "282", "id3": "None"}
{"id": "284", "content": "Compared with common image segmentation tasks targeted at low-resolutionimages, higher resolution detailed image segmentation receives much lessattention. In this paper, we propose and study a task named Meticulous ObjectSegmentation (MOS), which is focused on segmenting well-defined foregroundobjects with elaborate shapes in high resolution images (e.g. 2k - 4k). To thisend, we propose the MeticulousNet which leverages a dedicated decoder tocapture the object boundary details. Specifically, we design a HierarchicalPoint-wise Refining (HierPR) block to better delineate object boundaries, andreformulate the decoding process as a recursive coarse to fine refinement ofthe object mask. To evaluate segmentation quality near object boundaries, wepropose the Meticulosity Quality (MQ) score considering both the mask coverageand boundary precision. In addition, we collect a MOS benchmark datasetincluding 600 high quality images with complex objects. We providecomprehensive empirical evidence showing that MeticulousNet can revealpixel-accurate segmentation boundaries and is superior to state-of-the-artmethods for high resolution object segmentation tasks. ", "id2": "283", "id3": "None"}
{"id": "285", "content": "Semantic segmentation has achieved significant advances in recent years.While deep neural networks perform semantic segmentation well, their successrely on pixel level supervision which is expensive and time-consuming. Further,training using data from one domain may not generalize well to data from a newdomain due to a domain gap between data distributions in the different domains.This domain gap is particularly evident in aerial images where visualappearance depends on the type of environment imaged, season, weather, and timeof day when the environment is imaged. Subsequently, this distribution gapleads to severe accuracy loss when using a pretrained segmentation model toanalyze new data with different characteristics. In this paper, we propose anovel unsupervised domain adaptation framework to address domain shift in thecontext of aerial semantic image segmentation. To this end, we solve theproblem of domain shift by learn the soft label distribution difference betweenthe source and target domains. Further, we also apply entropy minimization onthe target domain to produce high-confident prediction rather than usinghigh-confident prediction by pseudo-labeling. We demonstrate the effectivenessof our domain adaptation framework using the challenge image segmentationdataset of ISPRS, and show improvement over state-of-the-art methods in termsof various metrics. ", "id2": "284", "id3": "None"}
{"id": "286", "content": "Despite deep convolutional neural networks achieved impressive progress inmedical image computing and analysis, its paradigm of supervised learningdemands a large number of annotations for training to avoid overfitting andachieving promising results. In clinical practices, massive semanticannotations are difficult to acquire in some conditions where specializedbiomedical expert knowledge is required, and it is also a common conditionwhere only few annotated classes are available. In this work, we proposed anovel method for few-shot medical image segmentation, which enables asegmentation model to fast generalize to an unseen class with few trainingimages. We construct our few-shot image segmentor using a deep convolutionalnetwork trained episodically. Motivated by the spatial consistency andregularity in medical images, we developed an efficient global correlationmodule to capture the correlation between a support and query image andincorporate it into the deep network called global correlation network.Moreover, we enhance discriminability of deep embedding to encourage clusteringof the feature domains of the same class while keep the feature domains ofdifferent organs far apart. Ablation Study proved the effectiveness of theproposed global correlation module and discriminative embedding loss. Extensiveexperiments on anatomical abdomen images on both CT and MRI modalities areperformed to demonstrate the state-of-the-art performance of our proposedmodel. ", "id2": "285", "id3": "None"}
{"id": "287", "content": "We present CROP (Central Roundish Object Painter), which identifies andpaints the object at the center of an RGB image. Primarily CROP works forroundish fruits in various illumination conditions, but surprisingly, it couldalso deal with images of other organic or inorganic materials, or ones byoptical and electron microscopes, although CROP was trained solely by 172images of fruits. The method involves image segmentation by deep learning, andthe architecture of the neural network is a deeper version of the originalU-Net. This technique could provide us with a means of automatically collectingstatistical data of fruit growth in farms. As an example, we describe ourexperiment of processing 510 time series photos automatically to collect thedata on the size and the position of the target fruit. Our trained neuralnetwork CROP and the above automatic programs are available on GitHub withuser-friendly interface programs. ", "id2": "286", "id3": "None"}
{"id": "288", "content": "Artist, year and style classification of fine-art paintings are generallyachieved using standard image classification methods, image segmentation, ormore recently, convolutional neural networks (CNNs). This works aims to usenewly developed face recognition methods such as FaceNet that use CNNs tocluster fine-art paintings using the extracted faces in the paintings, whichare found abundantly. A dataset consisting of over 80,000 paintings from over1000 artists is chosen, and three separate face recognition and clusteringtasks are performed. The produced clusters are analyzed by the file names ofthe paintings and the clusters are named by their majority artist, year range,and style. The clusters are further analyzed and their performance metrics arecalculated. The study shows promising results as the artist, year, and stylesare clustered with an accuracy of 58.8, 63.7, and 81.3 percent, while theclusters have an average purity of 63.1, 72.4, and 85.9 percent. ", "id2": "287", "id3": "None"}
{"id": "289", "content": "In recent years, the idea of using morphological operations as networks hasreceived much attention. Mathematical morphology provides very efficient anduseful image processing and image analysis tools based on basic operators likedilation and erosion, defined in terms of kernels. Many other morphologicaloperations are built up using the dilation and erosion operations. Although thelearning of structuring elements such as dilation or erosion using thebackpropagation algorithm is not new, the order and the way these morphologicaloperations are used is not standard. In this paper, we have theoreticallyanalyzed the use of morphological operations for processing 1D feature vectorsand shown that this gets extended to the 2D case in a simple manner. Ourtheoretical results show that a morphological block represents a sum of hingefunctions. Hinge functions are used in many places for classification andregression tasks (Breiman (1993)). We have also proved a universalapproximation theorem -- a stack of two morphological blocks can approximateany continuous function over arbitrary compact sets. To experimentally validatethe efficacy of this network in real-life applications, we have evaluated itsperformance on satellite image classification datasets since morphologicaloperations are very sensitive to geometrical shapes and structures. We havealso shown results on a few tasks like segmentation of blood vessels fromfundus images, segmentation of lungs from chest x-ray and image dehazing. Theresults are encouraging and further establishes the potential of morphologicalnetworks. ", "id2": "288", "id3": "None"}
{"id": "290", "content": "The data-driven nature of deep learning models for semantic segmentationrequires a large number of pixel-level annotations. However, large-scale andfully labeled medical datasets are often unavailable for practical tasks.Recently, partially supervised methods have been proposed to utilize imageswith incomplete labels to mitigate the data scarcity problem in the medicaldomain. As an emerging research area, the breakthroughs made by existingmethods rely on either large-scale data or complex model design, which makesthem 1) less practical for certain real-life tasks and 2) less robust forsmall-scale data. It is time to step back and think about the robustness ofpartially supervised methods and how to maximally utilize small-scale andpartially labeled data for medical image segmentation tasks. To bridge themethodological gaps in label-efficient deep learning with partial supervision,we propose RAMP, a simple yet efficient data augmentation framework forpartially supervised medical image segmentation by exploiting the assumptionthat patients share anatomical similarities. We systematically evaluate RAMPand the previous methods in various controlled multi-structure segmentationtasks. Compared to the mainstream approaches, RAMP consistently improves theperformance of traditional segmentation networks on small-scale partiallylabeled data and utilize additional image-wise weak annotations. ", "id2": "289", "id3": "None"}
{"id": "291", "content": "Gliomas are the most common primary brain malignancies, with differentdegrees of aggressiveness, variable prognosis and various heterogeneoushistological sub-regions, i.e., peritumoral edema, necrotic core, enhancing andnon-enhancing tumour core. Although brain tumours can easily be detected usingmulti-modal MRI, accurate tumor segmentation is a challenging task. Hence,using the data provided by the BraTS Challenge 2020, we propose a 3Dvolume-to-volume Generative Adversarial Network for segmentation of braintumours. The model, called Vox2Vox, generates realistic segmentation outputsfrom multi-channel 3D MR images, segmenting the whole, core and enhancing tumorwith mean values of 87.20%, 81.14%, and 78.67% as dice scores and 6.44mm,24.36mm, and 18.95mm for Hausdorff distance 95 percentile for the BraTS testingset after ensembling 10 Vox2Vox models obtained with a 10-foldcross-validation. ", "id2": "290", "id3": "None"}
{"id": "292", "content": "It has been widely recognized that the success of deep learning in imagesegmentation relies overwhelmingly on a myriad amount of densely annotatedtraining data, which, however, are difficult to obtain due to the tremendouslabor and expertise required, particularly for annotating 3D medical images.Although self-supervised learning (SSL) has shown great potential to addressthis issue, most SSL approaches focus only on image-level global consistency,but ignore the local consistency which plays a pivotal role in capturingstructural information for dense prediction tasks such as segmentation. In thispaper, we propose a PriorGuided Local (PGL) self-supervised model that learnsthe region-wise local consistency in the latent feature space. Specifically, weuse the spatial transformations, which produce different augmented views of thesame image, as a prior to deduce the location relation between two views, whichis then used to align the feature maps of the same local region but beingextracted on two views. Next, we construct a local consistency loss to minimizethe voxel-wise discrepancy between the aligned feature maps. Thus, our PGLmodel learns the distinctive representations of local regions, and hence isable to retain structural information. This ability is conducive to downstreamsegmentation tasks. We conducted an extensive evaluation on four publiccomputerized tomography (CT) datasets that cover 11 kinds of major human organsand two tumors. The results indicate that using pre-trained PGL model toinitialize a downstream network leads to a substantial performance improvementover both random initialization and the initialization with globalconsistency-based models. Code and pre-trained weights will be made availableat: https://git.io/PGL. ", "id2": "291", "id3": "None"}
{"id": "293", "content": "Infrared (IR) image segmentation is essential in many urban defenceapplications, such as pedestrian surveillance, vehicle counting, securitymonitoring, etc. Active contour model (ACM) is one of the most widely usedimage segmentation tools at present, but the existing methods only utilize thelocal or global single feature information of image to minimize the energyfunction, which is easy to cause false segmentations in IR images. In thispaper, we propose a multi-feature driven active contour segmentation model tohandle IR images with intensity inhomogeneity. Firstly, an especially-designedsigned pressure force (SPF) function is constructed by combining the globalinformation calculated by global average gray information and the localmulti-feature information calculated by local entropy, local standard deviationand gradient information. Then, we draw upon adaptive weight coefficientcalculated by local range to adjust the afore-mentioned global term and localterm. Next, the SPF function is substituted into the level set formulation(LSF) for further evolution. Finally, the LSF converges after a finite numberof iterations, and the IR image segmentation result is obtained from thecorresponding convergence result. Experimental results demonstrate that thepresented method outperforms the state-of-the-art models in terms of precisionrate and overlapping rate in IR test images. ", "id2": "292", "id3": "None"}
{"id": "294", "content": "In recent years, Convolutional Neural Networks (CNNs) have become thestate-of-the-art method for biomedical image analysis. However, these networksare usually trained in a supervised manner, requiring large amounts of labelledtraining data. These labelled data sets are often difficult to acquire in thebiomedical domain. In this work, we validate alternative ways to train CNNswith fewer labels for biomedical image segmentation using. We adapt two semi-and self-supervised image classification methods and analyse their performancefor semantic segmentation of biomedical microscopy images. ", "id2": "293", "id3": "None"}
{"id": "295", "content": "Today, deep convolutional neural networks (CNNs) have demonstrated state ofthe art performance for supervised medical image segmentation, across variousimaging modalities and tasks. Despite early success, segmentation networks maystill generate anatomically aberrant segmentations, with holes or inaccuraciesnear the object boundaries. To mitigate this effect, recent research works havefocused on incorporating spatial information or prior knowledge to enforceanatomically plausible segmentation. If the integration of prior knowledge inimage segmentation is not a new topic in classical optimization approaches, itis today an increasing trend in CNN based image segmentation, as shown by thegrowing literature on the topic. In this survey, we focus on high level prior,embedded at the loss function level. We categorize the articles according tothe nature of the prior: the object shape, size, topology, and theinter-regions constraints. We highlight strengths and limitations of currentapproaches, discuss the challenge related to the design and the integration ofprior-based losses, and the optimization strategies, and draw future researchdirections. ", "id2": "294", "id3": "None"}
{"id": "296", "content": "Ferrograph image segmentation is of significance for obtaining features ofwear particles. However, wear particles are usually overlapped in the form ofdebris chains, which makes challenges to segment wear debris. An overlappingwear particle segmentation network (OWPSNet) is proposed in this study tosegment the overlapped debris chains. The proposed deep learning model includesthree parts: a region segmentation network, an edge detection network and afeature refine module. The region segmentation network is an improved U shapenetwork, and it is applied to separate the wear debris form background offerrograph image. The edge detection network is used to detect the edges ofwear particles. Then, the feature refine module combines low-level features andhigh-level semantic features to obtain the final results. In order to solve theproblem of sample imbalance, we proposed a square dice loss function tooptimize the model. Finally, extensive experiments have been carried out on aferrograph image dataset. Results show that the proposed model is capable ofseparating overlapping wear particles. Moreover, the proposed square dice lossfunction can improve the segmentation results, especially for the segmentationresults of wear particle edge. ", "id2": "295", "id3": "None"}
{"id": "297", "content": "Due to the intensive cost of labor and expertise in annotating 3D medicalimages at a voxel level, most benchmark datasets are equipped with theannotations of only one type of organs and/or tumors, resulting in theso-called partially labeling issue. To address this, we propose a dynamicon-demand network (DoDNet) that learns to segment multiple organs and tumors onpartially labeled datasets. DoDNet consists of a shared encoder-decoderarchitecture, a task encoding module, a controller for generating dynamicconvolution filters, and a single but dynamic segmentation head. Theinformation of the current segmentation task is encoded as a task-aware priorto tell the model what the task is expected to solve. Different from existingapproaches which fix kernels after training, the kernels in dynamic head aregenerated adaptively by the controller, conditioned on both input image andassigned task. Thus, DoDNet is able to segment multiple organs and tumors, asdone by multiple networks or a multi-head network, in a much efficient andflexible manner. We have created a large-scale partially labeled dataset,termed MOTS, and demonstrated the superior performance of our DoDNet over othercompetitors on seven organ and tumor segmentation tasks. We also transferredthe weights pre-trained on MOTS to a downstream multi-organ segmentation taskand achieved state-of-the-art performance. This study provides a general 3Dmedical image segmentation model that has been pre-trained on a large-scalepartially labelled dataset and can be extended (after fine-tuning) todownstream volumetric medical data segmentation tasks. The dataset and codeareavailableat: https://git.io/DoDNet ", "id2": "296", "id3": "None"}
{"id": "298", "content": "Segmentation of organs of interest in 3D medical images is necessary foraccurate diagnosis and longitudinal studies. Though recent advances using deeplearning have shown success for many segmentation tasks, large datasets arerequired for high performance and the annotation process is both time consumingand labor intensive. In this paper, we propose a 3D few shot segmentationframework for accurate organ segmentation using limited training samples of thetarget organ annotation. To achieve this, a U-Net like network is designed topredict segmentation by learning the relationship between 2D slices of supportdata and a query image, including a bidirectional gated recurrent unit (GRU)that learns consistency of encoded features between adjacent slices. Also, weintroduce a transfer learning method to adapt the characteristics of the targetimage and organ by updating the model before testing with arbitrary support andquery data sampled from the support data. We evaluate our proposed model usingthree 3D CT datasets with annotations of different organs. Our model yieldedsignificantly improved performance over state-of-the-art few shot segmentationmodels and was comparable to a fully supervised model trained with more targettraining data. ", "id2": "297", "id3": "None"}
{"id": "299", "content": "Dilated convolutions are widely used in deep semantic segmentation models asthey can enlarge the filters receptive field without adding additional weightsnor sacrificing spatial resolution. However, as dilated convolutional filtersdo not possess positional knowledge about the pixels on semantically meaningfulcontours, they could lead to ambiguous predictions on object boundaries. Inaddition, although dilating the filter can expand its receptive field, thetotal number of sampled pixels remains unchanged, which usually comprises asmall fraction of the receptive fields total area. Inspired by the LateralInhibition (LI) mechanisms in human visual systems, we propose the dilatedconvolution with lateral inhibitions (LI-Convs) to overcome these limitations.Introducing LI mechanisms improves the convolutional filters sensitivity tosemantic object boundaries. Moreover, since LI-Convs also implicitly take thepixels from the laterally inhibited zones into consideration, they can alsoextract features at a denser scale. By integrating LI-Convs into the Deeplabv3+architecture, we propose the Lateral Inhibited Atrous Spatial Pyramid Pooling(LI-ASPP), the Lateral Inhibited MobileNet-V2 (LI-MNV2) and the LateralInhibited ResNet (LI-ResNet). Experimental results on three benchmark datasets(PASCAL VOC 2012, CelebAMask-HQ and ADE20K) show that our LI-based segmentationmodels outperform the baseline on all of them, thus verify the effectivenessand generality of the proposed LI-Convs. ", "id2": "298", "id3": "None"}
{"id2": 1026, "id3": "298", "content": "Dilated convolutions are widely used in deep semantic segmentation models asthey can enlarge the filters receptive field without adding additional weightsnor sacrificing spatial resolution. However, as dilated convolutional filtersdo not possess positional knowledge about the pixels on semantically meaningfulcontours, they could lead to ambiguous predictions on object boundaries. Inaddition, although dilating the filter can expand its receptive field, thetotal number of sampled pixels remains unchanged, which usually comprises asmall fraction of the receptive fields total area. Inspired by the LateralInhibition (LI) mechanisms in human visual systems, we propose the dilatedconvolution with lateral inhibitions (LI-Convs) to overcome these limitations.Introducing LI mechanisms improves the convolutional filters sensitivity tosemantic object boundaries. Moreover, since LI-Convs also implicitly take thepixels from the laterally inhibited zones into consideration, they can alsoextract features at a denser scale. By integrating LI-Convs into the Deeplabv3+architecture, we propose the Lateral Inhibited Atrous Spatial Pyramid Pooling(LI-ASPP), the Lateral Inhibited MobileNet-V2 (LI-MNV2) and the LateralInhibited ResNet (LI-ResNet). Experimental results on three benchmark datasets(PASCAL VOC 2012, CelebAMask-HQ and ADE20K) show that our LI-based segmentationmodels outperform the baseline on all of them, thus verify the effectivenessand generality of the proposed LI-Convs."}
{"id": "300", "content": "In recent years there has been a growing interest in image generation throughdeep learning. While an important part of the evaluation of the generatedimages usually involves visual inspection, the inclusion of human perception asa factor in the training process is often overlooked. In this paper we proposean alternative perceptual regulariser for image-to-image translation usingconditional generative adversarial networks (cGANs). To do so automatically(avoiding visual inspection), we use the Normalised Laplacian Pyramid Distance(NLPD) to measure the perceptual similarity between the generated image and theoriginal image. The NLPD is based on the principle of normalising the value ofcoefficients with respect to a local estimate of mean energy at differentscales and has already been successfully tested in different experimentsinvolving human perception. We compare this regulariser with the originallyproposed L1 distance and note that when using NLPD the generated images containmore realistic values for both local and global contrast. We found that usingNLPD as a regulariser improves image segmentation accuracy on generated imagesas well as improving two no-reference image quality metrics. ", "id2": "299", "id3": "None"}
{"id": "301", "content": "Medical image segmentation is one of the major challenges addressed bymachine learning methods. Yet, deep learning methods profoundly depend on alarge amount of annotated data, which is time-consuming and costly. Though,semi-supervised learning methods approach this problem by leveraging anabundant amount of unlabeled data along with a small amount of labeled data inthe training process. Recently, MixUp regularizer has been successfullyintroduced to semi-supervised learning methods showing superior performance.MixUp augments the model with new data points through linear interpolation ofthe data at the input space. We argue that this option is limited. Instead, wepropose ROAM, a RandOm lAyer Mixup, which encourages the network to be lessconfident for interpolated data points at randomly selected space. ROAMgenerates more data points that have never seen before, and hence it avoidsover-fitting and enhances the generalization ability. We conduct extensiveexperiments to validate our method on three publicly available datasets onwhole-brain image segmentation. ROAM achieves state-of-the-art (SOTA) resultsin fully supervised (89.5%) and semi-supervised (87.0%) settings with arelative improvement of up to 2.40% and 16.50%, respectively for thewhole-brain segmentation. ", "id2": "300", "id3": "None"}
{"id": "302", "content": "Deep learning-based medical image segmentation technology aims at automaticrecognizing and annotating objects on the medical image. Non-local attentionand feature learning by multi-scale methods are widely used to model network,which drives progress in medical image segmentation. However, those attentionmechanism methods have weakly non-local receptive fields strengthenedconnection for small objects in medical images. Then, the features of importantsmall objects in abstract or coarse feature maps may be deserted, which leadsto unsatisfactory performance. Moreover, the existing multi-scale methods onlysimply focus on different sizes of view, whose sparse multi-scale featurescollected are not abundant enough for small objects segmentation. In this work,a multi-dimensional attention segmentation model with cascade multi-scaleconvolution is proposed to predict accurate segmentation for small objects inmedical images. As the weight function, multi-dimensional attention modulesprovide coefficient modification for significant/informative small objectsfeatures. Furthermore, The cascade multi-scale convolution modules in eachskip-connection path are exploited to capture multi-scale features in differentsemantic depth. The proposed method is evaluated on three datasets: KiTS19,Pancreas CT of Decathlon-10, and MICCAI 2018 LiTS Challenge, demonstratingbetter segmentation performances than the state-of-the-art baselines. ", "id2": "301", "id3": "None"}
{"id": "303", "content": "Image segmentation is a key topic in image processing and computer visionwith applications such as scene understanding, medical image analysis, roboticperception, video surveillance, augmented reality, and image compression, amongmany others. Various algorithms for image segmentation have been developed inthe literature. Recently, due to the success of deep learning models in a widerange of vision applications, there has been a substantial amount of worksaimed at developing image segmentation approaches using deep learning models.In this survey, we provide a comprehensive review of the literature at the timeof this writing, covering a broad spectrum of pioneering works for semantic andinstance-level segmentation, including fully convolutional pixel-labelingnetworks, encoder-decoder architectures, multi-scale and pyramid basedapproaches, recurrent networks, visual attention models, and generative modelsin adversarial settings. We investigate the similarity, strengths andchallenges of these deep learning models, examine the most widely useddatasets, report performances, and discuss promising future research directionsin this area. ", "id2": "302", "id3": "None"}
{"id": "304", "content": "Datasets with noisy labels are a common occurrence in practical applicationsof classification methods. We propose a simple probabilistic method fortraining deep classifiers under input-dependent (heteroscedastic) label noise.We assume an underlying heteroscedastic generative process for noisy labels. Tomake gradient based training feasible we use a temperature parameterizedsoftmax as a smooth approximation to the assumed generative process. Weillustrate that the softmax temperature controls a bias-variance trade-off forthe approximation. By tuning the softmax temperature, we improve accuracy,log-likelihood and calibration on both image classification benchmarks withcontrolled label noise as well as Imagenet-21k which has naturally occurringlabel noise. For image segmentation, our method increases the mean IoU on thePASCAL VOC and Cityscapes datasets by more than 1% over the state-of-the-artmodel. ", "id2": "303", "id3": "None"}
{"id": "305", "content": "Video semantic segmentation is active in recent years benefited from thegreat progress of image semantic segmentation. For such a task, the per-frameimage segmentation is generally unacceptable in practice due to highcomputation cost. To tackle this issue, many works use the flow-based featurepropagation to reuse the features of previous frames. However, the optical flowestimation inevitably suffers inaccuracy and then causes the propagatedfeatures distorted. In this paper, we propose distortion-aware featurecorrection to alleviate the issue, which improves video segmentationperformance by correcting distorted propagated features. To be specific, wefirstly propose to transfer distortion patterns from feature into image spaceand conduct effective distortion map prediction. Benefited from the guidance ofdistortion maps, we proposed Feature Correction Module (FCM) to rectifypropagated features in the distorted areas. Our proposed method cansignificantly boost the accuracy of video semantic segmentation at a low price.The extensive experimental results on Cityscapes and CamVid show that ourmethod outperforms the recent state-of-the-art methods. ", "id2": "304", "id3": "None"}
{"id": "306", "content": "Deep learning (DL) models for disease classification or segmentation frommedical images are increasingly trained using transfer learning (TL) fromunrelated natural world images. However, shortcomings and utility of TL forspecialized tasks in the medical imaging domain remain unknown and are based onassumptions that increasing training data will improve performance. We reportdetailed comparisons, rigorous statistical analysis and comparisons of widelyused DL architecture for binary segmentation after TL with ImageNetinitialization (TII-models) with supervised learning with only medicalimages(LMI-models) of macroscopic optical skin cancer, microscopic prostatecore biopsy and Computed Tomography (CT) DICOM images. Through visualinspection of TII and LMI model outputs and their Grad-CAM counterparts, ourresults identify several counter intuitive scenarios where automatedsegmentation of one tumor by both models or the use of individual segmentationoutput masks in various combinations from individual models leads to 10%increase in performance. We also report sophisticated ensemble DL strategiesfor achieving clinical grade medical image segmentation and model explanationsunder low data regimes. For example; estimating performance, explanations andreplicability of LMI and TII models described by us can be used for situationsin which sparsity promotes better learning. A free GitHub repository of TII andLMI models, code and more than 10,000 medical images and their Grad-CAM outputfrom this study can be used as starting points for advanced computationalmedicine and DL research for biomedical discovery and applications. ", "id2": "305", "id3": "None"}
{"id": "307", "content": "Magnetic resonance (MR) protocols rely on several sequences to assesspathology and organ status properly. Despite advances in image analysis, wetend to treat each sequence, here termed modality, in isolation. Takingadvantage of the common information shared between modalities (an organsanatomy) is beneficial for multi-modality processing and learning. However, wemust overcome inherent anatomical misregistrations and disparities in signalintensity across the modalities to obtain this benefit. We present a methodthat offers improved segmentation accuracy of the modality of interest (over asingle input model), by learning to leverage information present in othermodalities, even if few (semi-supervised) or no (unsupervised) annotations areavailable for this specific modality. Core to our method is learning adisentangled decomposition into anatomical and imaging factors. Sharedanatomical factors from the different inputs are jointly processed and fused toextract more accurate segmentation masks. Image misregistrations are correctedwith a Spatial Transformer Network, which non-linearly aligns the anatomicalfactors. The imaging factor captures signal intensity characteristics acrossdifferent modality data and is used for image reconstruction, enablingsemi-supervised learning. Temporal and slice pairing between inputs are learneddynamically. We demonstrate applications in Late Gadolinium Enhanced (LGE) andBlood Oxygenation Level Dependent (BOLD) cardiac segmentation, as well as in T2abdominal segmentation. Code is available athttps://github.com/vios-s/multimodal_segmentation. ", "id2": "306", "id3": "None"}
{"id": "308", "content": "In autonomous Vehicles technology Image segmentation was a major problem invisual perception. This image segmentation process is mainly used in medicalapplications. Here we adopted an image segmentation process to visualperception tasks for predicting the agents on the surrounding environment,identifying the road boundaries and tracking the line markings. Main objectiveof the paper is to divide the input images using the image segmentation processand Convolution Neural Network method for efficient results of visualperception. For Sampling assume a local city data-set samples and validationprocess done in Jupyter Notebook using Python language. We proposed this imagesegmentation method planning to standard and further the development ofstate-of-the art methods for visual inspection system understanding. Theexperimental results achieves 73% mean IOU. Our method also achieves 90 FPSinference speed and using a NVDIA GeForce GTX 1050 GPU. ", "id2": "307", "id3": "None"}
{"id": "309", "content": "The Jaccard index, also known as Intersection-over-Union (IoU score), is oneof the most critical evaluation metrics in medical image segmentation. However,directly optimizing the mean IoU (mIoU) score over multiple objective classesis an open problem. Although some algorithms have been proposed to optimize itssurrogates, there is no guarantee provided for their generalization ability. Inthis paper, we present a novel data-distribution-aware margin calibrationmethod for a better generalization of the mIoU over the wholedata-distribution, underpinned by a rigid lower bound. This scheme ensures abetter segmentation performance in terms of IoU scores in practice. We evaluatethe effectiveness of the proposed margin calibration method on two medicalimage segmentation datasets, showing substantial improvements of IoU scoresover other learning schemes using deep segmentation models. ", "id2": "308", "id3": "None"}
{"id": "310", "content": "Waste recycling is an important way of saving energy and materials in theproduction process. In general cases recyclable objects are mixed withunrecyclable objects, which raises a need for identification andclassification. This paper proposes a convolutional neural network (CNN) modelto complete both tasks. The model uses transfer learning from a pretrainedResnet-50 CNN to complete feature extraction. A subsequent fully connectedlayer for classification was trained on the augmented TrashNet dataset [1]. Inthe application, sliding-window is used for image segmentation in thepre-classification stage. In the post-classification stage, the labelled samplepoints are integrated with Gaussian Clustering to locate the object. Theresulting model has achieved an overall detection rate of 48.4% in simulationand final classification accuracy of 92.4%. ", "id2": "309", "id3": "None"}
{"id": "311", "content": "Working with images, one often faces problems with incomplete or unclearinformation. Image inpainting can be used to restore missing image regions butfocuses, however, on low-level image features such as pixel intensity, pixelgradient orientation, and color. This paper aims to recover semantic imagefeatures (objects and positions) in images. Based on published gated PixelCNNs,we demonstrate a new approach referred to as quadro-directional PixelCNN torecover missing objects and return probable positions for objects based on thecontext. We call this approach context-based image segment labeling (CBISL).The results suggest that our four-directional model outperforms one-directionalmodels (gated PixelCNN) and returns a human-comparable performance. ", "id2": "310", "id3": "None"}
{"id": "312", "content": "Scene understanding is an essential technique in semantic segmentation.Although there exist several datasets that can be used for semanticsegmentation, they are mainly focused on semantic image segmentation with largedeep neural networks. Therefore, these networks are not useful for real timeapplications, especially in autonomous driving systems. In order to solve thisproblem, we make two contributions to semantic segmentation task. The firstcontribution is that we introduce the semantic video dataset, the HighwayDriving dataset, which is a densely annotated benchmark for a semantic videosegmentation task. The Highway Driving dataset consists of 20 video sequenceshaving a 30Hz frame rate, and every frame is densely annotated. Secondly, wepropose a baseline algorithm that utilizes a temporal correlation. Togetherwith our attempt to analyze the temporal correlation, we expect the HighwayDriving dataset to encourage research on semantic video segmentation. ", "id2": "311", "id3": "None"}
{"id": "313", "content": "A key requirement for the success of supervised deep learning is a largelabeled dataset - a condition that is difficult to meet in medical imageanalysis. Self-supervised learning (SSL) can help in this regard by providing astrategy to pre-train a neural network with unlabeled data, followed byfine-tuning for a downstream task with limited annotations. Contrastivelearning, a particular variant of SSL, is a powerful technique for learningimage-level representations. In this work, we propose strategies for extendingthe contrastive learning framework for segmentation of volumetric medicalimages in the semi-supervised setting with limited annotations, by leveragingdomain-specific and problem-specific cues. Specifically, we propose (1) novelcontrasting strategies that leverage structural similarity across volumetricmedical images (domain-specific cue) and (2) a local version of the contrastiveloss to learn distinctive representations of local regions that are useful forper-pixel segmentation (problem-specific cue). We carry out an extensiveevaluation on three Magnetic Resonance Imaging (MRI) datasets. In the limitedannotation setting, the proposed method yields substantial improvementscompared to other self-supervision and semi-supervised learning techniques.When combined with a simple data augmentation technique, the proposed methodreaches within 8% of benchmark performance using only two labeled MRI volumesfor training, corresponding to only 4% (for ACDC) of the training data used totrain the benchmark. The code is made public athttps://github.com/krishnabits001/domain_specific_cl. ", "id2": "312", "id3": "None"}
{"id2": 1027, "id3": "312", "content": "A key requirement for the success of supervised deep learning is a largelabeled dataset - a condition that is difficult to meet in medical imageanalysis. Self-supervised learning (SSL) can help in this regard by providing astrategy to pre-train a neural network with unlabeled data, followed byfine-tuning for a downstream task with limited annotations. Contrastivelearning, a particular variant of SSL, is a powerful technique for learningimage-level representations. In this work, we propose strategies for extendingthe contrastive learning framework for segmentation of volumetric medicalimages in the semi-supervised setting with limited annotations, by leveragingdomain-specific and problem-specific cues. Specifically, we propose (1) novelcontrasting strategies that leverage structural similarity across volumetricmedical images (domain-specific cue) and (2) a local version of the contrastiveloss to learn distinctive representations of local regions that are useful forper-pixel segmentation (problem-specific cue). We carry out an extensiveevaluation on three Magnetic Resonance Imaging (MRI) datasets. In the limitedannotation setting, the proposed method yields substantial improvementscompared to other self-supervision and semi-supervised learning techniques.When combined with a simple data augmentation technique, the proposed methodreaches within 8% of benchmark performance using only two labeled MRI volumesfor training, corresponding to only 4% (for ACDC) of the training data used totrain the benchmark. The code is made public athttps://github.com/krishnabits001/domain_specific_cl."}
{"id": "314", "content": "Although deep neural networks have been a dominant method for many 2D visiontasks, it is still challenging to apply them to 3D tasks, such as medical imagesegmentation, due to the limited amount of annotated 3D data and limitedcomputational resources. In this chapter, by rethinking the strategy to apply3D Convolutional Neural Networks to segment medical images, we propose a novel3D-based coarse-to-fine framework to efficiently tackle these challenges. Theproposed 3D-based framework outperforms their 2D counterparts by a large marginsince it can leverage the rich spatial information along all three axes. Wefurther analyze the threat of adversarial attacks on the proposed framework andshow how to defense against the attack. We conduct experiments on threedatasets, the NIH pancreas dataset, the JHMI pancreas dataset and the JHMIpathological cyst dataset, where the first two and the last one contain healthyand pathological pancreases respectively, and achieve the currentstate-of-the-art in terms of Dice-Sorensen Coefficient (DSC) on all of them.Especially, on the NIH pancreas segmentation dataset, we outperform theprevious best by an average of over $2 %$, and the worst case is improved by$7 %$ to reach almost $70 %$, which indicates the reliability of our frameworkin clinical applications. ", "id2": "313", "id3": "None"}
{"id": "315", "content": "Recent years have seen increasing use of supervised learning methods forsegmentation tasks. However, the predictive performance of these algorithmsdepends on the quality of labels. This problem is particularly pertinent in themedical image domain, where both the annotation cost and inter-observervariability are high. In a typical label acquisition process, different humanexperts provide their estimates of the true segmentation labels under theinfluence of their own biases and competence levels. Treating these noisylabels blindly as the ground truth limits the performance that automaticsegmentation algorithms can achieve. In this work, we present a method forjointly learning, from purely noisy observations alone, the reliability ofindividual annotators and the true segmentation label distributions, using twocoupled CNNs. The separation of the two is achieved by encouraging theestimated annotators to be maximally unreliable while achieving high fidelitywith the noisy training data. We first define a toy segmentation dataset basedon MNIST and study the properties of the proposed algorithm. We thendemonstrate the utility of the method on three public medical imagingsegmentation datasets with simulated (when necessary) and real diverseannotations: 1) MSLSC (multiple-sclerosis lesions); 2) BraTS (brain tumours);3) LIDC-IDRI (lung abnormalities). In all cases, our method outperformscompeting methods and relevant baselines particularly in cases where the numberof annotations is small and the amount of disagreement is large. Theexperiments also show strong ability to capture the complex spatialcharacteristics of annotators mistakes. ", "id2": "314", "id3": "None"}
{"id": "316", "content": "Standard segmentation of medical images based on full-supervisedconvolutional networks demands accurate dense annotations. Such learningframework is built on laborious manual annotation with restrict demands forexpertise, leading to insufficient high-quality labels. To overcome suchlimitation and exploit massive weakly labeled data, we relaxed the rigidlabeling requirement and developed a semi-supervised learning framework basedon a teacher-student fashion for organ and lesion segmentation with partialdense-labeled supervision and supplementary loose bounding-box supervisionwhich are easier to acquire. Observing the geometrical relation of an organ andits inner lesions in most cases, we propose a hierarchical organ-to-lesion(O2L) attention module in a teacher segmentor to produce pseudo-labels. Then astudent segmentor is trained with combinations of manual-labeled andpseudo-labeled annotations. We further proposed a localization branch realizedvia an aggregation of high-level features in a deep decoder to predictlocations of organ and lesion, which enriches student segmentor with preciselocalization information. We validated each design in our model on LiTSchallenge datasets by ablation study and showed its state-of-the-artperformance compared with recent methods. We show our model is robust to thequality of bounding box and achieves comparable performance compared withfull-supervised learning methods. ", "id2": "315", "id3": "None"}
{"id": "317", "content": "Continual learning protocols are attracting increasing attention from themedical imaging community. In a continual setup, data from different sourcesarrives sequentially and each batch is only available for a limited period.Given the inherent privacy risks associated with medical data, this setupreflects the reality of deployment for deep learning diagnostic radiologysystems. Many techniques exist to learn continuously for classification tasks,and several have been adapted to semantic segmentation. Yet most have at leastone of the following flaws: a) they rely too heavily on domain identityinformation during inference, or b) data as seen in early training stages doesnot profit from training with later data. In this work, we propose anevaluation framework that addresses both concerns, and introduce a fairmulti-model benchmark. We show that the benchmark outperforms two popularcontinual learning methods for the task of T2-weighted MR prostatesegmentation. ", "id2": "316", "id3": "None"}
{"id": "318", "content": "An automatic image segmentation procedure is an inevitable part of many imageanalyses and computer vision which deeply affect the rest of the system;therefore, a set of interactive segmentation evaluation methods cansubstantially simplify the system development process. This entry presents thestate of the art of quantitative evaluation metrics for color imagesegmentation methods by performing an analytical and comparative review of themeasures. The decision-making process in selecting a suitable evaluation metricis still very serious because each metric tends to favor a differentsegmentation method for each benchmark dataset. Furthermore, a conceptualcomparison of these metrics is provided at a high level of abstraction and isdiscussed for understanding the quantitative changes in different imagesegmentation results. ", "id2": "317", "id3": "None"}
{"id": "319", "content": "For the Convolutional Neural Networks (CNNs) applied in the intelligentdiagnosis of gastric cancer, existing methods mostly focus on individualcharacteristics or network frameworks without a policy to depict the integralinformation. Mainly, Conditional Random Field (CRF), an efficient and stablealgorithm for analyzing images containing complicated contents, cancharacterize spatial relation in images. In this paper, a novel HierarchicalConditional Random Field (HCRF) based Gastric Histopathology Image Segmentation(GHIS) method is proposed, which can automatically localize abnormal (cancer)regions in gastric histopathology images obtained by an optical microscope toassist histopathologists in medical work. This HCRF model is built up withhigher order potentials, including pixel-level and patch-level potentials, andgraph-based post-processing is applied to further improve its segmentationperformance. Especially, a CNN is trained to build up the pixel-levelpotentials and another three CNNs are fine-tuned to build up the patch-levelpotentials for sufficient spatial segmentation information. In the experiment,a hematoxylin and eosin (H&E) stained gastric histopathological dataset with560 abnormal images are divided into training, validation and test sets with aratio of 1 : 1 : 2. Finally, segmentation accuracy, recall and specificity of78.91%, 65.59%, and 81.33% are achieved on the test set. Our HCRF modeldemonstrates high segmentation performance and shows its effectiveness andfuture potential in the GHIS field. ", "id2": "318", "id3": "None"}
{"id": "320", "content": "Accurate and efficient catheter segmentation in 3D ultrasound (US) isessential for cardiac intervention. Currently, the state-of-the-artsegmentation algorithms are based on convolutional neural networks (CNNs),which achieved remarkable performances in a standard Cartesian volumetric data.Nevertheless, these approaches suffer the challenges of low efficiency and GPUunfriendly image size. Therefore, such difficulties and expensive hardwarerequirements become a bottleneck to build accurate and efficient segmentationmodels for real clinical application. In this paper, we propose a novel Frustumultrasound based catheter segmentation method. Specifically, Frustum ultrasoundis a polar coordinate based image, which includes same information of standardCartesian image but has much smaller size, which overcomes the bottleneck ofefficiency than conventional Cartesian images. Nevertheless, the irregular anddeformed Frustum images lead to more efforts for accurate voxel-levelannotation. To address this limitation, a weakly supervised learning frameworkis proposed, which only needs 3D bounding box annotations overlaying theregion-of-interest to training the CNNs. Although the bounding box annotationincludes noise and inaccurate annotation to mislead to model, it is addressedby the proposed pseudo label generated scheme. The labels of training voxelsare generated by incorporating class activation maps with line filtering, whichis iteratively updated during the training. Our experimental results show theproposed method achieved the state-of-the-art performance with an efficiency of0.25 second per volume. More crucially, the Frustum image segmentation providesa much faster and cheaper solution for segmentation in 3D US image, which meetthe demands of clinical applications. ", "id2": "319", "id3": "None"}
{"id": "321", "content": "Computer vision technology is widely used in biological and medical dataanalysis and understanding. However, there are still two major bottlenecks inthe field of cell membrane segmentation, which seriously hinder furtherresearch: lack of sufficient high-quality data and lack of suitable evaluationcriteria. In order to solve these two problems, this paper first proposes anUltra-high Resolution Image Segmentation dataset for the Cell membrane, calledU-RISC, the largest annotated Electron Microscopy (EM) dataset for the Cellmembrane with multiple iterative annotations and uncompressed high-resolutionraw data. During the analysis process of the U-RISC, we found that the currentpopular segmentation evaluation criteria are inconsistent with humanperception. This interesting phenomenon is confirmed by a subjective experimentinvolving twenty people. Furthermore, to resolve this inconsistency, we proposea new evaluation criterion called Perceptual Hausdorff Distance (PHD) tomeasure the quality of cell membrane segmentation results. Detailed performancecomparison and discussion of classic segmentation methods along with twoiterative manual annotation results under existing evaluation criteria and PHDis given. ", "id2": "320", "id3": "None"}
{"id": "322", "content": "This paper addresses representational block named Hierarchical-Split Block,which can be taken as a plug-and-play block to upgrade existing convolutionalneural networks, improves model performance significantly in a network.Hierarchical-Split Block contains many hierarchical split and concatenateconnections within one single residual block. We find multi-scale features isof great importance for numerous vision tasks. Moreover, Hierarchical-Splitblock is very flexible and efficient, which provides a large space of potentialnetwork architectures for different applications. In this work, we present acommon backbone based on Hierarchical-Split block for tasks: imageclassification, object detection, instance segmentation and semantic imagesegmentation/parsing. Our approach shows significant improvements over allthese core tasks in comparison with the baseline. As shown in Figure1, forimage classification, our 50-layers network(HS-ResNet50) achieves 81.28% top-1accuracy with competitive latency on ImageNet-1k dataset. It also outperformsmost state-of-the-art models. The source code and models will be available on:https://github.com/PaddlePaddle/PaddleClas ", "id2": "321", "id3": "None"}
{"id": "323", "content": "Road network and building footprint extraction is essential for manyapplications such as updating maps, traffic regulations, city planning,ride-hailing, disaster response  textit etc . Mapping road networks iscurrently both expensive and labor-intensive. Recently, improvements in imagesegmentation through the application of deep neural networks has shownpromising results in extracting road segments from large scale, high resolutionsatellite imagery. However, significant challenges remain due to lack of enoughlabeled training data needed to build models for industry grade applications.In this paper, we propose a two-stage transfer learning technique to improverobustness of semantic segmentation for satellite images that leverages noisypseudo ground truth masks obtained automatically (without human labor) fromcrowd-sourced OpenStreetMap (OSM) data. We further propose PyramidPooling-LinkNet (PP-LinkNet), an improved deep neural network for segmentationthat uses focal loss, poly learning rate, and context module. We demonstratethe strengths of our approach through evaluations done on three populardatasets over two tasks, namely, road extraction and building foot-printdetection. Specifically, we obtain 78.19 % meanIoU on SpaceNet buildingfootprint dataset, 67.03 % and 77.11 % on the road topology metric on SpaceNetand DeepGlobe road extraction dataset, respectively. ", "id2": "322", "id3": "None"}
{"id2": 1028, "id3": "322", "content": "Road network and building footprint extraction is essential for manyapplications such as updating maps, traffic regulations, city planning,ride-hailing, disaster response textit etc . Mapping road networks iscurrently both expensive and labor-intensive. Recently, improvements in imagesegmentation through the application of deep neural networks has shownpromising results in extracting road segments from large scale, high resolutionsatellite imagery. However, significant challenges remain due to lack of enoughlabeled training data needed to build models for industry grade applications.In this paper, we propose a two-stage transfer learning technique to improverobustness of semantic segmentation for satellite images that leverages noisypseudo ground truth masks obtained automatically (without human labor) fromcrowd-sourced OpenStreetMap (OSM) data. We further propose PyramidPooling-LinkNet (PP-LinkNet), an improved deep neural network for segmentationthat uses focal loss, poly learning rate, and context module. We demonstratethe strengths of our approach through evaluations done on three populardatasets over two tasks, namely, road extraction and building foot-printdetection. Specifically, we obtain 78.19 % meanIoU on SpaceNet buildingfootprint dataset, 67.03 % and 77.11 % on the road topology metric on SpaceNetand DeepGlobe road extraction dataset, respectively."}
{"id": "324", "content": "Deep convolutional neural networks have significantly boosted the performanceof fundus image segmentation when test datasets have the same distribution asthe training datasets. However, in clinical practice, medical images oftenexhibit variations in appearance for various reasons, e.g., different scannervendors and image quality. These distribution discrepancies could lead the deepnetworks to over-fit on the training datasets and lack generalization abilityon the unseen test datasets. To alleviate this issue, we present a novelDomain-oriented Feature Embedding (DoFE) framework to improve thegeneralization ability of CNNs on unseen target domains by exploring theknowledge from multiple source domains. Our DoFE framework dynamically enrichesthe image features with additional domain prior knowledge learned frommulti-source domains to make the semantic features more discriminative.Specifically, we introduce a Domain Knowledge Pool to learn and memorize theprior information extracted from multi-source domains. Then the original imagefeatures are augmented with domain-oriented aggregated features, which areinduced from the knowledge pool based on the similarity between the input imageand multi-source domain images. We further design a novel domain codeprediction branch to infer this similarity and employ an attention-guidedmechanism to dynamically combine the aggregated features with the semanticfeatures. We comprehensively evaluate our DoFE framework on two fundus imagesegmentation tasks, including the optic cup and disc segmentation and vesselsegmentation. Our DoFE framework generates satisfying segmentation results onunseen datasets and surpasses other domain generalization and networkregularization methods. ", "id2": "323", "id3": "None"}
{"id2": 1029, "id3": "323", "content": "Deep convolutional neural networks have significantly boosted the performanceof fundus image segmentation when test datasets have the same distribution asthe training datasets. However, in clinical practice, medical images oftenexhibit variations in appearance for various reasons, e.g., different scannervendors and image quality. These distribution discrepancies could lead the deepnetworks to over-fit on the training datasets and lack generalization abilityon the unseen test datasets. To alleviate this issue, we present a novelDomain-oriented Feature Embedding (DoFE) framework to improve thegeneralization ability of CNNs on unseen target domains by exploring theknowledge from multiple source domains. Our DoFE framework dynamically enrichesthe image features with additional domain prior knowledge learned frommulti-source domains to make the semantic features more discriminative.Specifically, we introduce a Domain Knowledge Pool to learn and memorize theprior information extracted from multi-source domains. Then the original imagefeatures are augmented with domain-oriented aggregated features, which areinduced from the knowledge pool based on the similarity between the input imageand multi-source domain images. We further design a novel domain codeprediction branch to infer this similarity and employ an attention-guidedmechanism to dynamically combine the aggregated features with the semanticfeatures. We comprehensively evaluate our DoFE framework on two fundus imagesegmentation tasks, including the optic cup and disc segmentation and vesselsegmentation. Our DoFE framework generates satisfying segmentation results onunseen datasets and surpasses other domain generalization and networkregularization methods."}
{"id": "325", "content": "Few-shot semantic segmentation (FSS) has great potential for medical imagingapplications. Most of the existing FSS techniques require abundant annotatedsemantic classes for training. However, these methods may not be applicable formedical images due to the lack of annotations. To address this problem we makeseveral contributions: (1) A novel self-supervised FSS framework for medicalimages in order to eliminate the requirement for annotations during training.Additionally, superpixel-based pseudo-labels are generated to providesupervision; (2) An adaptive local prototype pooling module plugged intoprototypical networks, to solve the common challenging foreground-backgroundimbalance problem in medical image segmentation; (3) We demonstrate the generalapplicability of the proposed approach for medical images using three differenttasks: abdominal organ segmentation for CT and MRI, as well as cardiacsegmentation for MRI. Our results show that, for medical image segmentation,the proposed method outperforms conventional FSS methods which require manualannotations for training. ", "id2": "324", "id3": "None"}
{"id": "326", "content": "We present a novel region based active learning method for semantic imagesegmentation, called MetaBox+. For acquisition, we train a meta regressionmodel to estimate the segment-wise Intersection over Union (IoU) of eachpredicted segment of unlabeled images. This can be understood as an estimationof segment-wise prediction quality. Queried regions are supposed to minimize tocompeting targets, i.e., low predicted IoU values / segmentation quality andlow estimated annotation costs. For estimating the latter we propose a simplebut practical method for annotation cost estimation. We compare our method toentropy based methods, where we consider the entropy as uncertainty of theprediction. The comparison and analysis of the results provide insights intoannotation costs as well as robustness and variance of the methods. Numericalexperiments conducted with two different networks on the Cityscapes datasetclearly demonstrate a reduction of annotation effort compared to randomacquisition. Noteworthily, we achieve 95%of the mean Intersection over Union(mIoU), using MetaBox+ compared to when training with the full dataset, withonly 10.47% / 32.01% annotation effort for the two networks, respectively. ", "id2": "325", "id3": "None"}
{"id": "327", "content": "Referring image segmentation aims to predict the foreground mask of theobject referred by a natural language sentence. Multimodal context of thesentence is crucial to distinguish the referent from the background. Existingmethods either insufficiently or redundantly model the multimodal context. Totackle this problem, we propose a gather-propagate-distribute scheme to modelmultimodal context by cross-modal interaction and implement this scheme as anovel Linguistic Structure guided Context Modeling (LSCM) module. Our LSCMmodule builds a Dependency Parsing Tree suppressed Word Graph (DPT-WG) whichguides all the words to include valid multimodal context of the sentence whileexcluding disturbing ones through three steps over the multimodal feature,i.e., gathering, constrained propagation and distributing. Extensiveexperiments on four benchmarks demonstrate that our method outperforms all theprevious state-of-the-arts. ", "id2": "326", "id3": "None"}
{"id": "328", "content": "Image segmentation has long been a basic problem in computer vision.Depth-wise Layering is a kind of segmentation that slices an image in adepth-wise sequence unlike the conventional image segmentation problems dealingwith surface-wise decomposition. The proposed Depth-wise Layering techniqueuses a single depth image of a static scene to slice it into multiple layers.The technique employs a thresholding approach to segment rows of the densedepth map into smaller partitions called Line-Segments in this paper. Then, ituses the line-segment labelling method to identify number of objects and layersof the scene independently. The final stage is to link objects of the scene totheir respective object-layers. We evaluate the efficiency of the proposedtechnique by applying that on many images along with their dense depth maps.The experiments have shown promising results of layering. ", "id2": "327", "id3": "None"}
{"id": "329", "content": "Image normalization is a building block in medical image analysis.Conventional approaches are customarily utilized on a per-dataset basis. Thisstrategy, however, prevents the current normalization algorithms from fullyexploiting the complex joint information available across multiple datasets.Consequently, ignoring such joint information has a direct impact on theperformance of segmentation algorithms. This paper proposes to revisit theconventional image normalization approach by instead learning a commonnormalizing function across multiple datasets. Jointly normalizing multipledatasets is shown to yield consistent normalized images as well as an improvedimage segmentation. To do so, a fully automated adversarial and task-drivennormalization approach is employed as it facilitates the training of realisticand interpretable images while keeping performance on-par with thestate-of-the-art. The adversarial training of our network aims at finding theoptimal transfer function to improve both the segmentation accuracy and thegeneration of realistic images. We evaluated the performance of our normalizeron both infant and adult brains images from the iSEG, MRBrainS and ABIDEdatasets. Results reveal the potential of our normalization approach forsegmentation, with Dice improvements of up to 57.5% over our baseline. Ourmethod can also enhance data availability by increasing the number of samplesavailable when learning from multiple imaging domains. ", "id2": "328", "id3": "None"}
{"id": "330", "content": "In recent years, deep learning techniques (e.g., U-Net, DeepLab) haveachieved tremendous success in image segmentation. The performance of thesemodels heavily relies on high-quality ground truth segment labels.Unfortunately, in many real-world problems, ground truth segment labels oftenhave geometric annotation errors due to manual annotation mistakes, GPS errors,or visually interpreting background imagery at a coarse resolution. Suchlocation errors will significantly impact the training performance of existingdeep learning algorithms. Existing research on label errors either modelsground truth errors in label semantics (assuming label locations to be correct)or models label location errors with simple square patch shifting. Thesemethods cannot fully incorporate the geometric properties of label locationerrors. To fill the gap, this paper proposes a generic learning framework basedon the EM algorithm to update deep learning model parameters and infer hiddentrue label locations simultaneously. Evaluations on a real-world hydrologicaldataset in the streamline refinement application show that the proposedframework outperforms baseline methods in classification accuracy (reducing thenumber of false positives by 67% and reducing the number of false negatives by55%). ", "id2": "329", "id3": "None"}
{"id": "331", "content": "Referring image segmentation aims at segmenting the foreground masks of theentities that can well match the description given in the natural languageexpression. Previous approaches tackle this problem using implicit featureinteraction and fusion between visual and linguistic modalities, but usuallyfail to explore informative words of the expression to well align features fromthe two modalities for accurately identifying the referred entity. In thispaper, we propose a Cross-Modal Progressive Comprehension (CMPC) module and aText-Guided Feature Exchange (TGFE) module to effectively address thechallenging task. Concretely, the CMPC module first employs entity andattribute words to perceive all the related entities that might be consideredby the expression. Then, the relational words are adopted to highlight thecorrect entity as well as suppress other irrelevant ones by multimodal graphreasoning. In addition to the CMPC module, we further leverage a simple yeteffective TGFE module to integrate the reasoned multimodal features fromdifferent levels with the guidance of textual information. In this way,features from multi-levels could communicate with each other and be refinedbased on the textual context. We conduct extensive experiments on four popularreferring segmentation benchmarks and achieve new state-of-the-artperformances. ", "id2": "330", "id3": "None"}
{"id": "332", "content": "The task of video object segmentation with referring expressions(language-guided VOS) is to, given a linguistic phrase and a video, generatebinary masks for the object to which the phrase refers. Our work argues thatexisting benchmarks used for this task are mainly composed of trivial cases, inwhich referents can be identified with simple phrases. Our analysis relies on anew categorization of the phrases in the DAVIS-2017 and Actor-Action datasetsinto trivial and non-trivial REs, with the non-trivial REs annotated with sevenRE semantic categories. We leverage this data to analyze the results of RefVOS,a novel neural network that obtains competitive results for the task oflanguage-guided image segmentation and state of the art results forlanguage-guided VOS. Our study indicates that the major challenges for the taskare related to understanding motion and static actions. ", "id2": "331", "id3": "None"}
{"id": "333", "content": "In this paper, we present a novel neural network using multi scale featurefusion at various scales for accurate and efficient semantic imagesegmentation. We used ResNet based feature extractor, dilated convolutionallayers in downsampling part, atrous convolutional layers in the upsampling partand used concat operation to merge them. A new attention module is proposed toencode more contextual information and enhance the receptive field of thenetwork. We present an in depth theoretical analysis of our network withtraining and optimization details. Our network was trained and tested on theCamvid dataset and Cityscapes dataset using mean accuracy per class andIntersection Over Union (IOU) as the evaluation metrics. Our model outperformsprevious state of the art methods on semantic segmentation achieving mean IOUvalue of 74.12 while running at >100 FPS. ", "id2": "332", "id3": "None"}
{"id": "334", "content": "Efficient and easy segmentation of images and volumes is of great practicalimportance. Segmentation problems that motivate our approach originate frommicroscopy imaging commonly used in materials science, medicine, and biology.We formulate image segmentation as a probabilistic pixel classificationproblem, and we apply segmentation as a step towards characterising imagecontent. Our method allows the user to define structures of interest byinteractively marking a subset of pixels. Thanks to the real-time feedback, theuser can place new markings strategically, depending on the current outcome.The final pixel classification may be obtained from a very modest user input.An important ingredient of our method is a graph that encodes image content.This graph is built in an unsupervised manner during initialisation and isbased on clustering of image features. Since we combine a limited amount ofuser-labelled data with the clustering information obtained from the unlabelledparts of the image, our method fits in the general framework of semi-supervisedlearning. We demonstrate how this can be a very efficient approach tosegmentation through pixel classification. ", "id2": "333", "id3": "None"}
{"id": "335", "content": "Semantic image segmentation is one of fastest growing areas in computervision with a variety of applications. In many areas, such as robotics andautonomous vehicles, semantic image segmentation is crucial, since it providesthe necessary context for actions to be taken based on a scene understanding atthe pixel level. Moreover, the success of medical diagnosis and treatmentrelies on the extremely accurate understanding of the data under considerationand semantic image segmentation is one of the important tools in many cases.Recent developments in deep learning have provided a host of tools to tacklethis problem efficiently and with increased accuracy. This work provides acomprehensive analysis of state-of-the-art deep learning architectures in imagesegmentation and, more importantly, an extensive list of techniques to achievefast inference and computational efficiency. The origins of these techniques aswell as their strengths and trade-offs are discussed with an in-depth analysisof their impact in the area. The best-performing architectures are summarizedwith a list of methods used to achieve these state-of-the-art results. ", "id2": "334", "id3": "None"}
{"id": "336", "content": "Medical image annotation is a major hurdle for developing precise and robustmachine learning models. Annotation is expensive, time-consuming, and oftenrequires expert knowledge, particularly in the medical field. Here, we suggestusing minimal user interaction in the form of extreme point clicks to train asegmentation model which, in effect, can be used to speed up medical imageannotation. An initial segmentation is generated based on the extreme pointsutilizing the random walker algorithm. This initial segmentation is then usedas a noisy supervision signal to train a fully convolutional network that cansegment the organ of interest, based on the provided user clicks. Throughexperimentation on several medical imaging datasets, we show that thepredictions of the network can be refined using several rounds of training withthe prediction from the same weakly annotated data. Further improvements areshown utilizing the clicked points within a custom-designed loss and attentionmechanism. Our approach has the potential to speed up the process of generatingnew training datasets for the development of new machine learning and deeplearning-based models for, but not exclusively, medical image analysis. ", "id2": "335", "id3": "None"}
{"id": "337", "content": "Dense pixel-wise classification maps output by deep neural networks are ofextreme importance for scene understanding. However, these maps are oftenpartially inaccurate due to a variety of possible factors. Therefore, wepropose to interactively refine them within a framework named DISCA (Deep ImageSegmentation with Continual Adaptation). It consists of continually adapting aneural network to a target image using an interactive learning process withsparse user annotations as ground-truth. We show through experiments on threedatasets using synthesized annotations the benefits of the approach, reachingan IoU improvement up to 4.7% for ten sampled clicks. Finally, we exhibit thatour approach can be particularly rewarding when it is faced to additionalissues such as domain adaptation. ", "id2": "336", "id3": "None"}
{"id": "338", "content": "Todays success of state of the art methods for semantic segmentation isdriven by large datasets. Data is considered an important asset that needs tobe protected, as the collection and annotation of such datasets comes atsignificant efforts and associated costs. In addition, visual data mightcontain private or sensitive information, that makes it equally unsuited forpublic release. Unfortunately, recent work on membership inference in thebroader area of adversarial machine learning and inference attacks on machinelearning models has shown that even black box classifiers leak information onthe dataset that they were trained on. We show that such membership inferenceattacks can be successfully carried out on complex, state of the art models forsemantic segmentation. In order to mitigate the associated risks, we also studya series of defenses against such membership inference attacks and findeffective counter measures against the existing risks with little effect on theutility of the segmentation method. Finally, we extensively evaluate ourattacks and defenses on a range of relevant real-world datasets: Cityscapes,BDD100K, and Mapillary Vistas. ", "id2": "337", "id3": "None"}
{"id": "339", "content": "We introduce a method for training neural networks to perform image or volumesegmentation in which prior knowledge about the topology of the segmentedobject can be explicitly provided and then incorporated into the trainingprocess. By using the differentiable properties of persistent homology, aconcept used in topological data analysis, we can specify the desired topologyof segmented objects in terms of their Betti numbers and then drive theproposed segmentations to contain the specified topological features.Importantly this process does not require any ground-truth labels, just priorknowledge of the topology of the structure being segmented. We demonstrate ourapproach in three experiments. Firstly we create a synthetic task in whichhandwritten MNIST digits are de-noised, and show that using this kind oftopological prior knowledge in the training of the network significantlyimproves the quality of the de-noised digits. Secondly we perform an experimentin which the task is segmenting the myocardium of the left ventricle fromcardiac magnetic resonance images. We show that the incorporation of the priorknowledge of the topology of this anatomy improves the resulting segmentationsin terms of both the topological accuracy and the Dice coefficient. Thirdly, weextend the method to 3D volumes and demonstrate its performance on the task ofsegmenting the placenta from ultrasound data, again showing that incorporatingtopological priors improves performance on this challenging task. We find thatembedding explicit prior knowledge in neural network segmentation tasks is mostbeneficial when the segmentation task is especially challenging and that it canbe used in either a semi-supervised or post-processing context to extract auseful training gradient from images without pixelwise labels. ", "id2": "338", "id3": "None"}
{"id": "340", "content": "Aggregating multi-level feature representation plays a critical role inachieving robust volumetric medical image segmentation, which is important forthe auxiliary diagnosis and treatment. Unlike the recent neural architecturesearch (NAS) methods that typically searched the optimal operators in eachnetwork layer, but missed a good strategy to search for feature aggregations,this paper proposes a novel NAS method for 3D medical image segmentation, namedUXNet, which searches both the scale-wise feature aggregation strategies aswell as the block-wise operators in the encoder-decoder network. UXNet hasseveral appealing benefits. (1) It significantly improves flexibility of theclassical UNet architecture, which only aggregates feature representations ofencoder and decoder in equivalent resolution. (2) A continuous relaxation ofUXNet is carefully designed, enabling its searching scheme performed in anefficient differentiable manner. (3) Extensive experiments demonstrate theeffectiveness of UXNet compared with recent NAS methods for medical imagesegmentation. The architecture discovered by UXNet outperforms existingstate-of-the-art models in terms of Dice on several public 3D medical imagesegmentation benchmarks, especially for the boundary locations and tinytissues. The searching computational complexity of UXNet is cheap, enabling tosearch a network with the best performance less than 1.5 days on two TitanXPGPUs. ", "id2": "339", "id3": "None"}
{"id": "341", "content": "Deep Learning (DL) models are becoming larger, because the increase in modelsize might offer significant accuracy gain. To enable the training of largedeep networks, data parallelism and model parallelism are two well-knownapproaches for parallel training. However, data parallelism does not helpreduce memory footprint per device. In this work, we introduce Large deep 3DConvNets with Automated Model Parallelism (LAMP) and investigate the impact ofboth inputs and deep 3D ConvNets size on segmentation accuracy. Throughautomated model parallelism, it is feasible to train large deep 3D ConvNetswith a large input patch, even the whole image. Extensive experimentsdemonstrate that, facilitated by the automated model parallelism, thesegmentation accuracy can be improved through increasing model size and inputcontext size, and large input yields significant inference speedup comparedwith sliding window of small patches in the inference. Code isavailable footnote https://monai.io/research/lamp-automated-model-parallelism . ", "id2": "340", "id3": "None"}
{"id2": 1030, "id3": "340", "content": "Deep Learning (DL) models are becoming larger, because the increase in modelsize might offer significant accuracy gain. To enable the training of largedeep networks, data parallelism and model parallelism are two well-knownapproaches for parallel training. However, data parallelism does not helpreduce memory footprint per device. In this work, we introduce Large deep 3DConvNets with Automated Model Parallelism (LAMP) and investigate the impact ofboth inputs and deep 3D ConvNets size on segmentation accuracy. Throughautomated model parallelism, it is feasible to train large deep 3D ConvNetswith a large input patch, even the whole image. Extensive experimentsdemonstrate that, facilitated by the automated model parallelism, thesegmentation accuracy can be improved through increasing model size and inputcontext size, and large input yields significant inference speedup comparedwith sliding window of small patches in the inference. Code isavailable footnote https://monai.io/research/lamp-automated-model-parallelism ."}
{"id": "342", "content": "Games such as go, chess and checkers have multiple equivalent game states,i.e. multiple board positions where symmetrical and opposite moves should bemade. These equivalences are not exploited by current state of the art neuralagents which instead must relearn similar information, thereby wastingcomputing time. Group equivariant CNNs in existing work create networks whichcan exploit symmetries to improve learning, however, they lack theexpressiveness to correctly reflect the move embeddings necessary for games. Weintroduce Finite Group Neural Networks (FGNNs), a method for creating agentswith an innate understanding of these board positions. FGNNs are shown toimprove the performance of networks playing checkers (draughts), and can beeasily adapted to other games and learning problems. Additionally, FGNNs can becreated from existing network architectures. These include, for the first time,those with skip connections and arbitrary layer types. We demonstrate that anequivariant version of U-Net (FGNN-U-Net) outperforms the unmodified network inimage segmentation. ", "id2": "341", "id3": "None"}
{"id2": 1031, "id3": "341", "content": "Games such as go, chess and checkers have multiple equivalent game states,i.e. multiple board positions where symmetrical and opposite moves should bemade. These equivalences are not exploited by current state of the art neuralagents which instead must relearn similar information, thereby wastingcomputing time. Group equivariant CNNs in existing work create networks whichcan exploit symmetries to improve learning, however, they lack theexpressiveness to correctly reflect the move embeddings necessary for games. Weintroduce Finite Group Neural Networks (FGNNs), a method for creating agentswith an innate understanding of these board positions. FGNNs are shown toimprove the performance of networks playing checkers (draughts), and can beeasily adapted to other games and learning problems. Additionally, FGNNs can becreated from existing network architectures. These include, for the first time,those with skip connections and arbitrary layer types. We demonstrate that anequivariant version of U-Net (FGNN-U-Net) outperforms the unmodified network inimage segmentation."}
{"id": "343", "content": "The need for labour intensive pixel-wise annotation is a major limitation ofmany fully supervised learning methods for segmenting bioimages that cancontain numerous object instances with thin separations. In this paper, weintroduce a deep convolutional neural network for microscopy imagesegmentation. Annotation issues are circumvented by letting the network beingtrainable on coarse labels combined with only a very small number of imageswith pixel-wise annotations. We call this new labelling strategy lazy labels.Image segmentation is stratified into three connected tasks: rough inner regiondetection, object separation and pixel-wise segmentation. These tasks arelearned in an end-to-end multi-task learning framework. The method isdemonstrated on two microscopy datasets, where we show that the model givesaccurate segmentation results even if exact boundary labels are missing for amajority of annotated data. It brings more flexibility and efficiency fortraining deep neural networks that are data hungry and is applicable tobiomedical images with poor contrast at the object boundaries or with diversetextures and repeated patterns. ", "id2": "342", "id3": "None"}
{"id": "344", "content": "Deep learning (DL)-based models have demonstrated good performance in medicalimage segmentation. However, the models trained on a known dataset often failwhen performed on an unseen dataset collected from different centers, vendorsand disease populations. In this work, we present a random style transfernetwork to tackle the domain generalization problem for multi-vendor and centercardiac image segmentation. Style transfer is used to generate training datawith a wider distribution/ heterogeneity, namely domain augmentation. As thetarget domain could be unknown, we randomly generate a modality vector for thetarget modality in the style transfer stage, to simulate the domain shift forunknown domains. The model can be trained in a semi-supervised manner bysimultaneously optimizing a supervised segmentation and an unsupervised styletranslation objective. Besides, the framework incorporates the spatialinformation and shape prior of the target by introducing two regularizationterms. We evaluated the proposed framework on 40 subjects from the M &Mschallenge2020, and obtained promising performance in the segmentation for datafrom unknown vendors and centers. ", "id2": "343", "id3": "None"}
{"id": "345", "content": "Slope difference distribution (SDD) is computed for the one-dimensionalcurve. It is not only robust to calculate the partitioning point to separatethe curve logically, but also robust to calculate the clustering center of eachpart of the separated curve. SDD has been proposed for image segmentation andit outperforms all existing image segmentation methods. For verificationpurpose, we have made the Matlab codes of comparing SDD method with existingimage segmentation methods freely available at Matlab Central. The contour ofthe object is similar to the histogram of the image. Thus, feature detection bySDD from the contour of the object is also feasible. In this letter, SDDfeatures are defined and they form the sparse representation of the objectcontour. The reference model of each object is built based on the SDD featuresand then model matching is used for on line object recognition. Theexperimental results are very encouraging. For the gesture recognition, SDDachieved 100% accuracy for two public datasets: the NUS dataset and thenear-infrared dataset. For the object recognition, SDD achieved 100% accuracyfor the Kimia 99 dataset. ", "id2": "344", "id3": "None"}
{"id": "346", "content": "Given a 3D surface defined by an elevation function on a 2D grid as well asnon-spatial features observed at each pixel, the problem of surfacesegmentation aims to classify pixels into contiguous classes based on bothnon-spatial features and surface topology. The problem has importantapplications in hydrology, planetary science, and biochemistry but is uniquelychallenging for several reasons. First, the spatial extent of class segmentsfollows surface contours in the topological space, regardless of their spatialshapes and directions. Second, the topological structure exists in multiplespatial scales based on different surface resolutions. Existing widelysuccessful deep learning models for image segmentation are often not applicabledue to their reliance on convolution and pooling operations to learn regularstructural patterns on a grid. In contrast, we propose to represent surfacetopological structure by a contour tree skeleton, which is a polytree capturingthe evolution of surface contours at different elevation levels. We furtherdesign a graph neural network based on the contour tree hierarchy to modelsurface topological structure at different spatial scales. Experimentalevaluations based on real-world hydrological datasets show that our modeloutperforms several baseline methods in classification accuracy. ", "id2": "345", "id3": "None"}
{"id2": 1032, "id3": "345", "content": "Given a 3D surface defined by an elevation function on a 2D grid as well asnon-spatial features observed at each pixel, the problem of surfacesegmentation aims to classify pixels into contiguous classes based on bothnon-spatial features and surface topology. The problem has importantapplications in hydrology, planetary science, and biochemistry but is uniquelychallenging for several reasons. First, the spatial extent of class segmentsfollows surface contours in the topological space, regardless of their spatialshapes and directions. Second, the topological structure exists in multiplespatial scales based on different surface resolutions. Existing widelysuccessful deep learning models for image segmentation are often not applicabledue to their reliance on convolution and pooling operations to learn regularstructural patterns on a grid. In contrast, we propose to represent surfacetopological structure by a contour tree skeleton, which is a polytree capturingthe evolution of surface contours at different elevation levels. We furtherdesign a graph neural network based on the contour tree hierarchy to modelsurface topological structure at different spatial scales. Experimentalevaluations based on real-world hydrological datasets show that our modeloutperforms several baseline methods in classification accuracy."}
{"id": "347", "content": "Image segmentation methods are usually trained with pixel-level annotations,which require significant human effort to collect. The most common solution toaddress this constraint is to implement weakly-supervised pipelines trainedwith lower forms of supervision, such as bounding boxes or scribbles. Anotheroption are semi-supervised methods, which leverage a large amount of unlabeleddata and a limited number of strongly-labeled samples. In this second setup,samples to be strongly-annotated can be selected randomly or with an activelearning mechanism that chooses the ones that will maximize the modelperformance. In this work, we propose a sample selection approach to decidewhich samples to annotate for semi-supervised instance segmentation. Our methodconsists in first predicting pseudo-masks for the unlabeled pool of samples,together with a score predicting the quality of the mask. This score is anestimate of the Intersection Over Union (IoU) of the segment with the groundtruth mask. We study which samples are better to annotate given the qualityscore, and show how our approach outperforms a random selection, leading toimproved performance for semi-supervised instance segmentation with lowannotation budgets. ", "id2": "346", "id3": "None"}
{"id": "348", "content": "Incorporating a human-in-the-loop system when deploying automated decisionsupport is critical in healthcare contexts to create trust, as well as providereliable performance on a patient-to-patient basis. Deep learning methods whilehaving high performance, do not allow for this patient-centered approach due tothe lack of uncertainty representation. Thus, we present a framework ofuncertainty representation evaluated for medical image segmentation, usingMCU-Net which combines a U-Net with Monte Carlo Dropout, evaluated with fourdifferent uncertainty metrics. The framework augments this by adding ahuman-in-the-loop aspect based on an uncertainty threshold for automatedreferral of uncertain cases to a medical professional. We demonstrate thatMCU-Net combined with epistemic uncertainty and an uncertainty threshold tunedfor this application maximizes automated performance on an individual patientlevel, yet refers truly uncertain cases. This is a step towards uncertaintyrepresentations when deploying machine learning based decision support inhealthcare settings. ", "id2": "347", "id3": "None"}
{"id": "349", "content": "This work presents use of Fully Convolutional Network (FCN-8) for semanticsegmentation of high-resolution RGB earth surface satel-lite images into landuse land cover (LULC) categories. Specically, we propose a non-overlappinggrid-based approach to train a Fully Convo-lutional Network (FCN-8) with vgg-16weights to segment satellite im-ages into four (forest, built-up, farmland andwater) classes. The FCN-8 semantically projects the discriminating features inlower resolution learned by the encoder onto the pixel space in higherresolution to get a dense classi cation. We experimented the proposed systemwith Gaofen-2 image dataset, that contains 150 images of over 60 di erentcities in china. For comparison, we used available ground-truth along withimages segmented using a widely used commeriial GIS software calledeCogni-tion. With the proposed non-overlapping grid-based approach, FCN-8obtains signi cantly improved performance, than the eCognition soft-ware. Ourmodel achieves average accuracy of 91.0% and average Inter-section over Union(IoU) of 0.84. In contrast, eCognitions average accu-racy is 74.0% and IoU is0.60. This paper also reports a detail analysis of errors occurred at the LULCboundary. ", "id2": "348", "id3": "None"}
{"id2": 1033, "id3": "348", "content": "This work presents use of Fully Convolutional Network (FCN-8) for semanticsegmentation of high-resolution RGB earth surface satel-lite images into landuse land cover (LULC) categories. Specically, we propose a non-overlappinggrid-based approach to train a Fully Convo-lutional Network (FCN-8) with vgg-16weights to segment satellite im-ages into four (forest, built-up, farmland andwater) classes. The FCN-8 semantically projects the discriminating features inlower resolution learned by the encoder onto the pixel space in higherresolution to get a dense classi cation. We experimented the proposed systemwith Gaofen-2 image dataset, that contains 150 images of over 60 di erentcities in china. For comparison, we used available ground-truth along withimages segmented using a widely used commeriial GIS software calledeCogni-tion. With the proposed non-overlapping grid-based approach, FCN-8obtains signi cantly improved performance, than the eCognition soft-ware. Ourmodel achieves average accuracy of 91.0% and average Inter-section over Union(IoU) of 0.84. In contrast, eCognitions average accu-racy is 74.0% and IoU is0.60. This paper also reports a detail analysis of errors occurred at the LULCboundary."}
{"id": "350", "content": "Semantic segmentation is a critical method in the field of autonomousdriving. When performing semantic image segmentation, a wider field of view(FoV) helps to obtain more information about the surrounding environment,making automatic driving safer and more reliable, which could be offered byfisheye cameras. However, large public fisheye datasets are not available, andthe fisheye images captured by the fisheye camera with large FoV comes withlarge distortion, so commonly-used semantic segmentation model cannot bedirectly utilized. In this paper, a seven degrees of freedom (DoF) augmentationmethod is proposed to transform rectilinear image to fisheye image in a morecomprehensive way. In the training process, rectilinear images are transformedinto fisheye images in seven DoF, which simulates the fisheye images taken bycameras of different positions, orientations and focal lengths. The resultshows that training with the seven-DoF augmentation can improve the modelsaccuracy and robustness against different distorted fisheye data. Thisseven-DoF augmentation provides a universal semantic segmentation solution forfisheye cameras in different autonomous driving applications. Also, we providespecific parameter settings of the augmentation for autonomous driving. Atlast, we tested our universal semantic segmentation model on real fisheyeimages and obtained satisfactory results. The code and configurations arereleased at https://github.com/Yaozhuwa/FisheyeSeg. ", "id2": "349", "id3": "None"}
{"id": "351", "content": "This paper presents an interactive approach for multi-class segmentation ofaerial images. Precisely, it is based on a deep neural network which exploitsboth RGB images and annotations. Starting from an initial output based on theimage only, our network then interactively refines this segmentation map usinga concatenation of the image and user annotations. Importantly, userannotations modify the inputs of the network - not its weights - enabling afast and smooth process. Through experiments on two public aerial datasets, weshow that user annotations are extremely rewarding: each click corrects roughly5000 pixels. We analyze the impact of different aspects of our framework suchas the representation of the annotations, the volume of training data or thenetwork architecture. Code is available at https://github.com/delair-ai/DISIR. ", "id2": "350", "id3": "None"}
{"id": "352", "content": "The 3D volumetric shape of the hearts left ventricle (LV) myocardium (MYO)wall provides important information for diagnosis of cardiac disease andinvasive procedure navigation. Many cardiac image segmentation methods haverelied on detection of region-of-interest as a pre-requisite for shapesegmentation and modeling. With segmentation results, a 3D surface mesh and acorresponding point cloud of the segmented cardiac volume can be reconstructedfor further analyses. Although state-of-the-art methods (e.g., U-Net) haveachieved decent performance on cardiac image segmentation in terms of accuracy,these segmentation results can still suffer from imaging artifacts and noise,which will lead to inaccurate shape modeling results. In this paper, we proposea PC-U net that jointly reconstructs the point cloud of the LV MYO walldirectly from volumes of 2D CT slices and generates its segmentation masks fromthe predicted 3D point cloud. Extensive experimental results show that byincorporating a shape prior from the point cloud, the segmentation masks aremore accurate than the state-of-the-art U-Net results in terms of Dicescoefficient and Hausdorff distance.The proposed joint learning framework of ourPC-U net is beneficial for automatic cardiac image analysis tasks because itcan obtain simultaneously the 3D shape and segmentation of the LV MYO walls. ", "id2": "351", "id3": "None"}
{"id2": 1034, "id3": "351", "content": "The 3D volumetric shape of the hearts left ventricle (LV) myocardium (MYO)wall provides important information for diagnosis of cardiac disease andinvasive procedure navigation. Many cardiac image segmentation methods haverelied on detection of region-of-interest as a pre-requisite for shapesegmentation and modeling. With segmentation results, a 3D surface mesh and acorresponding point cloud of the segmented cardiac volume can be reconstructedfor further analyses. Although state-of-the-art methods (e.g., U-Net) haveachieved decent performance on cardiac image segmentation in terms of accuracy,these segmentation results can still suffer from imaging artifacts and noise,which will lead to inaccurate shape modeling results. In this paper, we proposea PC-U net that jointly reconstructs the point cloud of the LV MYO walldirectly from volumes of 2D CT slices and generates its segmentation masks fromthe predicted 3D point cloud. Extensive experimental results show that byincorporating a shape prior from the point cloud, the segmentation masks aremore accurate than the state-of-the-art U-Net results in terms of Dicescoefficient and Hausdorff distance.The proposed joint learning framework of ourPC-U net is beneficial for automatic cardiac image analysis tasks because itcan obtain simultaneously the 3D shape and segmentation of the LV MYO walls."}
{"id": "353", "content": "Real-time semantic video segmentation is a challenging task due to the strictrequirements of inference speed. Recent approaches mainly devote great effortsto reducing the model size for high efficiency. In this paper, we rethink thisproblem from a different viewpoint: using knowledge contained in compressedvideos. We propose a simple and effective framework, dubbed TapLab, to tap intoresources from the compressed domain. Specifically, we design a fast featurewarping module using motion vectors for acceleration. To reduce the noiseintroduced by motion vectors, we design a residual-guided correction module anda residual-guided frame selection module using residuals. TapLab significantlyreduces redundant computations of the state-of-the-art fast semantic imagesegmentation models, running 3 to 10 times faster with controllable accuracydegradation. The experimental results show that TapLab achieves 70.6% mIoU onthe Cityscapes dataset at 99.8 FPS with a single GPU card for the 1024x2048videos. A high-speed version even reaches the speed of 160+ FPS. Codes will beavailable soon at https://github.com/Sixkplus/TapLab. ", "id2": "352", "id3": "None"}
{"id": "354", "content": "This paper describes the results of formally evaluating the MCV (Markovconcurrent vision) image labeling algorithm which is a (semi-) hierarchicalalgorithm commencing with a partition made up of single pixel regions andmerging regions or subsets of regions using a Markov random field (MRF) imagemodel. It is an example of a general approach to computer vision calledconcurrent vision in which the operations of image segmentation and imageclassification are carried out concurrently. While many image labelingalgorithms output a single partition, or segmentation, the MCV algorithmoutputs a sequence of partitions and this more elaborate structure may provideinformation that is valuable for higher level vision systems. With certaintypes of MRF the component of the system for image evaluation can beimplemented as a hardwired feed forward neural network. While being applicableto images (i.e. 2D signals), the algorithm is equally applicable to 1D signals(e.g. speech) or 3D signals (e.g. video sequences) (though its performance insuch domains remains to be tested). The algorithm is assessed using subjectiveand objective criteria with very good results. ", "id2": "353", "id3": "None"}
{"id": "355", "content": "In this paper, we address cell image segmentation task by Feedback Attentionmechanism like feedback processing. Unlike conventional neural network modelsof feedforward processing, we focused on the feedback processing in human brainand assumed that the network learns like a human by connecting feature mapsfrom deep layers to shallow layers. We propose some Feedback Attentions whichimitate human brain and feeds back the feature maps of output layer to closelayer to the input. U-Net with Feedback Attention showed better result than theconventional methods using only feedforward processing. ", "id2": "354", "id3": "None"}
{"id": "356", "content": "In this paper, we propose an easily trained yet powerful representationlearning approach with performance highly competitive to deep neural networksin a digital pathology image segmentation task. The method, called sparsecoding driven deep decision tree ensembles that we abbreviate as ScD2TE,provides a new perspective on representation learning. We explore thepossibility of stacking several layers based on non-differentiable pairwisemodules and generate a densely concatenated architecture holding thecharacteristics of feature map reuse and end-to-end dense learning. Under thisarchitecture, fast convolutional sparse coding is used to extract multi-levelfeatures from the output of each layer. In this way, rich image appearancemodels together with more contextual information are integrated by learning aseries of decision tree ensembles. The appearance and the high-level contextfeatures of all the previous layers are seamlessly combined by concatenatingthem to feed-forward as input, which in turn makes the outputs of subsequentlayers more accurate and the whole model efficient to train. Compared with deepneural networks, our proposed ScD2TE does not require back-propagationcomputation and depends on less hyper-parameters. ScD2TE is able to achieve afast end-to-end pixel-wise training in a layer-wise manner. We demonstrated thesuperiority of our segmentation technique by evaluating it on the multi-diseasestate and multi-organ dataset where consistently higher performances wereobtained for comparison against several state-of-the-art deep learning methodssuch as convolutional neural networks (CNN), fully convolutional networks(FCN), etc. ", "id2": "355", "id3": "None"}
{"id": "357", "content": "In this paper, we propose a new approach for building cellular automata tosolve real-world segmentation problems. We design and train a cellularautomaton that can successfully segment high-resolution images. We consider acolony that densely inhabits the pixel grid, and all cells are governed by arandomized update that uses the current state, the color, and the state of the$3 times 3$ neighborhood. The space of possible rules is defined by a smallneural network. The update rule is applied repeatedly in parallel to a largerandom subset of cells and after convergence is used to produce segmentationmasks that are then back-propagated to learn the optimal update rules usingstandard gradient descent methods. We demonstrate that such models can belearned efficiently with only limited trajectory length and that they showremarkable ability to organize the information to produce a globally consistentsegmentation result, using only local information exchange. From a practicalperspective, our approach allows us to build very efficient models -- oursmallest automaton uses less than 10,000 parameters to solve complexsegmentation tasks. ", "id2": "356", "id3": "None"}
{"id": "358", "content": "In medical imaging, the heterogeneity of multi-centre data impedes theapplicability of deep learning-based methods and results in significantperformance degradation when applying models in an unseen data domain, e.g. anew centreor a new scanner. In this paper, we propose an unsupervised domainadaptation framework for boosting image segmentation performance acrossmultiple domains without using any manual annotations from the new targetdomains, but by re-calibrating the networks on few images from the targetdomain. To achieve this, we enforce architectures to be adaptive to new data byrejecting improbable segmentation patterns and implicitly learning throughsemantic and boundary information, thus to capture disease-specific spatialpatterns in an adversarial optimization. The adaptation process needscontinuous monitoring, however, as we cannot assume the presence ofground-truth masks for the target domain, we propose two new metrics to monitorthe adaptation process, and strategies to train the segmentation algorithm in astable fashion. We build upon well-established 2D and 3D architectures andperform extensive experiments on three cross-centre brain lesion segmentationtasks, involving multicentre public and in-house datasets. We demonstrate thatrecalibrating the deep networks on a few unlabeled images from the targetdomain improves the segmentation accuracy significantly. ", "id2": "357", "id3": "None"}
{"id": "359", "content": "In convolutional neural network based medical image segmentation, theperiphery of foreground regions representing malignant tissues may bedisproportionately assigned as belonging to the background class of healthytissues cite attenUnet  cite AttenUnet2018  cite InterSeg  cite UnetFrontNeuro  cite LearnActiveContour .This leads to high false negative detection rates. In this paper, we propose anovel attention mechanism to directly address such high false negative rates,called Paying Attention to Mistakes. Our attention mechanism steers the modelstowards false positive identification, which counters the existing bias towardsfalse negatives. The proposed mechanism has two complementary implementations:(a) explicit steering of the model to attend to a larger Effective ReceptiveField on the foreground areas; (b) implicit steering towards false positives,by attending to a smaller Effective Receptive Field on the background areas. Wevalidated our methods on three tasks: 1) binary dense prediction betweenvehicles and the background using CityScapes; 2) Enhanced Tumour Coresegmentation with multi-modal MRI scans in BRATS2018; 3) segmenting strokelesions using ultrasound images in ISLES2018. We compared our methods withstate-of-the-art attention mechanisms in medical imaging, includingself-attention, spatial-attention and spatial-channel mixed attention. Acrossall of the three different tasks, our models consistently outperform thebaseline models in Intersection over Union (IoU) and/or Hausdorff Distance(HD). For instance, in the second task, the explicit implementation of ourmechanism reduces the HD of the best baseline by more than $26 %$, whilstimproving the IoU by more than $3 %$. We believe our proposed attentionmechanism can benefit a wide range of medical and computer vision tasks, whichsuffer from over-detection of background. ", "id2": "358", "id3": "None"}
{"id": "360", "content": "In medical image analysis, semi-supervised learning is an effective method toextract knowledge from a small amount of labeled data and a large amount ofunlabeled data. This paper focuses on a popular pipeline known as selflearning, and points out a weakness named lazy learning that refers to thedifficulty for a model to learn from the pseudo labels generated by itself. Toalleviate this issue, we propose ATSO, an asynchronous version ofteacher-student optimization. ATSO partitions the unlabeled data into twosubsets and alternately uses one subset to fine-tune the model and updates thelabel on the other subset. We evaluate ATSO on two popular medical imagesegmentation datasets and show its superior performance in varioussemi-supervised settings. With slight modification, ATSO transfers well tonatural image segmentation for autonomous driving data. ", "id2": "359", "id3": "None"}
{"id": "361", "content": "Although deep convolutional networks have reached state-of-the-artperformance in many medical image segmentation tasks, they have typicallydemonstrated poor generalisation capability. To be able to generalise from onedomain (e.g. one imaging modality) to another, domain adaptation has to beperformed. While supervised methods may lead to good performance, they requireto fully annotate additional data which may not be an option in practice. Incontrast, unsupervised methods dont need additional annotations but areusually unstable and hard to train. In this work, we propose a novelweakly-supervised method. Instead of requiring detailed but time-consumingannotations, scribbles on the target domain are used to perform domainadaptation. This paper introduces a new formulation of domain adaptation basedon structured learning and co-segmentation. Our method is easy to train, thanksto the introduction of a regularised loss. The framework is validated onVestibular Schwannoma segmentation (T1 to T2 scans). Our proposed methodoutperforms unsupervised approaches and achieves comparable performance to afully-supervised approach. ", "id2": "360", "id3": "None"}
{"id": "362", "content": "Polarimetric synthetic aperture radar (PolSAR) image segmentation iscurrently of great importance in image processing for remote sensingapplications. However, it is a challenging task due to two main reasons.Firstly, the label information is difficult to acquire due to high annotationcosts. Secondly, the speckle effect embedded in the PolSAR imaging processremarkably degrades the segmentation performance. To address these two issues,we present a contextual PolSAR image semantic segmentation method in thispaper.With a newly defined channelwise consistent feature set as input, thethree-dimensional discrete wavelet transform (3D-DWT) technique is employed toextract discriminative multi-scale features that are robust to speckle noise.Then Markov random field (MRF) is further applied to enforce label smoothnessspatially during segmentation. By simultaneously utilizing 3D-DWT features andMRF priors for the first time, contextual information is fully integratedduring the segmentation to ensure accurate and smooth segmentation. Todemonstrate the effectiveness of the proposed method, we conduct extensiveexperiments on three real benchmark PolSAR image data sets. Experimentalresults indicate that the proposed method achieves promising segmentationaccuracy and preferable spatial consistency using a minimal number of labeledpixels. ", "id2": "361", "id3": "None"}
{"id": "363", "content": "Medical image segmentation is inherently an ambiguous task due to factorssuch as partial volumes and variations in anatomical definitions. While in mostcases the segmentation uncertainty is around the border of structures ofinterest, there can also be considerable inter-rater differences. The class ofconditional variational autoencoders (cVAE) offers a principled approach toinferring distributions over plausible segmentations that are conditioned oninput images. Segmentation uncertainty estimated from samples of suchdistributions can be more informative than using pixel level probabilityscores. In this work, we propose a novel conditional generative model that isbased on conditional Normalizing Flow (cFlow). The basic idea is to increasethe expressivity of the cVAE by introducing a cFlow transformation step afterthe encoder. This yields improved approximations of the latent posteriordistribution, allowing the model to capture richer segmentation variations.With this we show that the quality and diversity of samples obtained from ourconditional generative model is enhanced. Performance of our model, which wecall cFlow Net, is evaluated on two medical imaging datasets demonstratingsubstantial improvements in both qualitative and quantitative measures whencompared to a recent cVAE based model. ", "id2": "362", "id3": "None"}
{"id": "364", "content": "We consider the problem of segmenting image regions given a natural languagephrase, and study it on a novel dataset of 77,262 images and 345,486phrase-region pairs. Our dataset is collected on top of the Visual Genomedataset and uses the existing annotations to generate a challenging set ofreferring phrases for which the corresponding regions are manually annotated.Phrases in our dataset correspond to multiple regions and describe a largenumber of object and stuff categories as well as their attributes such ascolor, shape, parts, and relationships with other entities in the image. Ourexperiments show that the scale and diversity of concepts in our dataset posessignificant challenges to the existing state-of-the-art. We systematicallyhandle the long-tail nature of these concepts and present a modular approach tocombine category, attribute, and relationship cues that outperforms existingapproaches. ", "id2": "363", "id3": "None"}
{"id": "365", "content": "Modern deep learning models have revolutionized the field of computer vision.But, a significant drawback of most of these models is that they require alarge number of labelled examples to generalize properly. Recent developmentsin few-shot learning aim to alleviate this requirement. In this paper, wepropose a novel lightweight CNN architecture for 1-shot image segmentation. Theproposed model is created by taking inspiration from well-performingarchitectures for semantic segmentation and adapting it to the 1-shot domain.We train our model using 4 meta-learning algorithms that have worked well forimage classification and compare the results. For the chosen dataset, ourproposed model has a 70% lower parameter count than the benchmark, while havingbetter or comparable mean IoU scores using all 4 of the meta-learningalgorithms. ", "id2": "364", "id3": "None"}
{"id": "366", "content": "Image segmentation is the initial step for every image analysis task. A largevariety of segmentation algorithm has been proposed in the literature duringseveral decades with some mixed success. Among them, the fuzzy energy basedactive contour models get attention to the researchers during last decade whichresults in development of various methods. A good segmentation algorithm shouldperform well in a large number of images containing noise, blur, low contrast,region in-homogeneity, etc. However, the performances of the most of theexisting fuzzy energy based active contour models have been evaluated typicallyon the limited number of images. In this article, our aim is to review theexisting fuzzy active contour models from the theoretical point of view andalso evaluate them experimentally on a large set of images under the variousconditions. The analysis under a large variety of images provides objectiveinsight into the strengths and weaknesses of various fuzzy active contourmodels. Finally, we discuss several issues and future research direction onthis particular topic. ", "id2": "365", "id3": "None"}
{"id": "367", "content": "The well-known technique outlined in the paper of Leon A. Gatys et al., ANeural Algorithm of Artistic Style, has become a trending topic both inacademic literature and industrial applications. Neural Style Transfer (NST)constitutes an essential tool for a wide range of applications, such asartistic stylization of 2D images, user-assisted creation tools and productiontools for entertainment applications. The purpose of this study is to present amethod for creating artistic maps from satellite images, based on the NSTalgorithm. This method includes three basic steps (i) application of semanticimage segmentation on the original satellite image, dividing its content intoclasses (i.e. land, water), (ii) application of neural style transfer for eachclass and (iii) creation of a collage, i.e. an artistic image consisting of acombination of the two stylized image generated on the previous step. ", "id2": "366", "id3": "None"}
{"id": "368", "content": "Deep neural networks (DNNs) have shown remarkable performance improvements onvision-related tasks such as object detection or image segmentation. Despitetheir success, they generally lack the understanding of 3D objects which formthe image, as it is not always possible to collect 3D information about thescene or to easily annotate it. Differentiable rendering is a novel field whichallows the gradients of 3D objects to be calculated and propagated throughimages. It also reduces the requirement of 3D data collection and annotation,while enabling higher success rate in various applications. This paper reviewsexisting literature and discusses the current state of differentiablerendering, its applications and open research problems. ", "id2": "367", "id3": "None"}
{"id": "369", "content": "Vision-based lane detection (LD) is a key part of autonomous drivingtechnology, and it is also a challenging problem. As one of the importantconstraints of scene composition, vanishing point (VP) may provide a usefulclue for lane detection. In this paper, we proposed a new multi-task fusionnetwork architecture for high-precision lane detection. Firstly, the ERFNet wasused as the backbone to extract the hierarchical features of the road image.Then, the lanes were detected using image segmentation. Finally, combining theoutput of lane detection and the hierarchical features extracted by thebackbone, the lane VP was predicted using heatmap regression. The proposedfusion strategy was tested using the public CULane dataset. The experimentalresults suggest that the lane detection accuracy of our method outperformsthose of state-of-the-art (SOTA) methods. ", "id2": "368", "id3": "None"}
{"id": "370", "content": "We present a multiple instance learning class activation map (MIL-CAM)approach for pixel-level minirhizotron image segmentation given weakimage-level labels. Minirhizotrons are used to image plant roots in situ.Minirhizotron imagery is often composed of soil containing a few long and thinroot objects of small diameter. The roots prove to be challenging for existingsemantic image segmentation methods to discriminate. In addition to learningfrom weak labels, our proposed MIL-CAM approach re-weights the root versus soilpixels during analysis for improved performance due to the heavy imbalancebetween soil and root pixels. The proposed approach outperforms other attentionmap and multiple instance learning methods for localization of root objects inminirhizotron imagery. ", "id2": "369", "id3": "None"}
{"id": "371", "content": "We present a method combining affinity prediction with region agglomeration,which improves significantly upon the state of the art of neuron segmentationfrom electron microscopy (EM) in accuracy and scalability. Our method consistsof a 3D U-NET, trained to predict affinities between voxels, followed byiterative region agglomeration. We train using a structured loss based onMALIS, encouraging topologically correct segmentations obtained from affinitythresholding. Our extension consists of two parts: First, we present aquasi-linear method to compute the loss gradient, improving over the originalquadratic algorithm. Second, we compute the gradient in two separate passes toavoid spurious gradient contributions in early training stages. Our predictionsare accurate enough that simple learning-free percentile-based agglomerationoutperforms more involved methods used earlier on inferior predictions. Wepresent results on three diverse EM datasets, achieving relative improvementsover previous results of 27%, 15%, and 250%. Our findings suggest that a singlemethod can be applied to both nearly isotropic block-face EM data andanisotropic serial sectioned EM data. The runtime of our method scales linearlywith the size of the volume and achieves a throughput of about 2.6 seconds permegavoxel, qualifying our method for the processing of very large datasets. ", "id2": "370", "id3": "None"}
{"id": "372", "content": "Deep learning has achieved great success as a powerful classification tooland also made great progress in sematic segmentation. As a result, manyresearchers also believe that deep learning is the most powerful tool for pixellevel image segmentation. Could deep learning achieve the same pixel levelaccuracy as traditional image segmentation techniques by mapping the featuresof the object into a non-linear function? This paper gives a short survey ofthe accuracies achieved by deep learning so far in image classification andimage segmentation. Compared to the high accuracies achieved by deep learningin classifying limited categories in international vision challenges, the imagesegmentation accuracies achieved by deep learning in the same challenges areonly about eighty percent. On the contrary, the image segmentation accuraciesachieved in international biomedical challenges are close to ninty fivepercent. Why the difference is so big? Since the accuracies of the competitorsmethods are only evaluated based on their submitted results instead ofreproducing the results by submitting the source codes or the software, are theachieved accuracies verifiable or overhyped? We are going to find it out byanalyzing the working principle of deep learning. Finally, we compared theaccuracies of state of the art deep learning methods with a threshold selectionmethod quantitatively. Experimental results showed that the threshold selectionmethod could achieve significantly higher accuracy than deep learning methodsin image segmentation. ", "id2": "371", "id3": "None"}
{"id": "373", "content": "The automated segmentation of buildings in remote sensing imagery is achallenging task that requires the accurate delineation of multiple buildinginstances over typically large image areas. Manual methods are often laboriousand current deep-learning-based approaches fail to delineate all buildinginstances and do so with adequate accuracy. As a solution, we present TrainableDeep Active Contours (TDACs), an automatic image segmentation framework thatintimately unites Convolutional Neural Networks (CNNs) and Active ContourModels (ACMs). The Eulerian energy functional of the ACM component includesper-pixel parameter maps that are predicted by the backbone CNN, which alsoinitializes the ACM. Importantly, both the ACM and CNN components are fullyimplemented in TensorFlow and the entire TDAC architecture is end-to-endautomatically differentiable and backpropagation trainable without userintervention. TDAC yields fast, accurate, and fully automatic simultaneousdelineation of arbitrarily many buildings in the image. We validate the modelon two publicly available aerial image datasets for building segmentation, andour results demonstrate that TDAC establishes a new state-of-the-artperformance. ", "id2": "372", "id3": "None"}
{"id": "374", "content": "Greenhouse segmentation has pivotal importance for climate-smart agriculturalland-use planning. Deep learning-based approaches provide state-of-the-artperformance in natural image segmentation. However, semantic segmentation onhigh-resolution optical satellite imagery is a challenging task because of thecomplex environment. In this paper, a sound methodology is proposed forpixel-wise classification on images acquired by the Azersky (SPOT-7) opticalsatellite. In particular, customized variations of U-Net-like architectures areemployed to identify greenhouses. Two models are proposed which uniquelyincorporate dilated convolutions and skip connections, and the results arecompared to that of the baseline U-Net model. The dataset used consists ofpan-sharpened orthorectified Azersky images (red, green, blue,and near infraredchannels) with 1.5-meter resolution and annotation masks, collected from 15regions in Azerbaijan where the greenhouses are densely congested. The imagescover the cumulative area of 1008 $km^2$ and annotation masks contain 47559polygons in total. The $F_1, Kappa, AUC$, and $IOU$ scores are used forperformance evaluation. It is observed that the use of the deconvolutionallayers alone throughout the expansive path does not yield satisfactory results;therefore, they are either replaced or coupled with bilinear interpolation. Allmodels benefit from the hard example mining (HEM) strategy. It is also reportedthat the best accuracy of $93.29 %$ ($F_1 ,score$) is recorded when theweighted binary cross-entropy loss is coupled with the dice loss. Experimentalresults showed that both of the proposed models outperformed the baseline U-Netarchitecture such that the best model proposed scored $4.48 %$ higher incomparison to the baseline architecture. ", "id2": "373", "id3": "None"}
{"id": "375", "content": "Segmentation of multiple surfaces in medical images is a challenging problem,further complicated by the frequent presence of weak boundary and mutualinfluence between adjacent objects. The traditional graph-based optimal surfacesegmentation method has proven its effectiveness with its ability of capturingvarious surface priors in a uniform graph model. However, its efficacy heavilyrelies on handcrafted features that are used to define the surface cost for thegoodness of a surface. Recently, deep learning (DL) is emerging as powerfultools for medical image segmentation thanks to its superior feature learningcapability. Unfortunately, due to the scarcity of training data in medicalimaging, it is nontrivial for DL networks to implicitly learn the globalstructure of the target surfaces, including surface interactions. In this work,we propose to parameterize the surface cost functions in the graph model andleverage DL to learn those parameters. The multiple optimal surfaces are thensimultaneously detected by minimizing the total surface cost while explicitlyenforcing the mutual surface interaction constraints. The optimization problemis solved by the primal-dual Internal Point Method, which can be implemented bya layer of neural networks, enabling efficient end-to-end training of the wholenetwork. Experiments on Spectral Domain Optical Coherence Tomography (SD-OCT)retinal layer segmentation and Intravascular Ultrasound (IVUS) vessel wallsegmentation demonstrated very promising results. All source code is public tofacilitate further research at this direction. ", "id2": "374", "id3": "None"}
{"id": "376", "content": "Semi-supervised learning has attracted much attention in medical imagesegmentation due to challenges in acquiring pixel-wise image annotations, whichis a crucial step for building high-performance deep learning methods. Mostexisting semi-supervised segmentation approaches either tend to neglectgeometric constraint in object segments, leading to incomplete object coverage,or impose strong shape prior that requires extra alignment. In this work, wepropose a novel shapeaware semi-supervised segmentation strategy to leverageabundant unlabeled data and to enforce a geometric shape constraint on thesegmentation output. To achieve this, we develop a multi-task deep network thatjointly predicts semantic segmentation and signed distance map(SDM) of objectsurfaces. During training, we introduce an adversarial loss between thepredicted SDMs of labeled and unlabeled data so that our network is able tocapture shape-aware features more effectively. Experiments on the AtrialSegmentation Challenge dataset show that our method outperforms currentstate-of-the-art approaches with improved shape estimation, which validates itsefficacy. Code is available at https://github.com/kleinzcy/SASSnet. ", "id2": "375", "id3": "None"}
{"id": "377", "content": "Panoramic segmentation is a scene where image segmentation tasks is moredifficult. With the development of CNN networks, panoramic segmentation taskshave been sufficiently developed.However, the current panoramic segmentationalgorithms are more concerned with context semantics, but the details of imageare not processed enough. Moreover, they cannot solve the problems whichcontains the accuracy of occluded object segmentation,little objectsegmentation,boundary pixel in object segmentation etc. Aiming to address theseissues, this paper presents some useful tricks. (a) By changing the basicsegmentation model, the model can take into account the large objects and theboundary pixel classification of image details. (b) Modify the loss function sothat it can take into account the boundary pixels of multiple objects in theimage. (c) Use a semi-supervised approach to regain control of the trainingprocess. (d) Using multi-scale training and reasoning. All these operationsnamed AinnoSeg, AinnoSeg can achieve state-of-art performance on the well-knowndataset ADE20K. ", "id2": "376", "id3": "None"}
{"id": "378", "content": "The usage of convolutional neural networks (CNNs) for unsupervised imagesegmentation was investigated in this study. In the proposed approach, labelprediction and network parameter learning are alternately iterated to meet thefollowing criteria: (a) pixels of similar features should be assigned the samelabel, (b) spatially continuous pixels should be assigned the same label, and(c) the number of unique labels should be large. Although these criteria areincompatible, the proposed approach minimizes the combination of similarityloss and spatial continuity loss to find a plausible solution of labelassignment that balances the aforementioned criteria well. The contributions ofthis study are four-fold. First, we propose a novel end-to-end network ofunsupervised image segmentation that consists of normalization and an argmaxfunction for differentiable clustering. Second, we introduce a spatialcontinuity loss function that mitigates the limitations of fixed segmentboundaries possessed by previous work. Third, we present an extension of theproposed method for segmentation with scribbles as user input, which showedbetter accuracy than existing methods while maintaining efficiency. Finally, weintroduce another extension of the proposed method: unseen image segmentationby using networks pre-trained with a few reference images without re-trainingthe networks. The effectiveness of the proposed approach was examined onseveral benchmark datasets of image segmentation. ", "id2": "377", "id3": "None"}
{"id": "379", "content": "Supervised learning in large discriminative models is a mainstay for moderncomputer vision. Such an approach necessitates investing in large-scalehuman-annotated datasets for achieving state-of-the-art results. In turn, theefficacy of supervised learning may be limited by the size of the humanannotated dataset. This limitation is particularly notable for imagesegmentation tasks, where the expense of human annotation is especially large,yet large amounts of unlabeled data may exist. In this work, we ask if we mayleverage semi-supervised learning in unlabeled video sequences and extra imagesto improve the performance on urban scene segmentation, simultaneously tacklingsemantic, instance, and panoptic segmentation. The goal of this work is toavoid the construction of sophisticated, learned architectures specific tolabel propagation (e.g., patch matching and optical flow). Instead, we simplypredict pseudo-labels for the unlabeled data and train subsequent models withboth human-annotated and pseudo-labeled data. The procedure is iterated forseveral times. As a result, our Naive-Student model, trained with such simpleyet effective iterative semi-supervised learning, attains state-of-the-artresults at all three Cityscapes benchmarks, reaching the performance of 67.8%PQ, 42.6% AP, and 85.2% mIOU on the test set. We view this work as a notablestep towards building a simple procedure to harness unlabeled video sequencesand extra images to surpass state-of-the-art performance on core computervision tasks. ", "id2": "378", "id3": "None"}
{"id": "380", "content": "The performance of deep networks for semantic image segmentation largelydepends on the availability of large-scale training images which are labelledat the pixel level. Typically, such pixel-level image labellings are obtainedmanually by a labour-intensive process. To alleviate the burden of manual imagelabelling, we propose an interesting learning approach to generate pixel-levelimage labellings automatically. A Guided Filter Network (GFN) is firstdeveloped to learn the segmentation knowledge from a source domain, and suchGFN then transfers such segmentation knowledge to generate coarse object masksin the target domain. Such coarse object masks are treated as pseudo labels andthey are further integrated to optimize/refine the GFN iteratively in thetarget domain. Our experiments on six image sets have demonstrated that ourproposed approach can generate fine-grained object masks (i.e., pixel-levelobject labellings), whose quality is very comparable to the manually-labelledones. Our proposed approach can also achieve better performance on semanticimage segmentation than most existing weakly-supervised approaches. ", "id2": "379", "id3": "None"}
{"id": "381", "content": "Single encoder-decoder methodologies for semantic segmentation are reachingtheir peak in terms of segmentation quality and efficiency per number oflayers. To address these limitations, we propose a new architecture based on adecoder which uses a set of shallow networks for capturing more informationcontent. The new decoder has a new topology of skip connections, namelybackward and stacked residual connections. In order to further improve thearchitecture we introduce a weight function which aims to re-balance classes toincrease the attention of the networks to under-represented objects. We carriedout an extensive set of experiments that yielded state-of-the-art results forthe CamVid, Gatech and Freiburg Forest datasets. Moreover, to further prove theeffectiveness of our decoder, we conducted a set of experiments studying theimpact of our decoder to state-of-the-art segmentation techniques.Additionally, we present a set of experiments augmenting semantic segmentationwith optical flow information, showing that motion clues can boost pure imagebased semantic segmentation approaches. ", "id2": "380", "id3": "None"}
{"id": "382", "content": "Segmentation of objects of interest is one of the central tasks in medicalimage analysis, which is indispensable for quantitative analysis. Whendeveloping machine-learning based methods for automated segmentation, manualannotations are usually used as the ground truth toward which the models learnto mimic. While the bulky parts of the segmentation targets are relatively easyto label, the peripheral areas are often difficult to handle due to ambiguousboundaries and the partial volume effect, etc., and are likely to be labeledwith uncertainty. This uncertainty in labeling may, in turn, result inunsatisfactory performance of the trained models. In this paper, we proposesuperpixel-based label softening to tackle the above issue. Generated byunsupervised over-segmentation, each superpixel is expected to represent alocally homogeneous area. If a superpixel intersects with the annotationboundary, we consider a high probability of uncertain labeling within thisarea. Driven by this intuition, we soften labels in this area based on signeddistances to the annotation boundary and assign probability values within [0,1] to them, in comparison with the original hard, binary labels of either 0or 1. The softened labels are then used to train the segmentation modelstogether with the hard labels. Experimental results on a brain MRI dataset andan optical coherence tomography dataset demonstrate that this conceptuallysimple and implementation-wise easy method achieves overall superiorsegmentation performances to baseline and comparison methods for both 3D and 2Dmedical images. ", "id2": "381", "id3": "None"}
{"id": "383", "content": "As the resolution of digital images increase significantly, the processing ofimages becomes more challenging in terms of accuracy and efficiency. In thispaper, we consider image segmentation by solving a partial differentiationequation (PDE) model based on the Mumford-Shah functional. We develop a newalgorithm by combining anisotropic mesh adaptation for image representation andfinite element method for solving the PDE model. Comparing to traditionalalgorithms solved by finite difference method, our algorithm provides fasterand better results without the need to resizing the images to lower quality. Wealso extend the algorithm to segment images with multiple regions. ", "id2": "382", "id3": "None"}
{"id": "384", "content": "In this work, we propose a new unsupervised image segmentation approach basedon mutual information maximization between different constructed views of theinputs. Taking inspiration from autoregressive generative models that predictthe current pixel from past pixels in a raster-scan ordering created withmasked convolutions, we propose to use different orderings over the inputsusing various forms of masked convolutions to construct different views of thedata. For a given input, the model produces a pair of predictions with twovalid orderings, and is then trained to maximize the mutual information betweenthe two outputs. These outputs can either be low-dimensional features forrepresentation learning or output clusters corresponding to semantic labels forclustering. While masked convolutions are used during training, in inference,no masking is applied and we fall back to the standard convolution where themodel has access to the full input. The proposed method outperforms currentstate-of-the-art on unsupervised image segmentation. It is simple and easy toimplement, and can be extended to other visual tasks and integrated seamlesslyinto existing unsupervised learning methods requiring different views of thedata. ", "id2": "383", "id3": "None"}
{"id2": 1035, "id3": "383", "content": "In this work, we propose a new unsupervised image segmentation approach basedon mutual information maximization between different constructed views of theinputs. Taking inspiration from autoregressive generative models that predictthe current pixel from past pixels in a raster-scan ordering created withmasked convolutions, we propose to use different orderings over the inputsusing various forms of masked convolutions to construct different views of thedata. For a given input, the model produces a pair of predictions with twovalid orderings, and is then trained to maximize the mutual information betweenthe two outputs. These outputs can either be low-dimensional features forrepresentation learning or output clusters corresponding to semantic labels forclustering. While masked convolutions are used during training, in inference,no masking is applied and we fall back to the standard convolution where themodel has access to the full input. The proposed method outperforms currentstate-of-the-art on unsupervised image segmentation. It is simple and easy toimplement, and can be extended to other visual tasks and integrated seamlesslyinto existing unsupervised learning methods requiring different views of thedata."}
{"id": "385", "content": "Convolutional neural networks have shown to achieve superior performance onimage segmentation tasks. However, convolutional neural networks, operating asblack-box systems, generally do not provide a reliable measure about theconfidence of their decisions. This leads to various problems in industrialsettings, amongst others, inadequate levels of trust from users in the modelsoutputs as well as a non-compliance with current policy guidelines (e.g., EU AIStrategy). To address these issues, we use uncertainty measures based onMonte-Carlo dropout in the context of a human-in-the-loop system to increasethe systems transparency and performance. In particular, we demonstrate thebenefits described above on a real-world multi-class image segmentation task ofwear analysis in the machining industry. Following previous work, we show thatthe quality of a prediction correlates with the models uncertainty.Additionally, we demonstrate that a multiple linear regression using themodels uncertainties as independent variables significantly explains thequality of a prediction ( (R^2=0.718 )). Within the uncertainty-basedhuman-in-the-loop system, the multiple regression aims at identifying failedpredictions on an image-level. The system utilizes a human expert to labelthese failed predictions manually. A simulation study demonstrates that theuncertainty-based human-in-the-loop system increases performance for differentlevels of human involvement in comparison to a random-based human-in-the-loopsystem. To ensure generalizability, we show that the presented approachachieves similar results on the publicly available Cityscapes dataset. ", "id2": "384", "id3": "None"}
{"id": "386", "content": "In this work, the case of semantic segmentation on a small image dataset(simulated by 1000 randomly selected images from PASCAL VOC 2012), where onlyweak supervision signals (scribbles from user interaction) are available isstudied. Especially, to tackle the problem of limited data annotations in imagesegmentation, transferring different pre-trained models and CRF based methodsare applied to enhance the segmentation performance. To this end, RotNet,DeeperCluster, and Semi&Weakly Supervised Learning (SWSL) pre-trained modelsare transferred and finetuned in a DeepLab-v2 baseline, and dense CRF isapplied both as a post-processing and loss regularization technique. Theresults of my study show that, on this small dataset, using a pre-trainedResNet50 SWSL model gives results that are 7.4% better than applying anImageNet pre-trained model; moreover, for the case of training on the fullPASCAL VOC 2012 training data, this pre-training approach increases the mIoUresults by almost 4%. On the other hand, dense CRF is shown to be veryeffective as well, enhancing the results both as a loss regularizationtechnique in weakly supervised training and as a post-processing tool. ", "id2": "385", "id3": "None"}
{"id": "387", "content": "The strict security requirements placed on medical records by various privacyregulations become major obstacles in the age of big data. To ensure efficientmachine learning as a service schemes while protecting data confidentiality, inthis work, we propose blind UNET (BUNET), a secure protocol that implementsprivacy-preserving medical image segmentation based on the UNET architecture.In BUNET, we efficiently utilize cryptographic primitives such as homomorphicencryption and garbled circuits (GC) to design a complete secure protocol forthe UNET neural architecture. In addition, we perform extensive architecturalsearch in reducing the computational bottleneck of GC-based secure activationprotocols with high-dimensional input data. In the experiment, we thoroughlyexamine the parameter space of our protocol, and show that we can achieve up to14x inference time reduction compared to the-state-of-the-art secure inferencetechnique on a baseline architecture with negligible accuracy degradation. ", "id2": "386", "id3": "None"}
{"id2": 1036, "id3": "386", "content": "The strict security requirements placed on medical records by various privacyregulations become major obstacles in the age of big data. To ensure efficientmachine learning as a service schemes while protecting data confidentiality, inthis work, we propose blind UNET (BUNET), a secure protocol that implementsprivacy-preserving medical image segmentation based on the UNET architecture.In BUNET, we efficiently utilize cryptographic primitives such as homomorphicencryption and garbled circuits (GC) to design a complete secure protocol forthe UNET neural architecture. In addition, we perform extensive architecturalsearch in reducing the computational bottleneck of GC-based secure activationprotocols with high-dimensional input data. In the experiment, we thoroughlyexamine the parameter space of our protocol, and show that we can achieve up to14x inference time reduction compared to the-state-of-the-art secure inferencetechnique on a baseline architecture with negligible accuracy degradation."}
{"id": "388", "content": "Uncertainty estimation is important for interpreting the trustworthiness ofmachine learning models in many applications. This is especially critical inthe data-driven active learning setting where the goal is to achieve a certainaccuracy with minimum labeling effort. In such settings, the model learns toselect the most informative unlabeled samples for annotation based on itsestimated uncertainty. The highly uncertain predictions are assumed to be moreinformative for improving model performance. In this paper, we exploreuncertainty calibration within an active learning framework for medical imagesegmentation, an area where labels often are scarce. Various uncertaintyestimation methods and acquisition strategies (regions and full images) areinvestigated. We observe that selecting regions to annotate instead of fullimages leads to more well-calibrated models. Additionally, we experimentallyshow that annotating regions can cut 50% of pixels that need to be labeled byhumans compared to annotating full images. ", "id2": "387", "id3": "None"}
{"id": "389", "content": "Medical image annotations are prohibitively time-consuming and expensive toobtain. To alleviate annotation scarcity, many approaches have been developedto efficiently utilize extra information, e.g.,semi-supervised learning furtherexploring plentiful unlabeled data, domain adaptation including multi-modalitylearning and unsupervised domain adaptation resorting to the prior knowledgefrom additional modality. In this paper, we aim to investigate the feasibilityof simultaneously leveraging abundant unlabeled data and well-establishedcross-modality data for annotation-efficient medical image segmentation. Tothis end, we propose a novel semi-supervised domain adaptation approach, namelyDual-Teacher, where the student model not only learns from labeled target data(e.g., CT), but also explores unlabeled target data and labeled source data(e.g., MR) by two teacher models. Specifically, the student model learns theknowledge of unlabeled target data from intra-domain teacher by encouragingprediction consistency, as well as the shape priors embedded in labeled sourcedata from inter-domain teacher via knowledge distillation. Consequently, thestudent model can effectively exploit the information from all three dataresources and comprehensively integrate them to achieve improved performance.We conduct extensive experiments on MM-WHS 2017 dataset and demonstrate thatour approach is able to concurrently utilize unlabeled data and cross-modalitydata with superior performance, outperforming semi-supervised learning anddomain adaptation methods with a large margin. ", "id2": "388", "id3": "None"}
{"id2": 1037, "id3": "388", "content": "Medical image annotations are prohibitively time-consuming and expensive toobtain. To alleviate annotation scarcity, many approaches have been developedto efficiently utilize extra information, e.g.,semi-supervised learning furtherexploring plentiful unlabeled data, domain adaptation including multi-modalitylearning and unsupervised domain adaptation resorting to the prior knowledgefrom additional modality. In this paper, we aim to investigate the feasibilityof simultaneously leveraging abundant unlabeled data and well-establishedcross-modality data for annotation-efficient medical image segmentation. Tothis end, we propose a novel semi-supervised domain adaptation approach, namelyDual-Teacher, where the student model not only learns from labeled target data(e.g., CT), but also explores unlabeled target data and labeled source data(e.g., MR) by two teacher models. Specifically, the student model learns theknowledge of unlabeled target data from intra-domain teacher by encouragingprediction consistency, as well as the shape priors embedded in labeled sourcedata from inter-domain teacher via knowledge distillation. Consequently, thestudent model can effectively exploit the information from all three dataresources and comprehensively integrate them to achieve improved performance.We conduct extensive experiments on MM-WHS 2017 dataset and demonstrate thatour approach is able to concurrently utilize unlabeled data and cross-modalitydata with superior performance, outperforming semi-supervised learning anddomain adaptation methods with a large margin."}
{"id": "390", "content": "Surgical tool segmentation in endoscopic images is an important problem: itis a crucial step towards full instrument pose estimation and it is used forintegration of pre- and intra-operative images into the endoscopic view. Whilemany recent approaches based on convolutional neural networks have shown greatresults, a key barrier to progress lies in the acquisition of a large number ofmanually-annotated images which is necessary for an algorithm to generalize andwork well in diverse surgical scenarios. Unlike the surgical image data itself,annotations are difficult to acquire and may be of variable quality. On theother hand, synthetic annotations can be automatically generated by usingforward kinematic model of the robot and CAD models of tools by projecting themonto an image plane. Unfortunately, this model is very inaccurate and cannot beused for supervised learning of image segmentation models. Since generatedannotations will not directly correspond to endoscopic images due to errors, weformulate the problem as an unpaired image-to-image translation where the goalis to learn the mapping between an input endoscopic image and a correspondingannotation using an adversarial model. Our approach allows to train imagesegmentation models without the need to acquire expensive annotations and canpotentially exploit large unlabeled endoscopic image collection outside theannotated distributions of image/annotation data. We test our proposed methodon Endovis 2017 challenge dataset and show that it is competitive withsupervised segmentation methods. ", "id2": "389", "id3": "None"}
{"id": "391", "content": "As constituent parts of image objects, superpixels can improve severalhigher-level operations. However, image segmentation methods might have theiraccuracy seriously compromised for reduced numbers of superpixels. We haveinvestigated a solution based on the Iterative Spanning Forest (ISF) framework.In this work, we present Dynamic ISF (DISF) -- a method based on the followingsteps. (a) It starts from an image graph and a seed set with considerably morepixels than the desired number of superpixels. (b) The seeds compete amongthemselves, and each seed conquers its most closely connected pixels, resultingin an image partition (spanning forest) with connected superpixels. In step(c), DISF assigns relevance values to seeds based on superpixel analysis andremoves the most irrelevant ones. Steps (b) and (c) are repeated until thedesired number of superpixels is reached. DISF has the chance to reconstructrelevant edges after each iteration, when compared to region mergingalgorithms. As compared to other seed-based superpixel methods, DISF is morelikely to find relevant seeds. It also introduces dynamic arc-weight estimationin the ISF framework for more effective superpixel delineation, and wedemonstrate all results on three datasets with distinct object properties. ", "id2": "390", "id3": "None"}
{"id": "392", "content": "To assist researchers to identify Environmental Microorganisms (EMs)effectively, a Multiscale CNN-CRF (MSCC) framework for the EM imagesegmentation is proposed in this paper. There are two parts in this framework:The first is a novel pixel-level segmentation approach, using a newlyintroduced Convolutional Neural Network (CNN), namely, mU-Net-B3, with adense Conditional Random Field (CRF) postprocessing. The second is a VGG-16based patch-level segmentation method with a novel buffer strategy, whichfurther improves the segmentation quality of the details of the EMs. In theexperiment, compared with the state-of-the-art methods on 420 EM images, theproposed MSCC method reduces the memory requirement from 355 MB to 103 MB,improves the overall evaluation indexes (Dice, Jaccard, Recall, Accuracy) from85.24%, 77.42%, 82.27%, and 96.76% to 87.13%, 79.74%, 87.12%, and 96.91%,respectively, and reduces the volume overlap error from 22.58% to 20.26%.Therefore, the MSCC method shows great potential in the EM segmentation field. ", "id2": "391", "id3": "None"}
{"id": "393", "content": "Convolutional Neural Networks (CNN) have recently seen tremendous success invarious computer vision tasks. However, their application to problems with highdimensional input and output, such as high-resolution image and videosegmentation or 3D medical imaging, has been limited by various factors.Primarily, in the training stage, it is necessary to store network activationsfor back propagation. In these settings, the memory requirements associatedwith storing activations can exceed what is feasible with current hardware,especially for problems in 3D. Motivated by the propagation of signals overphysical networks, that are governed by the hyperbolic Telegraph equation, inthis work we introduce a fully conservative hyperbolic network for problemswith high dimensional input and output. We introduce a coarsening operationthat allows completely reversible CNNs by using a learnable Discrete WaveletTransform and its inverse to both coarsen and interpolate the network state andchange the number of channels. We show that fully reversible networks are ableto achieve results comparable to the state of the art in 4D time-lapse hyperspectral image segmentation and full 3D video segmentation, with a much lowermemory footprint that is a constant independent of the network depth. We alsoextend the use of such networks to Variational Auto Encoders with highresolution input and output. ", "id2": "392", "id3": "None"}
{"id": "394", "content": "Deep neural networks have achieved satisfactory performance in piles ofmedical image analysis tasks. However the training of deep neural networkrequires a large amount of samples with high-quality annotations. In medicalimage segmentation, it is very laborious and expensive to acquire precisepixel-level annotations. Aiming at training deep segmentation models ondatasets with probably corrupted annotations, we propose a novel Meta CorruptedPixels Mining (MCPM) method based on a simple meta mask network. Our method istargeted at automatically estimate a weighting map to evaluate the importanceof every pixel in the learning of segmentation network. The meta mask networkwhich regards the loss value map of the predicted segmentation results asinput, is capable of identifying out corrupted layers and allocating smallweights to them. An alternative algorithm is adopted to train the segmentationnetwork and the meta mask network, simultaneously. Extensive experimentalresults on LIDC-IDRI and LiTS datasets show that our method outperformsstate-of-the-art approaches which are devised for coping with corruptedannotations. ", "id2": "393", "id3": "None"}
{"id": "395", "content": "For the majority of the learning-based segmentation methods, a large quantityof high-quality training data is required. In this paper, we present a novellearning-based segmentation model that could be trained semi- or un-supervised. Specifically, in the unsupervised setting, we parameterize theActive contour without edges (ACWE) framework via a convolutional neuralnetwork (ConvNet), and optimize the parameters of the ConvNet using aself-supervised method. In another setting (semi-supervised), the auxiliarysegmentation ground truth is used during training. We show that the methodprovides fast and high-quality bone segmentation in the context ofsingle-photon emission computed tomography (SPECT) image. ", "id2": "394", "id3": "None"}
{"id": "396", "content": "We introduce a fluid-based image augmentation method for medical imageanalysis. In contrast to existing methods, our framework generates anatomicallymeaningful images via interpolation from the geodesic subspace underlying givensamples. Our approach consists of three steps: 1) given a source image and aset of target images, we construct a geodesic subspace using the LargeDeformation Diffeomorphic Metric Mapping (LDDMM) model; 2) we sampletransformations from the resulting geodesic subspace; 3) we obtain deformedimages and segmentations via interpolation. Experiments on brain (LPBA) andknee (OAI) data illustrate the performance of our approach on two tasks: 1)data augmentation during training and testing for image segmentation; 2)one-shot learning for single atlas image segmentation. We demonstrate that ourapproach generates anatomically meaningful data and improves performance onthese tasks over competing approaches. Code is available athttps://github.com/uncbiag/easyreg. ", "id2": "395", "id3": "None"}
{"id": "397", "content": "In the interactive image segmentation task, the Particle Competition andCooperation (PCC) model is fed with a complex network, which is built from theinput image. In the network construction phase, a weight vector is needed todefine the importance of each element in the feature set, which consists ofcolor and location information of the corresponding pixels, thus demanding aspecialists intervention. The present paper proposes the elimination of theweight vector through modifications in the network construction phase. Theproposed model and the reference model, without the use of a weight vector,were compared using 151 images extracted from the Grabcut dataset, the PASCALVOC dataset and the Alpha matting dataset. Each model was applied 30 times toeach image to obtain an error average. These simulations resulted in an errorrate of only 0.49 % when classifying pixels with the proposed model while thereference model had an error rate of 3.14 %. The proposed method also presentedless error variation in the diversity of the evaluated images, when compared tothe reference model. ", "id2": "396", "id3": "None"}
{"id": "398", "content": "Machine learning has been widely adopted for medical image analysis in recentyears given its promising performance in image segmentation and classificationtasks. As a data-driven science, the success of machine learning, in particularsupervised learning, largely depends on the availability of manually annotateddatasets. For medical imaging applications, such annotated datasets are noteasy to acquire. It takes a substantial amount of time and resource to curatean annotated medical image set. In this paper, we propose an efficientannotation framework for brain tumour images that is able to suggestinformative sample images for human experts to annotate. Our experiments showthat training a segmentation model with only 19% suggestively annotated patientscans from BraTS 2019 dataset can achieve a comparable performance to traininga model on the full dataset for whole tumour segmentation task. It demonstratesa promising way to save manual annotation cost and improve data efficiency inmedical imaging applications. ", "id2": "397", "id3": "None"}
{"id": "399", "content": "Computer vision tasks such as semantic segmentation perform very well in goodweather conditions, but if the weather turns bad, they have problems to achievethis performance in these conditions. One possibility to obtain more robust andreliable results in adverse weather conditions is to use video-segmentationapproaches instead of commonly used single-image segmentation methods.Video-segmentation approaches capture temporal information of the previousvideo-frames in addition to current image information, and hence, they are morerobust against disturbances, especially if they occur in only a few frames ofthe video-sequence. However, video-segmentation approaches, which are oftenbased on recurrent neural networks, cannot be applied in real-time applicationsanymore, since their recurrent structures in the network are computationalexpensive. For instance, the inference time of the LSTM-ICNet, in whichrecurrent units are placed at proper positions in the single-segmentationapproach ICNet, increases up to 61 percent compared to the basic ICNet. Hence,in this work, the LSTM-ICNet is sped up by modifying the recurrent units of thenetwork so that it becomes real-time capable again. Experiments on differentdatasets and various weather conditions show that the inference time can bedecreased by about 23 percent by these modifications, while they achievesimilar performance than the LSTM-ICNet and outperform the single-segmentationapproach enormously in adverse weather conditions. ", "id2": "398", "id3": "None"}
{"id": "400", "content": "Although spatial information of images usually enhance the robustness of theFuzzy C-Means (FCM) algorithm, it greatly increases the computational costs forimage segmentation. To achieve a sound trade-off between the segmentationperformance and the speed of clustering, we come up with a Kullback-Leibler(KL) divergence-based FCM algorithm by incorporating a tight wavelet frametransform and a morphological reconstruction operation. To enhance FCMsrobustness, an observed image is first filtered by using the morphologicalreconstruction. A tight wavelet frame system is employed to decompose theobserved and filtered images so as to form their feature sets. Consideringthese feature sets as data of clustering, an modified FCM algorithm isproposed, which introduces a KL divergence term in the partition matrix intoits objective function. The KL divergence term aims to make membership degreesof each image pixel closer to those of its neighbors, which brings that themembership partition becomes more suitable and the parameter setting of FCMbecomes simplified. On the basis of the obtained partition matrix andprototypes, the segmented feature set is reconstructed by minimizing theinverse process of the modified objective function. To modify abnormal featuresproduced in the reconstruction process, each reconstructed feature isreassigned to the closest prototype. As a result, the segmentation accuracy ofKL divergence-based FCM is further improved. Whats more, the segmented imageis reconstructed by using a tight wavelet frame reconstruction operation.Finally, supporting experiments coping with synthetic, medical and color imagesare reported. Experimental results exhibit that the proposed algorithm workswell and comes with better segmentation performance than other comparativealgorithms. Moreover, the proposed algorithm requires less time than most ofthe FCM-related algorithms. ", "id2": "399", "id3": "None"}
{"id": "401", "content": "Automated pavement crack image segmentation is challenging because ofinherent irregular patterns, lighting conditions, and noise in images.Conventional approaches require a substantial amount of feature engineering todifferentiate crack regions from non-affected regions. In this paper, wepropose a deep learning technique based on a convolutional neural network toperform segmentation tasks on pavement crack images. Our approach requiresminimal feature engineering compared to other machine learning techniques. Wepropose a U-Net-based network architecture in which we replace the encoder witha pretrained ResNet-34 neural network. We use a one-cycle training schedulebased on cyclical learning rates to speed up the convergence. Our methodachieves an F1 score of 96% on the CFD dataset and 73% on the Crack500 dataset,outperforming other algorithms tested on these datasets. We perform ablationstudies on various techniques that helped us get marginal performance boosts,i.e., the addition of spatial and channel squeeze and excitation (SCSE)modules, training with gradually increasing image sizes, and training variousneural network layers with different learning rates. ", "id2": "400", "id3": "None"}
{"id": "402", "content": "Although having achieved great success in medical image segmentation, deeplearning-based approaches usually require large amounts of well-annotated data,which can be extremely expensive in the field of medical image analysis.Unlabeled data, on the other hand, is much easier to acquire. Semi-supervisedlearning and unsupervised domain adaptation both take the advantage ofunlabeled data, and they are closely related to each other. In this paper, wepropose uncertainty-aware multi-view co-training (UMCT), a unified frameworkthat addresses these two tasks for volumetric medical image segmentation. Ourframework is capable of efficiently utilizing unlabeled data for betterperformance. We firstly rotate and permute the 3D volumes into multiple viewsand train a 3D deep network on each view. We then apply co-training byenforcing multi-view consistency on unlabeled data, where an uncertaintyestimation of each view is utilized to achieve accurate labeling. Experimentson the NIH pancreas segmentation dataset and a multi-organ segmentation datasetshow state-of-the-art performance of the proposed framework on semi-supervisedmedical image segmentation. Under unsupervised domain adaptation settings, wevalidate the effectiveness of this work by adapting our multi-organsegmentation model to two pathological organs from the Medical SegmentationDecathlon Datasets. Additionally, we show that our UMCT-DA model can eveneffectively handle the challenging situation where labeled source data isinaccessible, demonstrating strong potentials for real-world applications. ", "id2": "401", "id3": "None"}
{"id": "403", "content": "Deep learning techniques have successfully been employed in numerous computervision tasks including image segmentation. The techniques have also beenapplied to medical image segmentation, one of the most critical tasks incomputer-aided diagnosis. Compared with natural images, the medical image is agray-scale image with low-contrast (even with some invisible parts). Becausesome organs have similar intensity and texture with neighboring organs, thereis usually a need to refine automatic segmentation results. In this paper, wepropose an interactive deep refinement framework to improve the traditionalsemantic segmentation networks such as U-Net and fully convolutional network.In the proposed framework, we added a refinement network to traditionalsegmentation network to refine the segmentation results.Experimental resultswith public dataset revealed that the proposed method could achieve higheraccuracy than other state-of-the-art methods. ", "id2": "402", "id3": "None"}
{"id": "404", "content": "Self-supervised learning has proven to be invaluable in making best use ofall of the available data in biomedical image segmentation. One particularlysimple and effective mechanism to achieve self-supervision is inpainting, thetask of predicting arbitrary missing areas based on the rest of an image. Inthis work, we focus on image inpainting as the self-supervised proxy task, andpropose two novel structural changes to further enhance the performance of adeep neural network. We guide the process of generating images to inpaint byusing supervoxel-based masking instead of random masking, and also by focusingon the area to be segmented in the primary task, which we term as theregion-of-interest. We postulate that these additions force the network tolearn semantics that are more attuned to the primary task, and test ourhypotheses on two applications: brain tumour and white matter hyperintensitiessegmentation. We empirically show that our proposed approach consistentlyoutperforms both supervised CNNs, without any self-supervision, andconventional inpainting-based self-supervision methods on both large and smalltraining set sizes. ", "id2": "403", "id3": "None"}
{"id": "405", "content": "We introduce Post-DAE, a post-processing method based on denoisingautoencoders (DAE) to improve the anatomical plausibility of arbitrarybiomedical image segmentation algorithms. Some of the most popular segmentationmethods (e.g. based on convolutional neural networks or random forestclassifiers) incorporate additional post-processing steps to ensure that theresulting masks fulfill expected connectivity constraints. These methodsoperate under the hypothesis that contiguous pixels with similar aspect shouldbelong to the same class. Even if valid in general, this assumption does notconsider more complex priors like topological restrictions or convexity, whichcannot be easily incorporated into these methods. Post-DAE leverages the latestdevelopments in manifold learning via denoising autoencoders. First, we learn acompact and non-linear embedding that represents the space of anatomicallyplausible segmentations. Then, given a segmentation mask obtained with anarbitrary method, we reconstruct its anatomically plausible version byprojecting it onto the learnt manifold. The proposed method is trained usingunpaired segmentation mask, what makes it independent of intensity informationand image modality. We performed experiments in binary and multi-labelsegmentation of chest X-ray and cardiac magnetic resonance images. We show howerroneous and noisy segmentation masks can be improved using Post-DAE. Withalmost no additional computation cost, our method brings erroneoussegmentations back to a feasible space. ", "id2": "404", "id3": "None"}
{"id": "406", "content": "We propose a deep learning framework to detect and categorize oil spills insynthetic aperture radar (SAR) images at a large scale. By means of a carefullydesigned neural network model for image segmentation trained on an extensivedataset, we are able to obtain state-of-the-art performance in oil spilldetection, achieving results that are comparable to results produced by humanoperators. We also introduce a classification task, which is novel in thecontext of oil spill detection in SAR. Specifically, after being detected, eachoil spill is also classified according to different categories pertaining toits shape and texture characteristics. The classification results providevaluable insights for improving the design of oil spill services byworld-leading providers. As the last contribution, we present our operationalpipeline and a visualization tool for large-scale data, which allows to detectand analyze the historical presence of oil spills worldwide. ", "id2": "405", "id3": "None"}
{"id": "407", "content": "A challenge still to be overcome in the field of visual perception forvehicle and robotic navigation on heavily damaged and unpaved roads is the taskof reliable path and obstacle detection. The vast majority of the researcheshave as scenario roads in good condition, from developed countries. These workscope with few situations of variation on the road surface and even fewersituations presenting surface damages. In this paper we present an approach forroad detection considering variation in surface types, identifying paved andunpaved surfaces and also detecting damage and other information on other roadsurface that may be relevant to driving safety. We also present a new GroundTruth with image segmentation, used in our approach and that allowed us toevaluate our results. Our results show that it is possible to use passivevision for these purposes, even using images captured with low cost cameras. ", "id2": "406", "id3": "None"}
{"id": "408", "content": "Image segmentation is a fundamental and challenging problem in computervision with applications spanning multiple areas, such as medical imaging,remote sensing, and autonomous vehicles. Recently, convolutional neuralnetworks (CNNs) have gained traction in the design of automated segmentationpipelines. Although CNN-based models are adept at learning abstract featuresfrom raw image data, their performance is dependent on the availability andsize of suitable training datasets. Additionally, these models are often unableto capture the details of object boundaries and generalize poorly to unseenclasses. In this thesis, we devise novel methodologies that address theseissues and establish robust representation learning frameworks forfully-automatic semantic segmentation in medical imaging and mainstreamcomputer vision. In particular, our contributions include (1) state-of-the-art2D and 3D image segmentation networks for computer vision and medical imageanalysis, (2) an end-to-end trainable image segmentation framework that unifiesCNNs and active contour models with learnable parameters for fast and robustobject delineation, (3) a novel approach for disentangling edge and textureprocessing in segmentation networks, and (4) a novel few-shot learning model inboth supervised settings and semi-supervised settings where synergies betweenlatent and image spaces are leveraged to learn to segment images given limitedtraining data. ", "id2": "407", "id3": "None"}
{"id2": 1038, "id3": "407", "content": "Image segmentation is a fundamental and challenging problem in computervision with applications spanning multiple areas, such as medical imaging,remote sensing, and autonomous vehicles. Recently, convolutional neuralnetworks (CNNs) have gained traction in the design of automated segmentationpipelines. Although CNN-based models are adept at learning abstract featuresfrom raw image data, their performance is dependent on the availability andsize of suitable training datasets. Additionally, these models are often unableto capture the details of object boundaries and generalize poorly to unseenclasses. In this thesis, we devise novel methodologies that address theseissues and establish robust representation learning frameworks forfully-automatic semantic segmentation in medical imaging and mainstreamcomputer vision. In particular, our contributions include (1) state-of-the-art2D and 3D image segmentation networks for computer vision and medical imageanalysis, (2) an end-to-end trainable image segmentation framework that unifiesCNNs and active contour models with learnable parameters for fast and robustobject delineation, (3) a novel approach for disentangling edge and textureprocessing in segmentation networks, and (4) a novel few-shot learning model inboth supervised settings and semi-supervised settings where synergies betweenlatent and image spaces are leveraged to learn to segment images given limitedtraining data."}
{"id": "409", "content": "This work introduces pyramidal convolution (PyConv), which is capable ofprocessing the input at multiple filter scales. PyConv contains a pyramid ofkernels, where each level involves different types of filters with varying sizeand depth, which are able to capture different levels of details in the scene.On top of these improved recognition capabilities, PyConv is also efficientand, with our formulation, it does not increase the computational cost andparameters compared to standard convolution. Moreover, it is very flexible andextensible, providing a large space of potential network architectures fordifferent applications. PyConv has the potential to impact nearly everycomputer vision task and, in this work, we present different architecturesbased on PyConv for four main tasks on visual recognition: imageclassification, video action classification/recognition, object detection andsemantic image segmentation/parsing. Our approach shows significantimprovements over all these core tasks in comparison with the baselines. Forinstance, on image recognition, our 50-layers network outperforms in terms ofrecognition performance on ImageNet dataset its counterpart baseline ResNetwith 152 layers, while having 2.39 times less parameters, 2.52 times lowercomputational complexity and more than 3 times less layers. On imagesegmentation, our novel framework sets a new state-of-the-art on thechallenging ADE20K benchmark for scene parsing. Code is available at:https://github.com/iduta/pyconv ", "id2": "408", "id3": "None"}
{"id": "410", "content": "Deep convolutional neural networks (DCNNs) have contributed manybreakthroughs in segmentation tasks, especially in the field of medicalimaging. However,  textit domain shift  and  textit corrupted annotations ,which are two common problems in medical imaging, dramatically degrade theperformance of DCNNs in practice. In this paper, we propose a novel robustcross-denoising framework using two peer networks to address domain shift andcorrupted label problems with a peer-review strategy. Specifically, eachnetwork performs as a mentor, mutually supervised to learn from reliablesamples selected by the peer network to combat with corrupted labels. Inaddition, a noise-tolerant loss is proposed to encourage the network to capturethe key location and filter the discrepancy under various noise-contaminantlabels. To further reduce the accumulated error, we introduce aclass-imbalanced cross learning using most confident predictions at theclass-level. Experimental results on REFUGE and Drishti-GS datasets for opticdisc (OD) and optic cup (OC) segmentation demonstrate the superior performanceof our proposed approach to the state-of-the-art methods. ", "id2": "409", "id3": "None"}
{"id": "411", "content": "Convolutional neural networks (CNN) have had unprecedented success in medicalimaging and, in particular, in medical image segmentation. However, despite thefact that segmentation results are closer than ever to the inter-expertvariability, CNNs are not immune to producing anatomically inaccuratesegmentations, even when built upon a shape prior. In this paper, we present aframework for producing cardiac image segmentation maps that are guaranteed torespect pre-defined anatomical criteria, while remaining within theinter-expert variability. The idea behind our method is to use a well-trainedCNN, have it process cardiac images, identify the anatomically implausibleresults and warp these results toward the closest anatomically valid cardiacshape. This warping procedure is carried out with a constrained variationalautoencoder (cVAE) trained to learn a representation of valid cardiac shapesthrough a smooth, yet constrained, latent space. With this cVAE, we can projectany implausible shape into the cardiac latent space and steer it toward theclosest correct shape. We tested our framework on short-axis MRI as well asapical two and four-chamber view ultrasound images, two modalities for whichcardiac shapes are drastically different. With our method, CNNs can now produceresults that are both within the inter-expert variability and alwaysanatomically plausible without having to rely on a shape prior. ", "id2": "410", "id3": "None"}
{"id": "412", "content": "We introduce a new spectral method for image segmentation that incorporateslong range relationships for global appearance modeling. The approach combinestwo different graphs, one is a sparse graph that captures spatial relationshipsbetween nearby pixels and another is a dense graph that captures pairwisesimilarity between all pairs of pixels. We extend the spectral method forNormalized Cuts to this setting by combining the transition matrices of Markovchains associated with each graph. We also derive an efficient method that usesimportance sampling for sparsifying the dense graph of appearancerelationships. This leads to a practical algorithm for segmentinghigh-resolution images. The resulting method can segment challenging imageswithout any filtering or pre-processing. ", "id2": "411", "id3": "None"}
{"id": "413", "content": "Deep neural network (DNN) based approaches have been widely investigated anddeployed in medical image analysis. For example, fully convolutional neuralnetworks (FCN) achieve the state-of-the-art performance in several applicationsof 2D/3D medical image segmentation. Even the baseline neural network models(U-Net, V-Net, etc.) have been proven to be very effective and efficient whenthe training process is set up properly. Nevertheless, to fully exploit thepotentials of neural networks, we propose an automated searching approach forthe optimal training strategy with reinforcement learning. The proposedapproach can be utilized for tuning hyper-parameters, and selecting necessarydata augmentation with certain probabilities. The proposed approach isvalidated on several tasks of 3D medical image segmentation. The performance ofthe baseline model is boosted after searching, and it can achieve comparableaccuracy to other manually-tuned state-of-the-art segmentation approaches. ", "id2": "412", "id3": "None"}
{"id": "414", "content": "Mining and learning the shape variability of underlying population hasbenefited the applications including parametric shape modeling, 3D animation,and image segmentation. The current statistical shape modeling method workswell on learning unstructured shape variations without obvious pose changes(relative rotations of the body parts). Studying the pose variations within ashape population involves segmenting the shapes into different articulatedparts and learning the transformations of the segmented parts. This paperformulates the pose learning problem as mixtures of factor analyzers. Thesegmentation is obtained by components posterior probabilities and therotations in pose variations are learned by the factor loading matrices. Toguarantee that the factor loading matrices are composed by rotation matrices,constraints are imposed and the corresponding closed form optimal solution isderived. Based on the proposed method, the pose variations are automaticallylearned from the given shape populations. The method is applied in motionanimation where new poses are generated by interpolating the existing poses inthe training set. The obtained results are smooth and realistic. ", "id2": "413", "id3": "None"}
{"id": "415", "content": "Shortage of fully annotated datasets has been a limiting factor in developingdeep learning based image segmentation algorithms and the problem becomes morepronounced in multi-organ segmentation. In this paper, we propose a unifiedtraining strategy that enables a novel multi-scale deep neural network to betrained on multiple partially labeled datasets for multi-organ segmentation. Inaddition, a new network architecture for multi-scale feature abstraction isproposed to integrate pyramid input and feature analysis into a U-shape pyramidstructure. To bridge the semantic gap caused by directly merging features fromdifferent scales, an equal convolutional depth mechanism is introduced.Furthermore, we employ a deep supervision mechanism to refine the outputs indifferent scales. To fully leverage the segmentation features from all thescales, we design an adaptive weighting layer to fuse the outputs in anautomatic fashion. All these mechanisms together are integrated into a PyramidInput Pyramid Output Feature Abstraction Network (PIPO-FAN). Our proposedmethod was evaluated on four publicly available datasets, including BTCV, LiTS,KiTS and Spleen, where very promising performance has been achieved. The sourcecode of this work is publicly shared at https://github.com/DIAL-RPI/PIPO-FANfor others to easily reproduce the work and build their own models with theintroduced mechanisms. ", "id2": "414", "id3": "None"}
{"id": "416", "content": "We focus on an important yet challenging problem: using a 2D deep network todeal with 3D segmentation for medical image analysis. Existing approacheseither applied multi-view planar (2D) networks or directly used volumetric (3D)networks for this purpose, but both of them are not ideal: 2D networks cannotcapture 3D contexts effectively, and 3D networks are both memory-consuming andless stable arguably due to the lack of pre-trained models.  In this paper, we bridge the gap between 2D and 3D using a novel approachnamed Elastic Boundary Projection (EBP). The key observation is that, althoughthe object is a 3D volume, what we really need in segmentation is to find itsboundary which is a 2D surface. Therefore, we place a number of pivot points inthe 3D space, and for each pivot, we determine its distance to the objectboundary along a dense set of directions. This creates an elastic shell aroundeach pivot which is initialized as a perfect sphere. We train a 2D deep networkto determine whether each ending point falls within the object, and graduallyadjust the shell so that it gradually converges to the actual shape of theboundary and thus achieves the goal of segmentation. EBP allows boundary-basedsegmentation without cutting a 3D volume into slices or patches, which standsout from conventional 2D and 3D approaches. EBP achieves promising accuracy inabdominal organ segmentation. Our code has been open-sourcedhttps://github.com/twni2016/Elastic-Boundary-Projection. ", "id2": "415", "id3": "None"}
{"id": "417", "content": "Blur detection is the separation of blurred and clear regions of an image,which is an important and challenging task in computer vision. In this work, weregard blur detection as an image segmentation problem. Inspired by the successof the U-net architecture for image segmentation, we design a Multi-ScaleDilated convolutional neural network based on U-net, which we call MSDU-net.The MSDU-net uses a group of multi-scale feature extractors with dilatedconvolutions to extract texture information at different scales. The U-shapearchitecture of the MSDU-net fuses the different-scale texture features andgenerates a semantic feature which allows us to achieve better results on theblur detection task. We show that using the MSDU-net we are able to outperformother state of the art blur detection methods on two publicly availablebenchmarks. ", "id2": "416", "id3": "None"}
{"id": "418", "content": "Most existing black-box optimization methods assume that all variables in thesystem being optimized have equal cost and can change freely at each iteration.However, in many real world systems, inputs are passed through a sequence ofdifferent operations or modules, making variables in earlier stages ofprocessing more costly to update. Such structure imposes a cost on switchingvariables in early parts of a data processing pipeline. In this work, wepropose a new algorithm for switch cost-aware optimization called Lazy ModularBayesian Optimization (LaMBO). This method efficiently identifies the globaloptimum while minimizing cost through a passive change of variables in earlymodules. The method is theoretical grounded and achieves vanishing regret whenaugmented with switching cost. We apply LaMBO to multiple synthetic functionsand a three-stage image segmentation pipeline used in a neuroscienceapplication, where we obtain promising improvements over prevailing cost-awareBayesian optimization algorithms. Our results demonstrate that LaMBO is aneffective strategy for black-box optimization that is capable of minimizingswitching costs in modular systems. ", "id2": "417", "id3": "None"}
{"id2": 1039, "id3": "417", "content": "Most existing black-box optimization methods assume that all variables in thesystem being optimized have equal cost and can change freely at each iteration.However, in many real world systems, inputs are passed through a sequence ofdifferent operations or modules, making variables in earlier stages ofprocessing more costly to update. Such structure imposes a cost on switchingvariables in early parts of a data processing pipeline. In this work, wepropose a new algorithm for switch cost-aware optimization called Lazy ModularBayesian Optimization (LaMBO). This method efficiently identifies the globaloptimum while minimizing cost through a passive change of variables in earlymodules. The method is theoretical grounded and achieves vanishing regret whenaugmented with switching cost. We apply LaMBO to multiple synthetic functionsand a three-stage image segmentation pipeline used in a neuroscienceapplication, where we obtain promising improvements over prevailing cost-awareBayesian optimization algorithms. Our results demonstrate that LaMBO is aneffective strategy for black-box optimization that is capable of minimizingswitching costs in modular systems."}
{"id": "419", "content": "The semantic image segmentation task consists of classifying each pixel of animage into an instance, where each instance corresponds to a class. This taskis a part of the concept of scene understanding or better explaining the globalcontext of an image. In the medical image analysis domain, image segmentationcan be used for image-guided interventions, radiotherapy, or improvedradiological diagnostics. In this review, we categorize the leading deeplearning-based medical and non-medical image segmentation solutions into sixmain groups of deep architectural, data synthesis-based, loss function-based,sequenced models, weakly supervised, and multi-task methods and provide acomprehensive review of the contributions in each of these groups. Further, foreach group, we analyze each variant of these groups and discuss the limitationsof the current approaches and present potential future research directions forsemantic image segmentation. ", "id2": "418", "id3": "None"}
{"id": "420", "content": "Deep Convolutional Neural Networks (DCNNs) are used extensively in medicalimage segmentation and hence 3D navigation for robot-assisted MinimallyInvasive Surgeries (MISs). However, current DCNNs usually use down samplinglayers for increasing the receptive field and gaining abstract semanticinformation. These down sampling layers decrease the spatial dimension offeature maps, which can be detrimental to image segmentation. Atrousconvolution is an alternative for the down sampling layer. It increases thereceptive field whilst maintains the spatial dimension of feature maps. In thispaper, a method for effective atrous rate setting is proposed to achieve thelargest and fully-covered receptive field with a minimum number of atrousconvolutional layers. Furthermore, a new and full resolution DCNN - AtrousConvolutional Neural Network (ACNN), which incorporates cascaded atrousII-blocks, residual learning and Instance Normalization (IN) is proposed.Application results of the proposed ACNN to Magnetic Resonance Imaging (MRI)and Computed Tomography (CT) image segmentation demonstrate that the proposedACNN can achieve higher segmentation Intersection over Unions (IoUs) than U-Netand Deeplabv3+, but with reduced trainable parameters. ", "id2": "419", "id3": "None"}
{"id": "421", "content": "Transfer learning is widely used for training machine learning models. Here,we study the role of transfer learning for training fully convolutionalnetworks (FCNs) for medical image segmentation. Our experiments show thatalthough transfer learning reduces the training time on the target task, theimprovement in segmentation accuracy is highly task/data-dependent. Largerimprovements in accuracy are observed when the segmentation task is morechallenging and the target training data is smaller. We observe thatconvolutional filters of an FCN change little during training for medical imagesegmentation, and still look random at convergence. We further show that quiteaccurate FCNs can be built by freezing the encoder section of the network atrandom values and only training the decoder section. At least for medical imagesegmentation, this finding challenges the common belief that the encodersection needs to learn data/task-specific representations. We examine theevolution of FCN representations to gain a better insight into the effects oftransfer learning on the training dynamics. Our analysis shows that althoughFCNs trained via transfer learning learn different representations than FCNstrained with random initialization, the variability among FCNs trained viatransfer learning can be as high as that among FCNs trained with randominitialization. Moreover, feature reuse is not restricted to the early encoderlayers; rather, it can be more significant in deeper layers. These findingsoffer new insights and suggest alternative ways of training FCNs for medicalimage segmentation. ", "id2": "420", "id3": "None"}
{"id": "422", "content": "Image segmentation is a fundamental vision task and a crucial step for manyapplications. In this paper, we propose a fast image segmentation method basedon a novel super boundary-to-pixel direction (super-BPD) and a customizedsegmentation algorithm with super-BPD. Precisely, we define BPD on each pixelas a two-dimensional unit vector pointing from its nearest boundary to thepixel. In the BPD, nearby pixels from different regions have oppositedirections departing from each other, and adjacent pixels in the same regionhave directions pointing to the other or each other (i.e., around medialpoints). We make use of such property to partition an image into super-BPDs,which are novel informative superpixels with robust direction similarity forfast grouping into segmentation regions. Extensive experimental results onBSDS500 and Pascal Context demonstrate the accuracy and efficency of theproposed super-BPD in segmenting images. In practice, the proposed super-BPDachieves comparable or superior performance with MCG while running at ~25fpsvs. 0.07fps. Super-BPD also exhibits a noteworthy transferability to unseenscenes. The code is publicly available athttps://github.com/JianqiangWan/Super-BPD. ", "id2": "421", "id3": "None"}
{"id": "423", "content": "In deep networks, the lost data details significantly degrade theperformances of image segmentation. In this paper, we propose to apply DiscreteWavelet Transform (DWT) to extract the data details during feature mapdown-sampling, and adopt Inverse DWT (IDWT) with the extracted details duringthe up-sampling to recover the details. We firstly transform DWT/IDWT asgeneral network layers, which are applicable to 1D/2D/3D data and variouswavelets like Haar, Cohen, and Daubechies, etc. Then, we design waveletintegrated deep networks for image segmentation (WaveSNets) based on variousarchitectures, including U-Net, SegNet, and DeepLabv3+. Due to theeffectiveness of the DWT/IDWT in processing data details, experimental resultson CamVid, Pascal VOC, and Cityscapes show that our WaveSNets achieve bettersegmentation performances than their vanilla versions. ", "id2": "422", "id3": "None"}
{"id": "424", "content": "From the autonomous car driving to medical diagnosis, the requirement of thetask of image segmentation is everywhere. Segmentation of an image is one ofthe indispensable tasks in computer vision. This task is comparativelycomplicated than other vision tasks as it needs low-level spatial information.Basically, image segmentation can be of two types: semantic segmentation andinstance segmentation. The combined version of these two basic tasks is knownas panoptic segmentation. In the recent era, the success of deep convolutionalneural networks (CNN) has influenced the field of segmentation greatly and gaveus various successful models to date. In this survey, we are going to take aglance at the evolution of both semantic and instance segmentation work basedon CNN. We have also specified comparative architectural details of somestate-of-the-art models and discuss their training details to present a lucidunderstanding of hyper-parameter tuning of those models. We have also drawn acomparison among the performance of those models on different datasets. Lastly,we have given a glimpse of some state-of-the-art panoptic segmentation models. ", "id2": "423", "id3": "None"}
{"id": "425", "content": "The standard petrography test method for measuring air voids in concrete(ASTM C457) requires a meticulous and long examination of sample phasecomposition under a stereomicroscope. The high expertise and specializedequipment discourage this test for routine concrete quality control. Though thetask can be alleviated with the aid of color-based image segmentation,additional surface color treatment is required. Recently, deep learningalgorithms using convolutional neural networks (CNN) have achievedunprecedented segmentation performance on image testing benchmarks. In thisstudy, we investigated the feasibility of using CNN to conduct concretesegmentation without the use of color treatment. The CNN demonstrated a strongpotential to process a wide range of concretes, including those not involved inmodel training. The experimental results showed that CNN outperforms thecolor-based segmentation by a considerable margin, and has comparable accuracyto human experts. Furthermore, the segmentation time is reduced to mereseconds. ", "id2": "424", "id3": "None"}
{"id": "426", "content": "Convolutional Neural Networks (CNNs) have shown to be powerful medical imagesegmentation models. In this study, we address some of the main unresolvedissues regarding these models. Specifically, training of these models on smallmedical image datasets is still challenging, with many studies promotingtechniques such as transfer learning. Moreover, these models are infamous forproducing over-confident predictions and for failing silently when presentedwith out-of-distribution (OOD) data at test time. In this paper, we advocatefor multi-task learning, i.e., training a single model on several differentdatasets, spanning several different organs of interest and different imagingmodalities. We show that not only a single CNN learns to automaticallyrecognize the context and accurately segment the organ of interest in eachcontext, but also that such a joint model often has more accurate andbetter-calibrated predictions than dedicated models trained separately on eachdataset. Our experiments show that multi-task learning can outperform transferlearning in medical image segmentation tasks. For detecting OOD data, wepropose a method based on spectral analysis of CNN feature maps. We show thatdifferent datasets, representing different imaging modalities and/or differentorgans of interest, have distinct spectral signatures, which can be used toidentify whether or not a test image is similar to the images used to train amodel. We show that this approach is far more accurate than OOD detection basedon prediction uncertainty. The methods proposed in this paper contributesignificantly to improving the accuracy and reliability of CNN-based medicalimage segmentation models. ", "id2": "425", "id3": "None"}
{"id": "427", "content": "Recently, Deep-Neural-Network (DNN) based edge prediction is progressingfast. Although the DNN based schemes outperform the traditional edge detectors,they have much higher computational complexity. It could be that the DNN basededge detectors often adopt the neural net structures designed for high-levelcomputer vision tasks, such as image segmentation and object recognition. Edgedetection is a rather local and simple job, the over-complicated architectureand massive parameters may be unnecessary. Therefore, we propose a traditionalmethod inspired framework to produce good edges with minimal complexity. Wesimplify the network architecture to include Feature Extractor, Enrichment, andSummarizer, which roughly correspond to gradient, low pass filter, and pixelconnection in the traditional edge detection schemes. The proposed structurecan effectively reduce the complexity and retain the edge prediction quality.Our TIN2 (Traditional Inspired Network) model has an accuracy higher than therecent BDCN2 (Bi-Directional Cascade Network) but with a smaller model. ", "id2": "426", "id3": "None"}
{"id": "428", "content": "Deep learning based image segmentation methods have achieved great success,even having human-level accuracy in some applications. However, due to theblack box nature of deep learning, the best method may fail in some situations.Thus predicting segmentation quality without ground truth would be very crucialespecially in clinical practice. Recently, people proposed to train neuralnetworks to estimate the quality score by regression. Although it can achievepromising prediction accuracy, the network suffers robustness problem, e.g. itis vulnerable to adversarial attacks. In this paper, we propose to alleviatethis problem by utilizing the difference between the input image and thereconstructed image, which is conditioned on the segmentation to be assessed,to lower the chance to overfit to the undesired image features from theoriginal input image, and thus to increase the robustness. Results on ACDC17dataset demonstrated our method is promising. ", "id2": "427", "id3": "None"}
{"id": "429", "content": "To extract information at scale, researchers increasingly apply semanticsegmentation techniques to remotely-sensed imagery. While fully-supervisedlearning enables accurate pixel-wise segmentation, compiling the exhaustivedatasets required is often prohibitively expensive. As a result, many non-urbansettings lack the ground-truth needed for accurate segmentation. Existing opensource infrastructure data for these regions can be inexact and non-exhaustive.Open source infrastructure annotations like OpenStreetMaps (OSM) arerepresentative of this issue: while OSM labels provide global insights to roadand building footprints, noisy and partial annotations limit the performance ofsegmentation algorithms that learn from them. In this paper, we present a noveland generalizable two-stage framework that enables improved pixel-wise imagesegmentation given misaligned and missing annotations. First, we introduce theAlignment Correction Network to rectify incorrectly registered open sourcelabels. Next, we demonstrate a segmentation model -- the Pointer SegmentationNetwork -- that uses corrected labels to predict infrastructure footprintsdespite missing annotations. We test sequential performance on the AIRSdataset, achieving a mean intersection-over-union score of 0.79; moreimportantly, model performance remains stable as we decrease the fraction ofannotations present. We demonstrate the transferability of our method to lowerquality data, by applying the Alignment Correction Network to OSM labels tocorrect building footprints; we also demonstrate the accuracy of the PointerSegmentation Network in predicting cropland boundaries in California frommedium resolution data. Overall, our methodology is robust for multipleapplications with varied amounts of training data present, thus offering amethod to extract reliable information from noisy, partial data. ", "id2": "428", "id3": "None"}
{"id": "430", "content": "Fully supervised deep neural networks for segmentation usually require amassive amount of pixel-level labels which are manually expensive to create. Inthis work, we develop a multi-task learning method to relax this constraint. Weregard the segmentation problem as a sequence of approximation subproblems thatare recursively defined and in increasing levels of approximation accuracy. Thesubproblems are handled by a framework that consists of 1) a segmentation taskthat learns from pixel-level ground truth segmentation masks of a smallfraction of the images, 2) a recursive approximation task that conducts partialobject regions learning and data-driven mask evolution starting from partialmasks of each object instance, and 3) other problem oriented auxiliary tasksthat are trained with sparse annotations and promote the learning of dedicatedfeatures. Most training images are only labeled by (rough) partial masks, whichdo not contain exact object boundaries, rather than by their full segmentationmasks. During the training phase, the approximation task learns the statisticsof these partial masks, and the partial regions are recursively increasedtowards object boundaries aided by the learned information from thesegmentation task in a fully data-driven fashion. The network is trained on anextremely small amount of precisely segmented images and a large set of coarselabels. Annotations can thus be obtained in a cheap way. We demonstrate theefficiency of our approach in three applications with microscopy images andultrasound images. ", "id2": "429", "id3": "None"}
{"id2": 1040, "id3": "429", "content": "Fully supervised deep neural networks for segmentation usually require amassive amount of pixel-level labels which are manually expensive to create. Inthis work, we develop a multi-task learning method to relax this constraint. Weregard the segmentation problem as a sequence of approximation subproblems thatare recursively defined and in increasing levels of approximation accuracy. Thesubproblems are handled by a framework that consists of 1) a segmentation taskthat learns from pixel-level ground truth segmentation masks of a smallfraction of the images, 2) a recursive approximation task that conducts partialobject regions learning and data-driven mask evolution starting from partialmasks of each object instance, and 3) other problem oriented auxiliary tasksthat are trained with sparse annotations and promote the learning of dedicatedfeatures. Most training images are only labeled by (rough) partial masks, whichdo not contain exact object boundaries, rather than by their full segmentationmasks. During the training phase, the approximation task learns the statisticsof these partial masks, and the partial regions are recursively increasedtowards object boundaries aided by the learned information from thesegmentation task in a fully data-driven fashion. The network is trained on anextremely small amount of precisely segmented images and a large set of coarselabels. Annotations can thus be obtained in a cheap way. We demonstrate theefficiency of our approach in three applications with microscopy images andultrasound images."}
{"id": "431", "content": "Worldwide, prostate cancer is one of the main cancers affecting men. Thefinal diagnosis of prostate cancer is based on the visual detection of Gleasonpatterns in prostate biopsy by pathologists. Computer-aided-diagnosis systemsallow to delineate and classify the cancerous patterns in the tissue viacomputer-vision algorithms in order to support the physicians task. Themethodological core of this work is a U-Net convolutional neural network forimage segmentation modified with residual blocks able to segment canceroustissue according to the full Gleason system. This model outperforms otherwell-known architectures, and reaches a pixel-level Cohens quadratic Kappa of0.52, at the level of previous image-level works in the literature, butproviding also a detailed localisation of the patterns. ", "id2": "430", "id3": "None"}
{"id": "432", "content": "In some complicated datasets, due to the presence of noisy data points andoutliers, cluster validity indices can give conflicting results in determiningthe optimal number of clusters. This paper presents a new validity index forfuzzy-possibilistic c-means clustering called Fuzzy-Possibilistic (FP) index,which works well in the presence of clusters that vary in shape and density.Moreover, FPCM like most of the clustering algorithms is susceptible to someinitial parameters. In this regard, in addition to the number of clusters, FPCMrequires a priori selection of the degree of fuzziness and the degree oftypicality. Therefore, we presented an efficient procedure for determiningtheir optimal values. The proposed approach has been evaluated using severalsynthetic and real-world datasets. Final computational results demonstrate thecapabilities and reliability of the proposed approach compared with severalwell-known fuzzy validity indices in the literature. Furthermore, to clarifythe ability of the proposed method in real applications, the proposed method isimplemented in microarray gene expression data clustering and medical imagesegmentation. ", "id2": "431", "id3": "None"}
{"id": "433", "content": "Convex Shapes (CS) are common priors for optic disc and cup segmentation ineye fundus images. It is important to design proper techniques to representconvex shapes. So far, it is still a problem to guarantee that the outputobjects from a Deep Neural Convolution Networks (DCNN) are convex shapes. Inthis work, we propose a technique which can be easily integrated into thecommonly used DCNNs for image segmentation and guarantee that outputs areconvex shapes. This method is flexible and it can handle multiple objects andallow some of the objects to be convex. Our method is based on the dualrepresentation of the sigmoid activation function in DCNNs. In the dual space,the convex shape prior can be guaranteed by a simple quadratic constraint on abinary representation of the shapes. Moreover, our method can also integratespatial regularization and some other shape prior using a soft thresholdingdynamics (STD) method. The regularization can make the boundary curves of thesegmentation objects to be simultaneously smooth and convex. We design a verystable active set projection algorithm to numerically solve our model. Thisalgorithm can form a new plug-and-play DCNN layer called CS-STD whose outputsmust be a nearly binary segmentation of convex objects. In the CS-STD block,the convexity information can be propagated to guide the DCNN in both forwardand backward propagation during training and prediction process. As anapplication example, we apply the convexity prior layer to the retinal fundusimages segmentation by taking the popular DeepLabV3+ as a backbone network.Experimental results on several public datasets show that our method isefficient and outperforms the classical DCNN segmentation methods. ", "id2": "432", "id3": "None"}
{"id": "434", "content": "Electrocardiogram (ECG) detection and delineation are key steps for numeroustasks in clinical practice, as ECG is the most performed non-invasive test forassessing cardiac condition. State-of-the-art algorithms employ digital signalprocessing (DSP), which require laborious rule adaptation to new morphologies.In contrast, deep learning (DL) algorithms, especially for classification, aregaining weight in academic and industrial settings. However, the lack of modelexplainability and small databases hinder their applicability. We demonstrateDL can be successfully applied to low interpretative tasks by embedding ECGdetection and delineation onto a segmentation framework. For this purpose, weadapted and validated the most used neural network architecture for imagesegmentation, the U-Net, to one-dimensional data. The model was trained usingPhysioNets QT database, comprised of 105 ambulatory ECG recordings, forsingle- and multi-lead scenarios. To alleviate data scarcity, dataregularization techniques such as pre-training with low-quality data labels,performing ECG-based data augmentation and applying strong model regularizersto the architecture were attempted. Other variations in the models capacity(U-Nets depth and width), alongside the application of state-of-the-artadditions, were evaluated. These variations were exhaustively validated in a5-fold cross-validation manner. The best performing configuration reachedprecisions of 90.12%, 99.14% and 98.25% and recalls of 98.73%, 99.94% and99.88% for the P, QRS and T waves, respectively, on par with DSP-basedapproaches. Despite being a data-hungry technique trained on a small dataset,DL-based approaches demonstrate to be a viable alternative to traditionalDSP-based ECG processing techniques. ", "id2": "433", "id3": "None"}
{"id": "435", "content": "Deep convolutional neural networks have achieved remarkable progress on avariety of medical image computing tasks. A common problem when applyingsupervised deep learning methods to medical images is the lack of labeled data,which is very expensive and time-consuming to be collected. In this paper, wepresent a novel semi-supervised method for medical image segmentation, wherethe network is optimized by the weighted combination of a common supervisedloss for labeled inputs only and a regularization loss for both labeled andunlabeled data. To utilize the unlabeled data, our method encourages theconsistent predictions of the network-in-training for the same input underdifferent regularizations. Aiming for the semi-supervised segmentation problem,we enhance the effect of regularization for pixel-level predictions byintroducing a transformation, including rotation and flipping, consistentscheme in our self-ensembling model. With the aim of semi-supervisedsegmentation tasks, we introduce a transformation consistent strategy in ourself-ensembling model to enhance the regularization effect for pixel-levelpredictions. We have extensively validated the proposed semi-supervised methodon three typical yet challenging medical image segmentation tasks: (i) skinlesion segmentation from dermoscopy images on International Skin ImagingCollaboration (ISIC) 2017 dataset, (ii) optic disc segmentation from fundusimages on Retinal Fundus Glaucoma Challenge (REFUGE) dataset, and (iii) liversegmentation from volumetric CT scans on Liver Tumor Segmentation Challenge(LiTS) dataset. Compared to the state-of-the-arts, our proposed method showssuperior segmentation performance on challenging 2D/3D medical images,demonstrating the effectiveness of our semi-supervised method for medical imagesegmentation. ", "id2": "434", "id3": "None"}
{"id": "436", "content": "Lake ice is a strong climate indicator and has been recognised as part of theEssential Climate Variables (ECV) by the Global Climate Observing System(GCOS). The dynamics of freezing and thawing, and possible shifts of freezingpatterns over time, can help in understanding the local and global climatesystems. One way to acquire the spatio-temporal information about lake iceformation, independent of clouds, is to analyse webcam images. This paperintends to move towards a universal model for monitoring lake ice with freelyavailable webcam data. We demonstrate good performance, including the abilityto generalise across different winters and different lakes, with astate-of-the-art Convolutional Neural Network (CNN) model for semantic imagesegmentation, Deeplab v3+. Moreover, we design a variant of that model, termedDeep-U-Lab, which predicts sharper, more correct segmentation boundaries. Wehave tested the models ability to generalise with data from multiple cameraviews and two different winters. On average, it achievesintersection-over-union (IoU) values of ~71% across different cameras and ~69%across different winters, greatly outperforming prior work. Going even further,we show that the model even achieves 60% IoU on arbitrary images scraped fromphoto-sharing web sites. As part of the work, we introduce a new benchmarkdataset of webcam images, Photi-LakeIce, from multiple cameras and twodifferent winters, along with pixel-wise ground truth annotations. ", "id2": "435", "id3": "None"}
{"id": "437", "content": "We extend first-order model agnostic meta-learning algorithms (includingFOMAML and Reptile) to image segmentation, present a novel neural networkarchitecture built for fast learning which we call EfficientLab, and leverage aformal definition of the test error of meta-learning algorithms to decreaseerror on out of distribution tasks. We show state of the art results on theFSS-1000 dataset by meta-training EfficientLab with FOMAML and using Bayesianoptimization to infer the optimal test-time adaptation routine hyperparameters.We also construct a small benchmark dataset, FP-k, for the empirical study ofhow meta-learning systems perform in both few- and many-shot settings. On theFP-k dataset, we show that meta-learned initializations provide value forcanonical few-shot image segmentation but their performance is quickly matchedby conventional transfer learning with performance being equal beyond 10labeled examples. Our code, meta-learned model, and the FP-k dataset areavailable at https://github.com/ml4ai/mliis . ", "id2": "436", "id3": "None"}
{"id": "438", "content": "Semi-supervised learning has recently been attracting attention as analternative to fully supervised models that require large pools of labeleddata. Moreover, optimizing a model for multiple tasks can provide bettergeneralizability than single-task learning. Leveraging self-supervision andadversarial training, we propose a novel general purpose semi-supervised,multiple-task model---namely, self-supervised, semi-supervised, multitasklearning (S$^4$MTL)---for accomplishing two important tasks in medical imaging,segmentation and diagnostic classification. Experimental results on chest andspine X-ray datasets suggest that our S$^4$MTL model significantly outperformssemi-supervised single task, semi/fully-supervised multitask, andfully-supervised single task models, even with a 50 % reduction of class andsegmentation labels. We hypothesize that our proposed model can be effective intackling limited annotation problems for joint training, not only in medicalimaging domains, but also for general-purpose vision tasks. ", "id2": "437", "id3": "None"}
{"id": "439", "content": "Few-shot segmentation (FSS) methods perform image segmentation for aparticular object class in a target (query) image, using a small set of(support) image-mask pairs. Recent deep neural network based FSS methodsleverage high-dimensional feature similarity between the foreground features ofthe support images and the query image features. In this work, we demonstrategaps in the utilization of this similarity information in existing methods, andpresent a framework - SimPropNet, to bridge those gaps. We propose to jointlypredict the support and query masks to force the support features to sharecharacteristics with the query features. We also propose to utilizesimilarities in the background regions of the query and support images using anovel foreground-background attentive fusion mechanism. Our method achievesstate-of-the-art results for one-shot and five-shot segmentation on thePASCAL-5i dataset. The paper includes detailed analysis and ablation studiesfor the proposed improvements and quantitative comparisons with contemporarymethods. ", "id2": "438", "id3": "None"}
{"id": "440", "content": "We propose adversarial constrained-CNN loss, a new paradigm ofconstrained-CNN loss methods, for weakly supervised medical image segmentation.In the new paradigm, prior knowledge is encoded and depicted by referencemasks, and is further employed to impose constraints on segmentation outputsthrough adversarial learning with reference masks. Unlike pseudo label methodsfor weakly supervised segmentation, such reference masks are used to train adiscriminator rather than a segmentation network, and thus are not required tobe paired with specific images. Our new paradigm not only greatly facilitatesimposing prior knowledge on networks outputs, but also provides stronger andhigher-order constraints, i.e., distribution approximation, through adversariallearning. Extensive experiments involving different medical modalities,different anatomical structures, different topologies of the object ofinterest, different levels of prior knowledge and weakly supervised annotationswith different annotation ratios is conducted to evaluate our ACCL method.Consistently superior segmentation results over the size constrained-CNN lossmethod have been achieved, some of which are close to the results of fullsupervision, thus fully verifying the effectiveness and generalization of ourmethod. Specifically, we report an average Dice score of 75.4% with an averageannotation ratio of 0.65%, surpassing the prior art, i.e., the sizeconstrained-CNN loss method, by a large margin of 11.4%. Our codes are madepublicly available at https://github.com/PengyiZhang/ACCL. ", "id2": "439", "id3": "None"}
{"id": "441", "content": "The ability of neural networks to continuously learn and adapt to new taskswhile retaining prior knowledge is crucial for many applications. However,current neural networks tend to forget previously learned tasks when trained onnew ones, i.e., they suffer from Catastrophic Forgetting (CF). The objective ofContinual Learning (CL) is to alleviate this problem, which is particularlyrelevant for medical applications, where it may not be feasible to store andaccess previously used sensitive patient data. In this work, we propose aContinual Learning approach for brain segmentation, where a single network isconsecutively trained on samples from different domains. We build upon animportance driven approach and adapt it for medical image segmentation.Particularly, we introduce learning rate regularization to prevent the loss ofthe networks knowledge. Our results demonstrate that directly restricting theadaptation of important network parameters clearly reduces CatastrophicForgetting for segmentation across domains. ", "id2": "440", "id3": "None"}
{"id2": 1041, "id3": "440", "content": "The ability of neural networks to continuously learn and adapt to new taskswhile retaining prior knowledge is crucial for many applications. However,current neural networks tend to forget previously learned tasks when trained onnew ones, i.e., they suffer from Catastrophic Forgetting (CF). The objective ofContinual Learning (CL) is to alleviate this problem, which is particularlyrelevant for medical applications, where it may not be feasible to store andaccess previously used sensitive patient data. In this work, we propose aContinual Learning approach for brain segmentation, where a single network isconsecutively trained on samples from different domains. We build upon animportance driven approach and adapt it for medical image segmentation.Particularly, we introduce learning rate regularization to prevent the loss ofthe networks knowledge. Our results demonstrate that directly restricting theadaptation of important network parameters clearly reduces CatastrophicForgetting for segmentation across domains."}
{"id": "442", "content": "Human brain is a layered structure, and performs not only a feedforwardprocess from a lower layer to an upper layer but also a feedback process froman upper layer to a lower layer. The layer is a collection of neurons, andneural network is a mathematical model of the function of neurons. Althoughneural network imitates the human brain, everyone uses only feedforward processfrom the lower layer to the upper layer, and feedback process from the upperlayer to the lower layer is not used. Therefore, in this paper, we proposeFeedback U-Net using Convolutional LSTM which is the segmentation method usingConvolutional LSTM and feedback process. The output of U-net gave feedback tothe input, and the second round is performed. By using Convolutional LSTM, thefeatures in the second round are extracted based on the features acquired inthe first round. On both of the Drosophila cell image and Mouse cell imagedatasets, our method outperformed conventional U-Net which uses onlyfeedforward process. ", "id2": "441", "id3": "None"}
{"id": "443", "content": "Semantic image segmentation is one of the most important tasks in medicalimage analysis. Most state-of-the-art deep learning methods require a largenumber of accurately annotated examples for model training. However, accurateannotation is difficult to obtain especially in medical applications. In thispaper, we propose a spatially constrained deep convolutional neural network(DCNN) to achieve smooth and robust image segmentation using inaccuratelyannotated labels for training. In our proposed method, image segmentation isformulated as a graph optimization problem that is solved by a DCNN modellearning process. The cost function to be optimized consists of a unary termthat is calculated by cross entropy measurement and a pairwise term that isbased on enforcing a local label consistency. The proposed method has beenevaluated based on corneal confocal microscopic (CCM) images for nerve fibersegmentation, where accurate annotations are extremely difficult to beobtained. Based on both the quantitative result of a synthetic dataset andqualitative assessment of a real dataset, the proposed method has achievedsuperior performance in producing high quality segmentation results even withinaccurate labels for training. ", "id2": "442", "id3": "None"}
{"id": "444", "content": "We study the energy minimization problem in low-level vision tasks from anovel perspective. We replace the heuristic regularization term with alearnable subspace constraint, and preserve the data term to exploit domainknowledge derived from the first principle of a task. This learning subspaceminimization (LSM) framework unifies the network structures and the parametersfor many low-level vision tasks, which allows us to train a single network formultiple tasks simultaneously with completely shared parameters, and evengeneralizes the trained network to an unseen task as long as its data term canbe formulated. We demonstrate our LSM framework on four low-level tasksincluding interactive image segmentation, video segmentation, stereo matching,and optical flow, and validate the network on various datasets. The experimentsshow that the proposed LSM generates state-of-the-art results with smallermodel size, faster training convergence, and real-time inference. ", "id2": "443", "id3": "None"}
{"id": "445", "content": "3D convolution neural networks (CNN) have been proved very successful inparsing organs or tumours in 3D medical images, but it remains sophisticatedand time-consuming to choose or design proper 3D networks given different taskcontexts. Recently, Neural Architecture Search (NAS) is proposed to solve thisproblem by searching for the best network architecture automatically. However,the inconsistency between search stage and deployment stage often exists in NASalgorithms due to memory constraints and large search space, which could becomemore serious when applying NAS to some memory and time consuming tasks, such as3D medical image segmentation. In this paper, we propose coarse-to-fine neuralarchitecture search (C2FNAS) to automatically search a 3D segmentation networkfrom scratch without inconsistency on network size or input size. Specifically,we divide the search procedure into two stages: 1) the coarse stage, where wesearch the macro-level topology of the network, i.e. how each convolutionmodule is connected to other modules; 2) the fine stage, where we search atmicro-level for operations in each cell based on previous searched macro-leveltopology. The coarse-to-fine manner divides the search procedure into twoconsecutive stages and meanwhile resolves the inconsistency. We evaluate ourmethod on 10 public datasets from Medical Segmentation Decalthon (MSD)challenge, and achieve state-of-the-art performance with the network searchedusing one dataset, which demonstrates the effectiveness and generalization ofour searched models. ", "id2": "444", "id3": "None"}
{"id2": 1042, "id3": "444", "content": "3D convolution neural networks (CNN) have been proved very successful inparsing organs or tumours in 3D medical images, but it remains sophisticatedand time-consuming to choose or design proper 3D networks given different taskcontexts. Recently, Neural Architecture Search (NAS) is proposed to solve thisproblem by searching for the best network architecture automatically. However,the inconsistency between search stage and deployment stage often exists in NASalgorithms due to memory constraints and large search space, which could becomemore serious when applying NAS to some memory and time consuming tasks, such as3D medical image segmentation. In this paper, we propose coarse-to-fine neuralarchitecture search (C2FNAS) to automatically search a 3D segmentation networkfrom scratch without inconsistency on network size or input size. Specifically,we divide the search procedure into two stages: 1) the coarse stage, where wesearch the macro-level topology of the network, i.e. how each convolutionmodule is connected to other modules; 2) the fine stage, where we search atmicro-level for operations in each cell based on previous searched macro-leveltopology. The coarse-to-fine manner divides the search procedure into twoconsecutive stages and meanwhile resolves the inconsistency. We evaluate ourmethod on 10 public datasets from Medical Segmentation Decalthon (MSD)challenge, and achieve state-of-the-art performance with the network searchedusing one dataset, which demonstrates the effectiveness and generalization ofour searched models."}
{"id": "446", "content": "Segmentation partitions an image into different regions containing pixelswith similar attributes. A standard non-contextual variant of Fuzzy C-meansclustering algorithm (FCM), considering its simplicity is generally used inimage segmentation. Using FCM has its disadvantages like it is dependent on theinitial guess of the number of clusters and highly sensitive to noise.Satisfactory visual segments cannot be obtained using FCM. Particle SwarmOptimization (PSO) belongs to the class of evolutionary algorithms and has goodconvergence speed and fewer parameters compared to Genetic Algorithms (GAs). Anoptimized version of PSO can be combined with FCM to act as a properinitializer for the algorithm thereby reducing its sensitivity to initialguess. A hybrid PSO algorithm named Adaptive Particle Swarm Optimization (APSO)which improves in the calculation of various hyper parameters like inertiaweight, learning factors over standard PSO, using insights from swarmbehaviour, leading to improvement in cluster quality can be used. This paperpresents a new image segmentation algorithm called Adaptive Particle SwarmOptimization and Fuzzy C-means Clustering Algorithm (APSOF), which is based onAdaptive Particle Swarm Optimization (APSO) and Fuzzy C-means clustering.Experimental results show that APSOF algorithm has edge over FCM in correctlyidentifying the optimum cluster centers, there by leading to accurateclassification of the image pixels. Hence, APSOF algorithm has superiorperformance in comparison with classic Particle Swarm Optimization (PSO) andFuzzy C-means clustering algorithm (FCM) for image segmentation. ", "id2": "445", "id3": "None"}
{"id": "447", "content": "Deep learning based image segmentation has achieved the state-of-the-artperformance in many medical applications such as lesion quantification, organdetection, etc. However, most of the methods rely on supervised learning, whichrequire a large set of high-quality labeled data. Data annotation is generallyan extremely time-consuming process. To address this problem, we propose ageneric semi-supervised learning framework for image segmentation based on adeep convolutional neural network (DCNN). An encoder-decoder based DCNN isinitially trained using a few annotated training samples. This initiallytrained model is then copied into sub-models and improved iteratively usingrandom subsets of unlabeled data with pseudo labels generated from modelstrained in the previous iteration. The number of sub-models is graduallydecreased to one in the final iteration. We evaluate the proposed method on apublic grand-challenge dataset for skin lesion segmentation. Our method is ableto significantly improve beyond fully supervised model learning byincorporating unlabeled data. ", "id2": "446", "id3": "None"}
{"id2": 1043, "id3": "446", "content": "Deep learning based image segmentation has achieved the state-of-the-artperformance in many medical applications such as lesion quantification, organdetection, etc. However, most of the methods rely on supervised learning, whichrequire a large set of high-quality labeled data. Data annotation is generallyan extremely time-consuming process. To address this problem, we propose ageneric semi-supervised learning framework for image segmentation based on adeep convolutional neural network (DCNN). An encoder-decoder based DCNN isinitially trained using a few annotated training samples. This initiallytrained model is then copied into sub-models and improved iteratively usingrandom subsets of unlabeled data with pseudo labels generated from modelstrained in the previous iteration. The number of sub-models is graduallydecreased to one in the final iteration. We evaluate the proposed method on apublic grand-challenge dataset for skin lesion segmentation. Our method is ableto significantly improve beyond fully supervised model learning byincorporating unlabeled data."}
{"id": "448", "content": "In machine learning and other fields, suggesting a good solution to a problemis usually a harder task than evaluating the quality of such a solution. Thisasymmetry is the basis for a large number of selection oriented methods thatuse a generator system to guess a set of solutions and an evaluator system torank and select the best solutions. This work examines the use of this approachto the problem of panoptic image segmentation and class agnostic partssegmentation. The generator/evaluator approach for this case consists of twoindependent convolutional neural nets: a generator net that suggests varietysegments corresponding to objects, stuff and parts regions in the image, and anevaluator net that chooses the best segments to be merged into the segmentationmap. The result is a trial and error evolutionary approach in which a generatorthat guesses segments with low average accuracy, but with wide variability, canstill produce good results when coupled with an accurate evaluator. Thegenerator consists of a Pointer net that receives an image and a point in theimage, and predicts the region of the segment containing the point. Generatingand evaluating each segment separately is essential in this case since itdemands exponentially fewer guesses compared to a system that guesses andevaluates the full segmentation map in each try. The classification of theselected segments is done by an independent region-specific classification net.This allows the segmentation to be class agnostic and hence, capable ofsegmenting unfamiliar categories that were not part of the training set. Themethod was examined on the COCO Panoptic segmentation benchmark and gaveresults comparable to those of the basic semantic segmentation and Mask-RCNNmethods. In addition, the system was used for the task of splitting objects ofunseen classes (that did not appear in the training set) into parts. ", "id2": "447", "id3": "None"}
{"id": "449", "content": "At present, adversarial attacks are designed in a task-specific fashion.However, for downstream computer vision tasks such as image captioning, imagesegmentation etc., the current deep learning systems use an image classifierlike VGG16, ResNet50, Inception-v3 etc. as a feature extractor. Keeping this inmind, we propose Mimic and Fool, a task agnostic adversarial attack. Given afeature extractor, the proposed attack finds an adversarial image which canmimic the image feature of the original image. This ensures that the two imagesgive the same (or similar) output regardless of the task. We randomly select1000 MSCOCO validation images for experimentation. We perform experiments ontwo image captioning models, Show and Tell, Show Attend and Tell and one VQAmodel, namely, end-to-end neural module network (N2NMN). The proposed attackachieves success rate of 74.0%, 81.0% and 87.1% for Show and Tell, Show Attendand Tell and N2NMN respectively. We also propose a slight modification to ourattack to generate natural-looking adversarial images. In addition, we alsoshow the applicability of the proposed attack for invertible architecture.Since Mimic and Fool only requires information about the feature extractor ofthe model, it can be considered as a gray-box attack. ", "id2": "448", "id3": "None"}
{"id": "450", "content": "Recent years have witnessed the great progress of deep neural networks onsemantic segmentation, particularly in medical imaging. Nevertheless, traininghigh-performing models require large amounts of pixel-level ground truth masks,which can be prohibitive to obtain in the medical domain. Furthermore, trainingsuch models in a low-data regime highly increases the risk of overfitting.Recent attempts to alleviate the need for large annotated datasets havedeveloped training strategies under the few-shot learning paradigm, whichaddresses this shortcoming by learning a novel class from only a few labeledexamples. In this context, a segmentation model is trained on episodes, whichrepresent different segmentation problems, each of them trained with a verysmall labeled dataset. In this work, we propose a novel few-shot learningframework for semantic segmentation, where unlabeled images are also madeavailable at each episode. To handle this new learning paradigm, we propose toinclude surrogate tasks that can leverage very powerful supervisory signals--derived from the data itself-- for semantic feature learning. We show thatincluding unlabeled surrogate tasks in the episodic training leads to morepowerful feature representations, which ultimately results in bettergenerability to unseen tasks. We demonstrate the efficiency of our method inthe task of skin lesion segmentation in two publicly available datasets.Furthermore, our approach is general and model-agnostic, which can be combinedwith different deep architectures. ", "id2": "449", "id3": "None"}
{"id": "451", "content": "Exploiting more information from ground truth (GT) images now is a newresearch direction for further improving CNNs performance in CT imagesegmentation. Previous methods focus on devising the loss function forfulfilling such a purpose. However, it is rather difficult to devise a generaland optimization-friendly loss function. We here present a novel and practicalmethod that exploits GT images beyond the loss function. Our insight is thatfeature maps of two CNNs trained respectively on GT and CT images should besimilar on some metric space, because they both are used to describe the sameobjects for the same purpose. We hence exploit GT images by enforcing such twoCNNs feature maps to be consistent. We assess the proposed method on two datasets, and compare its performance to several competitive methods. Extensiveexperimental results show that the proposed method is effective, outperformingall the compared methods. ", "id2": "450", "id3": "None"}
{"id": "452", "content": "As supervised semantic segmentation is reaching satisfying results, manyrecent papers focused on making segmentation network architectures faster,smaller and more efficient. In particular, studies often aim to reach the stageto which they can claim to be real-time. Achieving this goal is especiallyrelevant in the context of real-time video operations for autonomous vehiclesand robots, or medical imaging during surgery.  The common metric used for assessing these methods is so far the same as theones used for image segmentation without time constraint: mean Intersectionover Union (mIoU). In this paper, we argue that this metric is not relevantenough for real-time video as it does not take into account the processing time(latency) of the network. We propose a similar but more relevant metric calledFLAME for video-segmentation networks, that compares the output segmentation ofthe network with the ground truth segmentation of the current video frame atthe time when the network finishes the processing.  We perform experiments to compare a few networks using this metric andpropose a simple addition to network training to enhance results according tothat metric. ", "id2": "451", "id3": "None"}
{"id": "453", "content": "Deep Convolutional Neural Networks (DCNNs) have recently shown outstandingperformance in semantic image segmentation. However, state-of-the-artDCNN-based semantic segmentation methods usually suffer from high computationalcomplexity due to the use of complex network architectures. This greatly limitstheir applications in the real-world scenarios that require real-timeprocessing. In this paper, we propose a real-time high-performance DCNN-basedmethod for robust semantic segmentation of urban street scenes, which achievesa good trade-off between accuracy and speed. Specifically, a LightweightBaseline Network with Atrous convolution and Attention (LBN-AA) is firstly usedas our baseline network to efficiently obtain dense feature maps. Then, theDistinctive Atrous Spatial Pyramid Pooling (DASPP), which exploits thedifferent sizes of pooling operations to encode the rich and distinctivesemantic information, is developed to detect objects at multiple scales.Meanwhile, a Spatial detail-Preserving Network (SPN) with shallow convolutionallayers is designed to generate high-resolution feature maps preserving thedetailed spatial information. Finally, a simple but practical Feature FusionNetwork (FFN) is used to effectively combine both shallow and deep featuresfrom the semantic branch (DASPP) and the spatial branch (SPN), respectively.Extensive experimental results show that the proposed method respectivelyachieves the accuracy of 73.6% and 68.0% mean Intersection over Union (mIoU)with the inference speed of 51.0 fps and 39.3 fps on the challenging Cityscapesand CamVid test datasets (by only using a single NVIDIA TITAN X card). Thisdemonstrates that the proposed method offers excellent performance at thereal-time speed for semantic segmentation of urban street scenes. ", "id2": "452", "id3": "None"}
{"id": "454", "content": "Flow-based generative models have highly desirable properties like exactlog-likelihood evaluation and exact latent-variable inference, however they arestill in their infancy and have not received as much attention as alternativegenerative models. In this paper, we introduce C-Flow, a novel conditioningscheme that brings normalizing flows to an entirely new scenario with greatpossibilities for multi-modal data modeling. C-Flow is based on a parallelsequence of invertible mappings in which a source flow guides the target flowat every step, enabling fine-grained control over the generation process. Wealso devise a new strategy to model unordered 3D point clouds that, incombination with the conditioning scheme, makes it possible to address 3Dreconstruction from a single image and its inverse problem of rendering animage given a point cloud. We demonstrate our conditioning method to be veryadaptable, being also applicable to image manipulation, style transfer andmulti-modal image-to-image mapping in a diversity of domains, including RGBimages, segmentation maps, and edge masks. ", "id2": "453", "id3": "None"}
{"id": "455", "content": "Video feedback provides a wealth of information about surgical procedures andis the main sensory cue for surgeons. Scene understanding is crucial tocomputer assisted interventions (CAI) and to post-operative analysis of thesurgical procedure. A fundamental building block of such capabilities is theidentification and localization of surgical instruments and anatomicalstructures through semantic segmentation. Deep learning has advanced semanticsegmentation techniques in the recent years but is inherently reliant on theavailability of labeled datasets for model training. This paper introduces adataset for semantic segmentation of cataract surgery videos. The annotatedimages are part of the publicly available CATARACTS challenge dataset. Inaddition, we benchmark the performance of several state-of-the-art deeplearning models for semantic segmentation on the presented dataset. The datasetis publicly available at https://cataracts.grand-challenge.org/CaDIS/ . ", "id2": "454", "id3": "None"}
{"id": "456", "content": "The Know Your Customer (KYC) and Anti Money Laundering (AML) are worldwidepractices to online customer identification based on personal identificationdocuments, similarity and liveness checking, and proof of address. To answerthe basic regulation question: are you whom you say you are? The customer needsto upload valid identification documents (ID). This task imposes somecomputational challenges since these documents are diverse, may presentdifferent and complex backgrounds, some occlusion, partial rotation, poorquality, or damage. Advanced text and document segmentation algorithms wereused to process the ID images. In this context, we investigated a method basedon U-Net to detect the document edges and text regions in ID images. Besidesthe promising results on image segmentation, the U-Net based approach iscomputationally expensive for a real application, since the image segmentationis a customer device task. We propose a model optimization based on OctaveConvolutions to qualify the method to situations where storage, processing, andtime resources are limited, such as in mobile and robotic applications. Weconducted the evaluation experiments in two new datasets CDPhotoDataset andDTDDataset, which are composed of real ID images of Brazilian documents. Ourresults showed that the proposed models are efficient to document segmentationtasks and portable. ", "id2": "455", "id3": "None"}
{"id": "457", "content": "Biomedical imaging is a driver of scientific discovery and core component ofmedical care, currently stimulated by the field of deep learning. Whilesemantic segmentation algorithms enable 3D image analysis and quantification inmany applications, the design of respective specialised solutions isnon-trivial and highly dependent on dataset properties and hardware conditions.We propose nnU-Net, a deep learning framework that condenses the current domainknowledge and autonomously takes the key decisions required to transfer a basicarchitecture to different datasets and segmentation tasks. Without manualtuning, nnU-Net surpasses most specialised deep learning pipelines in 19 publicinternational competitions and sets a new state of the art in the majority ofthe 49 tasks. The results demonstrate a vast hidden potential in the systematicadaptation of deep learning methods to different datasets. We make nnU-Netpublicly available as an open-source tool that can effectively be usedout-of-the-box, rendering state of the art segmentation accessible tonon-experts and catalyzing scientific progress as a framework for automatedmethod design. ", "id2": "456", "id3": "None"}
{"id": "458", "content": "This paper presents an efficient annotation procedure and an applicationthereof to end-to-end, rich semantic segmentation of the sensed environmentusing FMCW scanning radar. We advocate radar over the traditional sensors usedfor this task as it operates at longer ranges and is substantially more robustto adverse weather and illumination conditions. We avoid laborious manuallabelling by exploiting the largest radar-focused urban autonomy datasetcollected to date, correlating radar scans with RGB cameras and LiDAR sensors,for which semantic segmentation is an already consolidated procedure. Thetraining procedure leverages a state-of-the-art natural image segmentationsystem which is publicly available and as such, in contrast to previousapproaches, allows for the production of copious labels for the radar stream byincorporating four camera and two LiDAR streams. Additionally, the losses arecomputed taking into account labels to the radar sensor horizon by accumulatingLiDAR returns along a pose-chain ahead and behind of the current vehicleposition. Finally, we present the network with multi-channel radar scan inputsin order to deal with ephemeral and dynamic scene objects. ", "id2": "457", "id3": "None"}
{"id": "459", "content": "Land use and land cover mapping are essential to various fields of study,including forestry, agriculture, and urban management. Using earth observationsatellites both facilitate and accelerate the task. Lately, deep learningmethods have proven to be excellent at automating the mapping via semanticimage segmentation. However, because deep neural networks require large amountsof labeled data, it is not easy to exploit the full potential of satelliteimagery. Additionally, the land cover tends to differ in appearance from oneregion to another; therefore, having labeled data from one location does notnecessarily help in mapping others. Furthermore, satellite images come invarious multispectral bands (the bands could range from RGB to over twelvebands). In this paper, we aim at using domain adaptation to solve theaforementioned problems. We applied a well-performing domain adaptationapproach on datasets we have built using RGB images from Sentinel-2,WorldView-2, and Pleiades-1 satellites with Corine Land Cover as ground-truthlabels. We have also used the DeepGlobe land cover dataset. Experiments show asignificant improvement over results obtained without the use of domainadaptation. In some cases, an improvement of over 20% MIoU. At times it evenmanages to correct errors in the ground-truth labels. ", "id2": "458", "id3": "None"}
{"id": "460", "content": "We present BiLingUNet, a state-of-the-art model for image segmentation usingreferring expressions. BiLingUNet uses language to customize visual filters andoutperforms approaches that concatenate a linguistic representation to thevisual input. We find that using language to modulate both bottom-up andtop-down visual processing works better than just making the top-downprocessing language-conditional. We argue that common 1x1 language-conditionalfilters cannot represent relational concepts and experimentally demonstratethat wider filters work better. Our model achieves state-of-the-art performanceon four referring expression datasets. ", "id2": "459", "id3": "None"}
{"id2": 1044, "id3": "459", "content": "We present BiLingUNet, a state-of-the-art model for image segmentation usingreferring expressions. BiLingUNet uses language to customize visual filters andoutperforms approaches that concatenate a linguistic representation to thevisual input. We find that using language to modulate both bottom-up andtop-down visual processing works better than just making the top-downprocessing language-conditional. We argue that common 1x1 language-conditionalfilters cannot represent relational concepts and experimentally demonstratethat wider filters work better. Our model achieves state-of-the-art performanceon four referring expression datasets."}
{"id": "461", "content": "The perceptual-based grouping process produces a hierarchical andcompositional image representation that helps both human and machine visionsystems recognize heterogeneous visual concepts. Examples can be found in theclassical hierarchical superpixel segmentation or image parsing works. However,the grouping process is largely overlooked in modern CNN-based imagesegmentation networks due to many challenges, including the inherentincompatibility between the grid-shaped CNN feature map and theirregular-shaped perceptual grouping hierarchy. Overcoming these challenges, wepropose a deep grouping model (DGM) that tightly marries the two types ofrepresentations and defines a bottom-up and a top-down process for featureexchanging. When evaluating the model on the recent Broden+ dataset for theunified perceptual parsing task, it achieves state-of-the-art results whilehaving a small computational overhead compared to other contextual-basedsegmentation models. Furthermore, the DGM has better interpretability comparedwith modern CNN methods. ", "id2": "460", "id3": "None"}
{"id": "462", "content": "Over the past few years, state-of-the-art image segmentation algorithms arebased on deep convolutional neural networks. To render a deep network with theability to understand a concept, humans need to collect a large amount ofpixel-level annotated data to train the models, which is time-consuming andtedious. Recently, few-shot segmentation is proposed to solve this problem.Few-shot segmentation aims to learn a segmentation model that can begeneralized to novel classes with only a few training images. In this paper, wepropose a cross-reference network (CRNet) for few-shot segmentation. Unlikeprevious works which only predict the mask in the query image, our proposedmodel concurrently make predictions for both the support image and the queryimage. With a cross-reference mechanism, our network can better find theco-occurrent objects in the two images, thus helping the few-shot segmentationtask. We also develop a mask refinement module to recurrently refine theprediction of the foreground regions. For the $k$-shot learning, we propose tofinetune parts of networks to take advantage of multiple labeled supportimages. Experiments on the PASCAL VOC 2012 dataset show that our networkachieves state-of-the-art performance. ", "id2": "461", "id3": "None"}
{"id": "463", "content": "We present a generalized and scalable method, called Gen-LaneNet, to detect3D lanes from a single image. The method, inspired by the lateststate-of-the-art 3D-LaneNet, is a unified framework solving image encoding,spatial transform of features and 3D lane prediction in a single network.However, we propose unique designs for Gen-LaneNet in two folds. First, weintroduce a new geometry-guided lane anchor representation in a new coordinateframe and apply a specific geometric transformation to directly calculate real3D lane points from the network output. We demonstrate that aligning the lanepoints with the underlying top-view features in the new coordinate frame iscritical towards a generalized method in handling unfamiliar scenes. Second, wepresent a scalable two-stage framework that decouples the learning of imagesegmentation subnetwork and geometry encoding subnetwork. Compared to3D-LaneNet, the proposed Gen-LaneNet drastically reduces the amount of 3D lanelabels required to achieve a robust solution in real-world application.Moreover, we release a new synthetic dataset and its construction strategy toencourage the development and evaluation of 3D lane detection methods. Inexperiments, we conduct extensive ablation study to substantiate the proposedGen-LaneNet significantly outperforms 3D-LaneNet in average precision(AP) andF-score. ", "id2": "462", "id3": "None"}
{"id": "464", "content": "We introduce a one-shot segmentation method to alleviate the burden of manualannotation for medical images. The main idea is to treat one-shot segmentationas a classical atlas-based segmentation problem, where voxel-wisecorrespondence from the atlas to the unlabelled data is learned. Subsequently,segmentation label of the atlas can be transferred to the unlabelled data withthe learned correspondence. However, since ground truth correspondence betweenimages is usually unavailable, the learning system must be well-supervised toavoid mode collapse and convergence failure. To overcome this difficulty, weresort to the forward-backward consistency, which is widely used incorrespondence problems, and additionally learn the backward correspondencesfrom the warped atlases back to the original atlas. This cycle-correspondencelearning design enables a variety of extra, cycle-consistency-based supervisionsignals to make the training process stable, while also boost the performance.We demonstrate the superiority of our method over both deep learning-basedone-shot segmentation methods and a classical multi-atlas segmentation methodvia thorough experiments. ", "id2": "463", "id3": "None"}
{"id": "465", "content": "One usage of medical ultrasound imaging is to visualize and characterizehuman tongue shape and motion during a real-time speech to study healthy orimpaired speech production. Due to the low-contrast characteristic and noisynature of ultrasound images, it might require expertise for non-expert users torecognize tongue gestures in applications such as visual training of a secondlanguage. Moreover, quantitative analysis of tongue motion needs the tonguedorsum contour to be extracted, tracked, and visualized. Manual tongue contourextraction is a cumbersome, subjective, and error-prone task. Furthermore, itis not a feasible solution for real-time applications. The growth of deeplearning has been vigorously exploited in various computer vision tasks,including ultrasound tongue contour tracking. In the current methods, theprocess of tongue contour extraction comprises two steps of image segmentationand post-processing. This paper presents a new novel approach of automatic andreal-time tongue contour tracking using deep neural networks. In the proposedmethod, instead of the two-step procedure, landmarks of the tongue surface aretracked. This novel idea enables researchers in this filed to benefits fromavailable previously annotated databases to achieve high accuracy results. Ourexperiment disclosed the outstanding performances of the proposed technique interms of generalization, performance, and accuracy. ", "id2": "464", "id3": "None"}
{"id": "466", "content": "In the literature, many fusion techniques are registered for the segmentationof images, but they primarily focus on observed output or belief score orprobability score of the output classes. In the present work, we have utilizedinter source statistical dependency among different classifiers for ensemblingof different deep learning techniques for semantic segmentation of images. Forthis purpose, in the present work, a class-wise Copula-based ensembling methodis newly proposed for solving the multi-class segmentation problem.Experimentally, it is observed that the performance has improved more forsemantic image segmentation using the proposed class-specific Copula functionthan the traditionally used single Copula function for the problem. Theperformance is also compared with three state-of-the-art ensembling methods. ", "id2": "465", "id3": "None"}
{"id": "467", "content": "In this paper, we propose deep learning algorithms for ranking responsesurfaces, with applications to optimal stopping problems in financialmathematics. The problem of ranking response surfaces is motivated byestimating optimal feedback policy maps in stochastic control problems, aimingto efficiently find the index associated to the minimal response across theentire continuous input space $ mathcal X   subseteq  mathbb R ^d$. Byconsidering points in $ mathcal X $ as pixels and indices of the minimalsurfaces as labels, we recast the problem as an image segmentation problem,which assigns a label to every pixel in an image such that pixels with the samelabel share certain characteristics. This provides an alternative method forefficiently solving the problem instead of using sequential design in ourprevious work [R. Hu and M. Ludkovski, SIAM/ASA Journal on UncertaintyQuantification, 5 (2017), 212--239].  Deep learning algorithms are scalable, parallel and model-free, i.e., noparametric assumptions needed on the response surfaces. Considering rankingresponse surfaces as image segmentation allows one to use a broad class of deepneural networks, e.g., UNet, SegNet, DeconvNet, which have been widely appliedand numerically proved to possess high accuracy in the field. We alsosystematically study the dependence of deep learning algorithms on the inputdata generated on uniform grids or by sequential design sampling, and observethat the performance of deep learning is   it not  sensitive to the noise andlocations (close to/away from boundaries) of training data. We present a fewexamples including synthetic ones and the Bermudan option pricing problem toshow the efficiency and accuracy of this method. ", "id2": "466", "id3": "None"}
{"id": "468", "content": "Learning Enabled Components (LECs) are widely being used in a variety ofperception based autonomy tasks like image segmentation, object detection,end-to-end driving, etc. These components are trained with large image datasetswith multimodal factors like weather conditions, time-of-day, traffic-density,etc. The LECs learn from these factors during training, and while testing ifthere is variation in any of these factors, the components get confusedresulting in low confidence predictions. The images with factors not seenduring training is commonly referred to as Out-of-Distribution (OOD). For safeautonomy it is important to identify the OOD images, so that a suitablemitigation strategy can be performed. Classical one-class classifiers like SVMand SVDD are used to perform OOD detection. However, the multiple labelsattached to the images in these datasets, restricts the direct application ofthese techniques. We address this problem using the latent space of the$ beta$-Variational Autoencoder ($ beta$-VAE). We use the fact that compactlatent space generated by an appropriately selected $ beta$-VAE will encode theinformation about these factors in a few latent variables, and that can be usedfor computationally inexpensive detection. We evaluate our approach on thenuScenes dataset, and our results shows the latent space of $ beta$-VAE issensitive to encode changes in the values of the generative factor. ", "id2": "467", "id3": "None"}
{"id": "469", "content": "For proper generalization performance of convolutional neural networks (CNNs)in medical image segmentation, the learnt features should be invariant underparticular non-linear shape variations of the input. To induce invariance inCNNs to such transformations, we propose Probabilistic Augmentation of Datausing Diffeomorphic Image Transformation (PADDIT) -- a systematic framework forgenerating realistic transformations that can be used to augment data fortraining CNNs. We show that CNNs trained with PADDIT outperforms CNNs trainedwithout augmentation and with generic augmentation in segmenting white matterhyperintensities from T1 and FLAIR brain MRI scans. ", "id2": "468", "id3": "None"}
{"id": "470", "content": "Boosting is a method for learning a single accurate predictor by linearlycombining a set of less accurate weak learners. Recently, structured learninghas found many applications in computer vision. Inspired by structured supportvector machines (SSVM), here we propose a new boosting algorithm for structuredoutput prediction, which we refer to as StructBoost. StructBoost supportsnonlinear structured learning by combining a set of weak structured learners.As SSVM generalizes SVM, our StructBoost generalizes standard boostingapproaches such as AdaBoost, or LPBoost to structured learning. The resultingoptimization problem of StructBoost is more challenging than SSVM in the sensethat it may involve exponentially many variables and constraints. In contrast,for SSVM one usually has an exponential number of constraints and acutting-plane method is used. In order to efficiently solve StructBoost, weformulate an equivalent $ 1 $-slack formulation and solve it using acombination of cutting planes and column generation. We show the versatilityand usefulness of StructBoost on a range of problems such as optimizing thetree loss for hierarchical multi-class classification, optimizing the Pascaloverlap criterion for robust visual tracking and learning conditional randomfield parameters for image segmentation. ", "id2": "469", "id3": "None"}
{"id": "471", "content": "Fine-grained annotations---e.g. dense image labels, image segmentation andtext tagging---are useful in many ML applications but they are labor-intensiveto generate. Moreover there are often systematic, structured errors in thesefine-grained annotations. For example, a car might be entirely unannotated inthe image, or the boundary between a car and street might only be coarselyannotated. Standard ML training on data with such structured errors producesmodels with biases and poor performance. In this work, we propose a novelframework of Error-Correcting Networks (ECN) to address the challenge oflearning in the presence structured error in fine-grained annotations. Given alarge noisy dataset with commonly occurring structured errors, and a muchsmaller dataset with more accurate annotations, ECN is able to substantiallyimprove the prediction of fine-grained annotations compared to standardapproaches for training on noisy data. It does so by learning to leverage thestructures in the annotations and in the noisy labels. Systematic experimentson image segmentation and text tagging demonstrate the strong performance ofECN in improving training on noisy structured labels. ", "id2": "470", "id3": "None"}
{"id": "472", "content": "Usually, Neural Networks models are trained with a large dataset of images inhomogeneous backgrounds. The issue is that the performance of the networkmodels trained could be significantly degraded in a complex and heterogeneousenvironment. To mitigate the issue, this paper develops a framework thatpermits to autonomously generate a training dataset in heterogeneous clutteredbackgrounds. It is clear that the learning effectiveness of the proposedframework should be improved in complex and heterogeneous environments,compared with the ones with the typical dataset. In our framework, astate-of-the-art image segmentation technique called DeepLab is used to extractobjects of interest from a picture and Chroma-key technique is then used tomerge the extracted objects of interest into specific heterogeneousbackgrounds. The performance of the proposed framework is investigated throughempirical tests and compared with that of the model trained with the COCOdataset. The results show that the proposed framework outperforms the modelcompared. This implies that the learning effectiveness of the frameworkdeveloped is superior to the models with the typical dataset. ", "id2": "471", "id3": "None"}
{"id": "473", "content": "Convolutional neural networks have become state-of-the-art in a wide range ofimage recognition tasks. The interpretation of their predictions, however, isan active area of research. Whereas various interpretation methods have beensuggested for image classification, the interpretation of image segmentationstill remains largely unexplored. To that end, we propose SEG-GRAD-CAM, agradient-based method for interpreting semantic segmentation. Our method is anextension of the widely-used Grad-CAM method, applied locally to produceheatmaps showing the relevance of individual pixels for semantic segmentation. ", "id2": "472", "id3": "None"}
{"id": "474", "content": "Building a large image dataset with high-quality object masks for semanticsegmentation is costly and time consuming. In this paper, we introduce aprincipled semi-supervised framework that only uses a small set of fullysupervised images (having semantic segmentation labels and box labels) and aset of images with only object bounding box labels (we call it the weak set).Our framework trains the primary segmentation model with the aid of anancillary model that generates initial segmentation labels for the weak set anda self-correction module that improves the generated labels during trainingusing the increasingly accurate primary model. We introduce two variants of theself-correction module using either linear or convolutional functions.Experiments on the PASCAL VOC 2012 and Cityscape datasets show that our modelstrained with a small fully supervised set perform similar to, or better than,models trained with a large fully supervised set while requiring ~7x lessannotation effort. ", "id2": "473", "id3": "None"}
{"id": "475", "content": "Until now, all single level segmentation algorithms except CNN-based oneslead to over segmentation. And CNN-based segmentation algorithms have their ownproblems. To avoid over segmentation, multiple thresholds of criteria areadopted in region merging process to produce hierarchical segmentation results.However, there still has extreme over segmentation in the low level of thehierarchy, and outstanding tiny objects are merged to their large adjacenciesin the high level of the hierarchy. This paper proposes a region-merging-basedimage segmentation method that we call it Dam Burst. As a single levelsegmentation algorithm, this method avoids over segmentation and retainsdetails by the same time. It is named because of that it simulates a floodingfrom underground destroys dams between water-pools. We treat edge detectionresults as strengthening structure of a dam if it is on the dam. To simulate aflooding from underground, regions are merged by ascending order of the averagegra-dient inside the region. ", "id2": "474", "id3": "None"}
{"id": "476", "content": "Convolutional neural networks (CNNs) have been successfully applied tomedical image classification, segmentation, and related tasks. Among the manyCNNs architectures, U-Net and its improved versions based are widely used andachieve state-of-the-art performance these years. These improved architecturesfocus on structural improvements and the size of the convolution kernel isgenerally fixed. In this paper, we propose a module that combines the benefitsof multiple kernel sizes and we apply the proposed module to U-Net and itsvariants. We test our module on three segmentation benchmark datasets andexperimental results show significant improvement. ", "id2": "475", "id3": "None"}
{"id": "477", "content": "Image segmentation is a fundamental research topic in image processing andcomputer vision. In the last decades, researchers developed a large number ofsegmentation algorithms for various applications. Amongst these algorithms, theNormalized cut (Ncut) segmentation method is widely applied due to its goodperformance. The Ncut segmentation model is an optimization problem whoseenergy is defined on a specifically designed graph. Thus, the segmentationresults of the existing Ncut method are largely dependent on a pre-constructedsimilarity measure on the graph since this measure is usually given empiricallyby users. This flaw will lead to some undesirable segmentation results. In thispaper, we propose a Ncut-based segmentation algorithm by integrating anadaptive similarity measure and spatial regularization. The proposed modelcombines the Parzen-Rosenblatt window method, non-local weights entropy, Ncutenergy, and regularizer of phase field in a variational framework. Our methodcan adaptively update the similarity measure function by estimating someparameters. This adaptive procedure enables the proposed algorithm finding abetter similarity measure for classification than the Ncut method. We providesome mathematical interpretation of the proposed adaptive similarity frommulti-viewpoints such as statistics and convex optimization. In addition, theregularizer of phase field can guarantee that the proposed algorithm has arobust performance in the presence of noise, and it can also rectify thesimilarity measure with a spatial priori. The well-posed theory such as theexistence of the minimizer for the proposed model is given in the paper.Compared with some existing segmentation methods such as the traditionalNcut-based model and the classical Chan-Vese model, the numerical experimentsshow that our method can provide promising segmentation results. ", "id2": "476", "id3": "None"}
{"id": "478", "content": "While making a tremendous impact in various fields, deep neural networksusually require large amounts of labeled data for training which are expensiveto collect in many applications, especially in the medical domain. Unlabeleddata, on the other hand, is much more abundant. Semi-supervised learningtechniques, such as co-training, could provide a powerful tool to leverageunlabeled data. In this paper, we propose a novel framework, uncertainty-awaremulti-view co-training (UMCT), to address semi-supervised learning on 3D data,such as volumetric data from medical imaging. In our work, co-training isachieved by exploiting multi-viewpoint consistency of 3D data. We generatedifferent views by rotating or permuting the 3D data and utilize asymmetrical3D kernels to encourage diversified features in different sub-networks. Inaddition, we propose an uncertainty-weighted label fusion mechanism to estimatethe reliability of each views prediction with Bayesian deep learning. As oneview requires the supervision from other views in co-training, ourself-adaptive approach computes a confidence score for the prediction of eachunlabeled sample in order to assign a reliable pseudo label. Thus, our approachcan take advantage of unlabeled data during training. We show the effectivenessof our proposed semi-supervised method on several public datasets from medicalimage segmentation tasks (NIH pancreas & LiTS liver tumor dataset). Meanwhile,a fully-supervised method based on our approach achieved state-of-the-artperformances on both the LiTS liver tumor segmentation and the MedicalSegmentation Decathlon (MSD) challenge, demonstrating the robustness and valueof our framework, even when fully supervised training is feasible. ", "id2": "477", "id3": "None"}
{"id": "479", "content": "Accurate medical image segmentation commonly requires effective learning ofthe complementary information from multimodal data. However, in clinicalpractice, we often encounter the problem of missing imaging modalities. Wetackle this challenge and propose a novel multimodal segmentation frameworkwhich is robust to the absence of imaging modalities. Our network uses featuredisentanglement to decompose the input modalities into the modality-specificappearance code, which uniquely sticks to each modality, and themodality-invariant content code, which absorbs multimodal information for thesegmentation task. With enhanced modality-invariance, the disentangled contentcode from each modality is fused into a shared representation which gainsrobustness to missing data. The fusion is achieved via a learning-basedstrategy to gate the contribution of different modalities at differentlocations. We validate our method on the important yet challenging multimodalbrain tumor segmentation task with the BRATS challenge dataset. Withcompetitive performance to the state-of-the-art approaches for full modality,our method achieves outstanding robustness under various missing modality(ies)situations, significantly exceeding the state-of-the-art method by over 16% inaverage for Dice on whole tumor segmentation. ", "id2": "478", "id3": "None"}
{"id": "480", "content": "Despite recent progress on semantic segmentation, there still exist hugechallenges in medical ultra-resolution image segmentation. The methods based onmulti-branch structure can make a good balance between computational burdensand segmentation accuracy. However, the fusion structure in these methodsrequire to be designed elaborately to achieve desirable result, which leads tomodel redundancy. In this paper, we propose Meta Segmentation Network (MSN) tosolve this challenging problem. With the help of meta-learning, the fusionmodule of MSN is quite simple but effective. MSN can fast generate the weightsof fusion layers through a simple meta-learner, requiring only a few trainingsamples and epochs to converge. In addition, to avoid learning all branchesfrom scratch, we further introduce a particular weight sharing mechanism torealize a fast knowledge adaptation and share the weights among multiplebranches, resulting in the performance improvement and significant parametersreduction. The experimental results on two challenging ultra-resolution medicaldatasets BACH and ISIC show that MSN achieves the best performance comparedwith the state-of-the-art methods. ", "id2": "479", "id3": "None"}
{"id": "481", "content": "Deep learning has shown its great promise in various biomedical imagesegmentation tasks. Existing models are typically based on U-Net and rely on anencoder-decoder architecture with stacked local operators to aggregatelong-range information gradually. However, only using the local operatorslimits the efficiency and effectiveness. In this work, we propose the non-localU-Nets, which are equipped with flexible global aggregation blocks, forbiomedical image segmentation. These blocks can be inserted into U-Net assize-preserving processes, as well as down-sampling and up-sampling layers. Weperform thorough experiments on the 3D multimodality isointense infant brain MRimage segmentation task to evaluate the non-local U-Nets. Results show that ourproposed models achieve top performances with fewer parameters and fastercomputation. ", "id2": "480", "id3": "None"}
{"id2": 1045, "id3": "480", "content": "Deep learning has shown its great promise in various biomedical imagesegmentation tasks. Existing models are typically based on U-Net and rely on anencoder-decoder architecture with stacked local operators to aggregatelong-range information gradually. However, only using the local operatorslimits the efficiency and effectiveness. In this work, we propose the non-localU-Nets, which are equipped with flexible global aggregation blocks, forbiomedical image segmentation. These blocks can be inserted into U-Net assize-preserving processes, as well as down-sampling and up-sampling layers. Weperform thorough experiments on the 3D multimodality isointense infant brain MRimage segmentation task to evaluate the non-local U-Nets. Results show that ourproposed models achieve top performances with fewer parameters and fastercomputation."}
{"id": "482", "content": "Arbitrary style transfer is the task of synthesis of an image that has neverbeen seen before, using two given images: content image and style image. Thecontent image forms the structure, the basic geometric lines and shapes of theresulting image, while the style image sets the color and texture of theresult. The word arbitrary in this context means the absence of any onepre-learned style. So, for example, convolutional neural networks capable oftransferring a new style only after training or retraining on a new amount ofdata are not con-sidered to solve such a problem, while networks based on theattention mech-anism that are capable of performing such a transformationwithout retraining - yes. An original image can be, for example, a photograph,and a style image can be a painting of a famous artist. The resulting image inthis case will be the scene depicted in the original photograph, made in thestylie of this picture. Recent arbitrary style transfer algorithms make itpossible to achieve good re-sults in this task, however, in processing portraitimages of people, the result of such algorithms is either unacceptable due toexcessive distortion of facial features, or weakly expressed, not bearing thecharacteristic features of a style image. In this paper, we consider anapproach to solving this problem using the combined architecture of deep neuralnetworks with a attention mechanism that transfers style based on the contentsof a particular image segment: with a clear predominance of style over the formfor the background part of the im-age, and with the prevalence of content overthe form in the image part con-taining directly the image of a person. ", "id2": "481", "id3": "None"}
{"id": "483", "content": "Complex classification performance metrics such as the F$  _ beta$-measureand Jaccard index are often used, in order to handle class-imbalanced casessuch as information retrieval and image segmentation. These performance metricsare not decomposable, that is, they cannot be expressed in a per-examplemanner, which hinders a straightforward application of M-estimation widely usedin supervised learning. In this paper, we consider linear-fractional metrics,which are a family of classification performance metrics that encompasses manystandard ones such as the F$  _ beta$-measure and Jaccard index, and proposemethods to directly maximize performances under those metrics. A clue to tackletheir direct optimization is a calibrated surrogate utility, which is atractable lower bound of the true utility function representing a given metric.We characterize sufficient conditions which make the surrogate maximizationcoincide with the maximization of the true utility. Simulation results onbenchmark datasets validate the effectiveness of our calibrated surrogatemaximization especially if the sample sizes are extremely small. ", "id2": "482", "id3": "None"}
{"id": "484", "content": "We present a new method for efficient high-quality image segmentation ofobjects and scenes. By analogizing classical computer graphics methods forefficient rendering with over- and undersampling challenges faced in pixellabeling tasks, we develop a unique perspective of image segmentation as arendering problem. From this vantage, we present the PointRend (Point-basedRendering) neural network module: a module that performs point-basedsegmentation predictions at adaptively selected locations based on an iterativesubdivision algorithm. PointRend can be flexibly applied to both instance andsemantic segmentation tasks by building on top of existing state-of-the-artmodels. While many concrete implementations of the general idea are possible,we show that a simple design already achieves excellent results. Qualitatively,PointRend outputs crisp object boundaries in regions that are over-smoothed byprevious methods. Quantitatively, PointRend yields significant gains on COCOand Cityscapes, for both instance and semantic segmentation. PointRendsefficiency enables output resolutions that are otherwise impractical in termsof memory or computation compared to existing approaches. Code has been madeavailable athttps://github.com/facebookresearch/detectron2/tree/master/projects/PointRend. ", "id2": "483", "id3": "None"}
{"id": "485", "content": "Learning-based approaches for semantic segmentation have two inherentchallenges. First, acquiring pixel-wise labels is expensive and time-consuming.Second, realistic segmentation datasets are highly unbalanced: some categoriesare much more abundant than others, biasing the performance to the mostrepresented ones. In this paper, we are interested in focusing human labellingeffort on a small subset of a larger pool of data, minimizing this effort whilemaximizing performance of a segmentation model on a hold-out set. We present anew active learning strategy for semantic segmentation based on deepreinforcement learning (RL). An agent learns a policy to select a subset ofsmall informative image regions -- opposed to entire images -- to be labeled,from a pool of unlabeled data. The region selection decision is made based onpredictions and uncertainties of the segmentation model being trained. Ourmethod proposes a new modification of the deep Q-network (DQN) formulation foractive learning, adapting it to the large-scale nature of semantic segmentationproblems. We test the proof of concept in CamVid and provide results in thelarge-scale dataset Cityscapes. On Cityscapes, our deep RL region-based DQNapproach requires roughly 30% less additional labeled data than our mostcompetitive baseline to reach the same performance. Moreover, we find that ourmethod asks for more labels of under-represented categories compared to thebaselines, improving their performance and helping to mitigate class imbalance. ", "id2": "484", "id3": "None"}
{"id": "486", "content": "Even though convolutional neural networks (CNNs) are driving progress inmedical image segmentation, standard models still have some drawbacks. First,the use of multi-scale approaches, i.e., encoder-decoder architectures, leadsto a redundant use of information, where similar low-level features areextracted multiple times at multiple scales. Second, long-range featuredependencies are not efficiently modeled, resulting in non-optimaldiscriminative feature representations associated with each semantic class. Inthis paper we attempt to overcome these limitations with the proposedarchitecture, by capturing richer contextual dependencies based on the use ofguided self-attention mechanisms. This approach is able to integrate localfeatures with their corresponding global dependencies, as well as highlightinterdependent channel maps in an adaptive manner. Further, the additional lossbetween different modules guides the attention mechanisms to neglect irrelevantinformation and focus on more discriminant regions of the image by emphasizingrelevant feature associations. We evaluate the proposed model in the context ofsemantic segmentation on three different datasets: abdominal organs,cardiovascular structures and brain tumors. A series of ablation experimentssupport the importance of these attention modules in the proposed architecture.In addition, compared to other state-of-the-art segmentation networks our modelyields better segmentation performance, increasing the accuracy of thepredictions while reducing the standard deviation. This demonstrates theefficiency of our approach to generate precise and reliable automaticsegmentations of medical images. Our code is made publicly available athttps://github.com/sinAshish/Multi-Scale-Attention ", "id2": "485", "id3": "None"}
{"id": "487", "content": "Particle competition and cooperation (PCC) is a graph-based semi-supervisedlearning approach. When PCC is applied to interactive image segmentation tasks,pixels are converted into network nodes, and each node is connected to itsk-nearest neighbors, according to the distance between a set of featuresextracted from the image. Building a proper network to feed PCC is crucial toachieve good segmentation results. However, some features may be more importantthan others to identify the segments, depending on the characteristics of theimage to be segmented. In this paper, an index to evaluate candidate networksis proposed. Thus, building the network becomes a problem of optimizing somefeature weights based on the proposed index. Computer simulations are performedon some real-world images from the Microsoft GrabCut database, and thesegmentation results related in this paper show the effectiveness of theproposed method. ", "id2": "486", "id3": "None"}
{"id": "488", "content": "Instead of directly utilizing an observed image including some outliers,noise or intensity inhomogeneity, the use of its ideal value (e.g. noise-freeimage) has a favorable impact on clustering. Hence, the accurate estimation ofthe residual (e.g. unknown noise) between the observed image and its idealvalue is an important task. To do so, we propose an $ ell_0$regularization-based Fuzzy $C$-Means (FCM) algorithm incorporating amorphological reconstruction operation and a tight wavelet frame transform. Toachieve a sound trade-off between detail preservation and noise suppression,morphological reconstruction is used to filter an observed image. By combiningthe observed and filtered images, a weighted sum image is generated. Since atight wavelet frame system has sparse representations of an image, it isemployed to decompose the weighted sum image, thus forming its correspondingfeature set. Taking it as data for clustering, we present an improved FCMalgorithm by imposing an $ ell_0$ regularization term on the residual betweenthe feature set and its ideal value, which implies that the favorableestimation of the residual is obtained and the ideal value participates inclustering. Spatial information is also introduced into clustering since it isnaturally encountered in image segmentation. Furthermore, it makes theestimation of the residual more reliable. To further enhance the segmentationeffects of the improved FCM algorithm, we also employ the morphologicalreconstruction to smoothen the labels generated by clustering. Finally, basedon the prototypes and smoothed labels, the segmented image is reconstructed byusing a tight wavelet frame reconstruction operation. Experimental resultsreported for synthetic, medical, and color images show that the proposedalgorithm is effective and efficient, and outperforms other algorithms. ", "id2": "487", "id3": "None"}
{"id": "489", "content": "Accurate image segmentation of the liver is a challenging problem owing toits large shape variability and unclear boundaries. Although the applicationsof fully convolutional neural networks (CNNs) have shown groundbreakingresults, limited studies have focused on the performance of generalization. Inthis study, we introduce a CNN for liver segmentation on abdominal computedtomography (CT) images that shows high generalization performance and accuracy.To improve the generalization performance, we initially propose an auto-contextalgorithm in a single CNN. The proposed auto-context neural network exploits aneffective high-level residual estimation to obtain the shape prior. Identicaldual paths are effectively trained to represent mutual complementary featuresfor an accurate posterior analysis of a liver. Further, we extend our networkby employing a self-supervised contour scheme. We trained sparse contourfeatures by penalizing the ground-truth contour to focus more contourattentions on the failures. The experimental results show that the proposednetwork results in better accuracy when compared to the state-of-the-artnetworks by reducing 10.31% of the Hausdorff distance. We used 180 abdominal CTimages for training and validation. Two-fold cross-validation is presented fora comparison with the state-of-the-art neural networks. Novel multiple N-foldcross-validations are conducted to verify the performance of generalization.The proposed network showed the best generalization performance among thenetworks. Additionally, we present a series of ablation experiments thatcomprehensively support the importance of the underlying concepts. ", "id2": "488", "id3": "None"}
{"id2": 1046, "id3": "488", "content": "Accurate image segmentation of the liver is a challenging problem owing toits large shape variability and unclear boundaries. Although the applicationsof fully convolutional neural networks (CNNs) have shown groundbreakingresults, limited studies have focused on the performance of generalization. Inthis study, we introduce a CNN for liver segmentation on abdominal computedtomography (CT) images that shows high generalization performance and accuracy.To improve the generalization performance, we initially propose an auto-contextalgorithm in a single CNN. The proposed auto-context neural network exploits aneffective high-level residual estimation to obtain the shape prior. Identicaldual paths are effectively trained to represent mutual complementary featuresfor an accurate posterior analysis of a liver. Further, we extend our networkby employing a self-supervised contour scheme. We trained sparse contourfeatures by penalizing the ground-truth contour to focus more contourattentions on the failures. The experimental results show that the proposednetwork results in better accuracy when compared to the state-of-the-artnetworks by reducing 10.31% of the Hausdorff distance. We used 180 abdominal CTimages for training and validation. Two-fold cross-validation is presented fora comparison with the state-of-the-art neural networks. Novel multiple N-foldcross-validations are conducted to verify the performance of generalization.The proposed network showed the best generalization performance among thenetworks. Additionally, we present a series of ablation experiments thatcomprehensively support the importance of the underlying concepts."}
{"id": "490", "content": "Measuring similarity between two objects is the core operation in existingcluster analyses in grouping similar objects into clusters. Cluster analyseshave been applied to a number of applications, including image segmentation,social network analysis, and computational biology. This paper introduces a newsimilarity measure called point-set kernel which computes the similaritybetween an object and a sample of objects generated from an unknowndistribution. The proposed clustering procedure utilizes this new measure tocharacterize both the typical point of every cluster and the cluster grown fromthe typical point. We show that the new clustering procedure is both effectiveand efficient such that it can deal with large scale datasets. In contrast,existing clustering algorithms are either efficient or effective; and evenefficient ones have difficulty dealing with large scale datasets withoutspecial hardware. We show that the proposed algorithm is more effective andruns orders of magnitude faster than the state-of-the-art density-peakclustering and scalable kernel k-means clustering when applying to datasets ofmillions of data points, on commonly used computing machines. ", "id2": "489", "id3": "None"}
{"id": "491", "content": "Many interactive image segmentation techniques are based on semi-supervisedlearning. The user may label some pixels from each object and the SSL algorithmwill propagate the labels from the labeled to the unlabeled pixels, findingobject boundaries. This paper proposes a new SSL graph-based interactive imagesegmentation approach, using undirected and unweighted kNN graphs, from whichthe unlabeled nodes receive contributions from other nodes (either labeled orunlabeled). It is simpler than many other techniques, but it still achievessignificant classification accuracy in the image segmentation task. Computersimulations are performed using some real-world images, extracted from theMicrosoft GrabCut dataset. The segmentation results show the effectiveness ofthe proposed approach. ", "id2": "490", "id3": "None"}
{"id": "492", "content": "In this thesis, we present new schemes which leverage a constrainedclustering method to solve several computer vision tasks ranging from imageretrieval, image segmentation and co-segmentation, to person re-identification.In the last decades clustering methods have played a vital role in computervision applications; herein, we focus on the extension, reformulation, andintegration of a well-known graph and game theoretic clustering method known asDominant Sets. Thus, we have demonstrated the validity of the proposed methodswith extensive experiments which are conducted on several benchmark datasets. ", "id2": "491", "id3": "None"}
{"id": "493", "content": "We use Deep Convolutional Neural Networks (DCNNs) for image segmentationproblems. DCNNs can well extract the features from natural images. However, theclassification functions in the existing network architecture of CNNs aresimple and lack capabilities to handle important spatial information in a waythat have been done for many well-known traditional variational models. Priorsuch as spatial regularity, volume prior and object shapes cannot be wellhandled by existing DCNNs. We propose a novel Soft Threshold Dynamics (STD)framework which can easily integrate many spatial priors of the classicalvariational models into the DCNNs for image segmentation. The novelty of ourmethod is to interpret the softmax activation function as a dual variable in avariational problem, and thus many spatial priors can be imposed in the dualspace. From this viewpoint, we can build a STD based framework which can enablethe outputs of DCNNs to have many special priors such as spatial regularity,volume constraints and star-shape priori. The proposed method is a generalmathematical framework and it can be applied to any semantic segmentationDCNNs. To show the efficiency and accuracy of our method, we applied it to thepopular DeepLabV3+ image segmentation network, and the experiments results showthat our method can work efficiently on data-driven image segmentation DCNNs. ", "id2": "492", "id3": "None"}
{"id": "494", "content": "Class imbalance has emerged as one of the major challenges for medical imagesegmentation. The model cascade (MC) strategy significantly alleviates theclass imbalance issue via running a set of individual deep models forcoarse-to-fine segmentation. Despite its outstanding performance, however, thismethod leads to undesired system complexity and also ignores the correlationamong the models. To handle these flaws, we propose a light-weight deep model,i.e., the One-pass Multi-task Network (OM-Net) to solve class imbalance betterthan MC does, while requiring only one-pass computation. First, OM-Netintegrates the separate segmentation tasks into one deep model, which consistsof shared parameters to learn joint features, as well as task-specificparameters to learn discriminative features. Second, to more effectivelyoptimize OM-Net, we take advantage of the correlation among tasks to designboth an online training data transfer strategy and a curriculum learning-basedtraining strategy. Third, we further propose sharing prediction results betweentasks and design a cross-task guided attention (CGA) module which canadaptively recalibrate channel-wise feature responses based on thecategory-specific statistics. Finally, a simple yet effective post-processingmethod is introduced to refine the segmentation results. Extensive experimentsare conducted to demonstrate the effectiveness of the proposed techniques. Mostimpressively, we achieve state-of-the-art performance on the BraTS 2015 testingset and BraTS 2017 online validation set. Using these proposed approaches, wealso won joint third place in the BraTS 2018 challenge among 64 participatingteams. The code is publicly available athttps://github.com/chenhong-zhou/OM-Net. ", "id2": "493", "id3": "None"}
{"id": "495", "content": "This paper presents an extension proposal of the semi-supervised learningmethod known as Particle Competition and Cooperation for carrying out tasks ofimage segmentation. Preliminary results show that this is a promising approach.  Este artigo apresenta uma proposta de extens ~ao do modelo de aprendizadosemi-supervisionado conhecido como Competi c c  ~ao e Coopera c c  ~ao entrePart iculas para a realiza c c  ~ao de tarefas de segmenta c c  ~ao deimagens. Resultados preliminares mostram que esta  e uma abordagem promissora. ", "id2": "494", "id3": "None"}
{"id2": 1047, "id3": "494", "content": "This paper presents an extension proposal of the semi-supervised learningmethod known as Particle Competition and Cooperation for carrying out tasks ofimage segmentation. Preliminary results show that this is a promising approach. Este artigo apresenta uma proposta de extens ~ao do modelo de aprendizadosemi-supervisionado conhecido como Competi c c ~ao e Coopera c c ~ao entrePart iculas para a realiza c c ~ao de tarefas de segmenta c c ~ao deimagens. Resultados preliminares mostram que esta e uma abordagem promissora."}
{"id": "496", "content": "Unsupervised segmentation of large images using a Potts model Hamiltonian isunique in that segmentation is governed by a resolution parameter which scalesthe sensitivity to small clusters. Here, the input image is first modeled as agraph, which is then segmented by minimizing a Hamiltonian cost functiondefined on the graph and the respective segments. However, there exists noclosed form solution of this optimization, and using previous iterativealgorithmic solution techniques, the problem scales quadratically in the InputLength. Therefore, while Potts model segmentation gives accurate segmentation,it is grossly underutilized as an unsupervised learning technique. We propose afast statistical down-sampling of input image pixels based on the respectivecolor features, and a new iterative method to minimize the Potts model energyconsidering pixel to segment relationship. This method is generalizable and canbe extended for image pixel texture features as well as spatial features. Wedemonstrate that this new method is highly efficient, and outperforms existingmethods for Potts model based image segmentation. We demonstrate theapplication of our method in medical microscopy image segmentation;particularly, in segmenting renal glomerular micro-environment in renalpathology. Our method is not limited to image segmentation, and can be extendedto any image/data segmentation/clustering task for arbitrary datasets withdiscrete features. ", "id2": "495", "id3": "None"}
{"id": "497", "content": "We present a method for segmenting neuron membranes in 2D electron microscopyimagery. This segmentation task has been a bottleneck to reconstruction effortsof the brains synaptic circuits. One common problem is the misclassificationof blurry membrane fragments as cell interior, which leads to merging of twoadjacent neuron sections into one via the blurry membrane region. Humanannotators can easily avoid such errors by implicitly performing gapcompletion, taking into account the continuity of membranes.  Drawing inspiration from these human strategies, we formulate thesegmentation task as an edge labeling problem on a graph with local topologicalconstraints. We derive an integer linear program (ILP) that enforces membranecontinuity, i.e. the absence of gaps. The cost function of the ILP is thepixel-wise deviation of the segmentation from a priori membrane probabilitiesderived from the data.  Based on membrane probability maps obtained using random forest classifiersand convolutional neural networks, our method improves the neuron boundarysegmentation accuracy compared to a variety of standard segmentationapproaches. Our method successfully performs gap completion and leads to fewertopological errors. The method could potentially also be incorporated intoother image segmentation pipelines with known topological constraints. ", "id2": "496", "id3": "None"}
{"id": "498", "content": "Image normalization is a critical step in medical imaging. This step is oftendone on a per-dataset basis, preventing current segmentation algorithms fromthe full potential of exploiting jointly normalized information across multipledatasets. To solve this problem, we propose an adversarial normalizationapproach for image segmentation which learns common normalizing functionsacross multiple datasets while retaining image realism. The adversarialtraining provides an optimal normalizer that improves both the segmentationaccuracy and the discrimination of unrealistic normalizing functions. Ourcontribution therefore leverages common imaging information from multipledomains. The optimality of our common normalizer is evaluated by combiningbrain images from both infants and adults. Results on the challenging iSEG andMRBrainS datasets reveal the potential of our adversarial normalizationapproach for segmentation, with Dice improvements of up to 59.6% over thebaseline. ", "id2": "497", "id3": "None"}
{"id2": 1048, "id3": "497", "content": "Image normalization is a critical step in medical imaging. This step is oftendone on a per-dataset basis, preventing current segmentation algorithms fromthe full potential of exploiting jointly normalized information across multipledatasets. To solve this problem, we propose an adversarial normalizationapproach for image segmentation which learns common normalizing functionsacross multiple datasets while retaining image realism. The adversarialtraining provides an optimal normalizer that improves both the segmentationaccuracy and the discrimination of unrealistic normalizing functions. Ourcontribution therefore leverages common imaging information from multipledomains. The optimality of our common normalizer is evaluated by combiningbrain images from both infants and adults. Results on the challenging iSEG andMRBrainS datasets reveal the potential of our adversarial normalizationapproach for segmentation, with Dice improvements of up to 59.6% over thebaseline."}
{"id": "499", "content": "We consider referring image segmentation. It is a problem at the intersectionof computer vision and natural language understanding. Given an input image anda referring expression in the form of a natural language sentence, the goal isto segment the object of interest in the image referred by the linguisticquery. To this end, we propose a dual convolutional LSTM (ConvLSTM) network totackle this problem. Our model consists of an encoder network and a decodernetwork, where ConvLSTM is used in both encoder and decoder networks to capturespatial and sequential information. The encoder network extracts visual andlinguistic features for each word in the expression sentence, and adopts anattention mechanism to focus on words that are more informative in themultimodal interaction. The decoder network integrates the features generatedby the encoder network at multiple levels as its input and produces the finalprecise segmentation mask. Experimental results on four challenging datasetsdemonstrate that the proposed network achieves superior segmentationperformance compared with other state-of-the-art methods. ", "id2": "498", "id3": "None"}
{"id": "500", "content": "Generative adversarial networks (GANs) have shown great success inapplications such as image generation and inpainting. However, they typicallyrequire large datasets, which are often not available, especially in thecontext of prediction tasks such as image segmentation that require labels.Therefore, methods such as the CycleGAN use more easily available unlabelleddata, but do not offer a way to leverage additional labelled data for improvedperformance. To address this shortcoming, we show how to factorise the jointdata distribution into a set of lower-dimensional distributions along withtheir dependencies. This allows splitting the discriminator in a GAN intomultiple sub-discriminators that can be independently trained from incompleteobservations. Their outputs can be combined to estimate the density ratiobetween the joint real and the generator distribution, which enables traininggenerators as in the original GAN framework. We apply our method to imagegeneration, image segmentation and audio source separation, and obtain improvedperformance over a standard GAN when additional incomplete training examplesare available. For the Cityscapes segmentation task in particular, our methodalso improves accuracy by an absolute 14.9% over CycleGAN while using only 25additional paired examples. ", "id2": "499", "id3": "None"}
{"id": "501", "content": "Robust Optimization is becoming increasingly important in machine learningapplications. This paper studies the problem of robust submodular minimizationsubject to combinatorial constraints. Constrained Submodular Minimizationarises in several applications such as co-operative cuts in image segmentation,co-operative matchings in image correspondence, etc. Many of these models aredefined over clusterings of data points (for example pixels in images), and itis important for these models to be robust to perturbations and uncertainty inthe data. While several existing papers have studied robust submodularmaximization, ours is the first work to study the minimization version under abroad range of combinatorial constraints including cardinality, knapsack,matroid as well as graph-based constraints such as cuts, paths, matchings, andtrees. In each case, we provide scalable approximation algorithms and alsostudy hardness bounds. Finally, we empirically demonstrate the utility of ouralgorithms on synthetic and real-world datasets. ", "id2": "500", "id3": "None"}
{"id": "502", "content": "There has recently been great progress in automatic segmentation of medicalimages with deep learning algorithms. In most works observer variation isacknowledged to be a problem as it makes training data heterogeneous but so farno attempts have been made to explicitly capture this variation. Here, wepropose an approach capable of mimicking different styles of segmentation,which potentially can improve quality and clinical acceptance of automaticsegmentation methods. In this work, instead of training one neural network onall available data, we train several neural networks on subgroups of databelonging to different segmentation variations separately. Because a priori itmay be unclear what styles of segmentation exist in the data and becausedifferent styles do not necessarily map one-on-one to different observers, thesubgroups should be automatically determined. We achieve this by searching forthe best data partition with a genetic algorithm. Therefore, each network canlearn a specific style of segmentation from grouped training data. We provideproof of principle results for open-sourced prostate segmentation MRI data withsimulated observer variations. Our approach provides an improvement of up to23% (depending on simulated variations) in terms of Dice and surface Dicecoefficients compared to one network trained on all data. ", "id2": "501", "id3": "None"}
{"id": "503", "content": "Image segmentation with a volume constraint is an important prior for manyreal applications. In this work, we present a novel volume preserving imagesegmentation algorithm, which is based on the framework of entropic regularizedoptimal transport theory. The classical Total Variation (TV) regularizer andvolume preserving are integrated into a regularized optimal transport model,and the volume and classification constraints can be regarded as two measurespreserving constraints in the optimal transport problem. By studying the dualproblem, we develop a simple and efficient dual algorithm for our model.Moreover, to be different from many variational based image segmentationalgorithms, the proposed algorithm can be directly unrolled to a new VolumePreserving and TV regularized softmax (VPTV-softmax) layer for semanticsegmentation in the popular Deep Convolution Neural Network (DCNN). Theexperiment results show that our proposed model is very competitive and canimprove the performance of many semantic segmentation nets such as the popularU-net. ", "id2": "502", "id3": "None"}
{"id": "504", "content": "This fourth and last tome is focusing on describing the envisioned works fora project that has been presented in the preceding tome. It is about a newapproach dedicated to the coding of still and moving pictures, trying to bridgethe MPEG-4 and MPEG-7 standard bodies. The aim of this project is to define theprinciples of self-descriptive video coding. In order to establish them, thedocument is composed in five chapters that describe the various envisionedtechniques for developing such a new approach in visual coding: - imagesegmentation, - computation of visual descriptors, - computation of perceptualgroupings, - building of visual dictionaries, - picture and video coding. Basedon the techniques of multiresolution computing, it is proposed to develop animage segmentation made from piecewise regular components, to computeattributes on the frame and the rendering of so produced shapes, independentlyto the geometric transforms that can occur in the image plane, and to gatherthem into perceptual groupings so as to be able in performing recognition ofpartially hidden patterns. Due to vector quantization of shapes frame andrendering, it will appear that simple shapes may be compared to a visualalphabet and that complex shapes then become words written using this alphabetand be recorded into a dictionary. With the help of a nearest neighbourscanning applied on the picture shapes, the self-descriptive coding will thengenerate a sentence made from words written using the simple shape alphabet. ", "id2": "503", "id3": "None"}
{"id": "505", "content": "We tackle biomedical image segmentation in the scenario of only a few labeledbrain MR images. This is an important and challenging task in medicalapplications, where manual annotations are time-consuming. Current multi-atlasbased segmentation methods use image registration to warp segments from labeledimages onto a new scan. In a different paradigm, supervised learning-basedsegmentation strategies have gained popularity. These method consistently userelatively large sets of labeled training data, and their behavior in theregime of a few labeled biomedical images has not been thoroughly evaluated. Inthis work, we provide two important results for segmentation in the scenariowhere few labeled images are available. First, we propose a straightforwardimplementation of efficient semi-supervised learning-based registration method,which we showcase in a multi-atlas segmentation framework. Second, through anextensive empirical study, we evaluate the performance of a supervisedsegmentation approach, where the training images are augmented via randomdeformations. Surprisingly, we find that in both paradigms, accuratesegmentation is generally possible even in the context of few labeled images. ", "id2": "504", "id3": "None"}
{"id": "506", "content": "Multi-modal learning is typically performed with network architecturescontaining modality-specific layers and shared layers, utilizing co-registeredimages of different modalities. We propose a novel learning scheme for unpairedcross-modality image segmentation, with a highly compact architecture achievingsuperior segmentation accuracy. In our method, we heavily reuse networkparameters, by sharing all convolutional kernels across CT and MRI, and onlyemploy modality-specific internal normalization layers which compute respectivestatistics. To effectively train such a highly compact model, we introduce anovel loss term inspired by knowledge distillation, by explicitly constrainingthe KL-divergence of our derived prediction distributions between modalities.We have extensively validated our approach on two multi-class segmentationproblems: i) cardiac structure segmentation, and ii) abdominal organsegmentation. Different network settings, i.e., 2D dilated network and 3DU-net, are utilized to investigate our methods general efficacy. Experimentalresults on both tasks demonstrate that our novel multi-modal learning schemeconsistently outperforms single-modal training and previous multi-modalapproaches. ", "id2": "505", "id3": "None"}
{"id": "507", "content": "Image segmentation and classification are the two main fundamental steps inpattern recognition. To perform medical image segmentation or classificationwith deep learning models, it requires training on large image dataset withannotation. The dermoscopy images (ISIC archive) considered for this work doesnot have ground truth information for lesion segmentation. Performing manuallabelling on this dataset is time-consuming. To overcome this issue,self-learning annotation scheme was proposed in the two-stage deep learningalgorithm. The two-stage deep learning algorithm consists of U-Net segmentationmodel with the annotation scheme and CNN classifier model. The annotationscheme uses a K-means clustering algorithm along with merging conditions toachieve initial labelling information for training the U-Net model. Theclassifier models namely ResNet-50 and LeNet-5 were trained and tested on theimage dataset without segmentation for comparison and with the U-Netsegmentation for implementing the proposed self-learning ArtificialIntelligence (AI) framework. The classification results of the proposed AIframework achieved training accuracy of 93.8% and testing accuracy of 82.42%when compared with the two classifier models directly trained on the inputimages. ", "id2": "506", "id3": "None"}
{"id": "508", "content": "In preoperative imaging, the demarcation of rectal cancer with magneticresonance images provides an important basis for cancer staging and treatmentplanning. Recently, deep learning has greatly improved the state-of-the-artmethod in automatic segmentation. However, limitations in data availability inthe medical field can cause large variance and consequent overfitting tomedical image segmentation networks. In this study, we propose methods toreduce the model variance of a rectal cancer segmentation network by adding arectum segmentation task and performing data augmentation; the geometriccorrelation between the rectum and rectal cancer motivated the former approach.Moreover, we propose a method to perform a bias-variance analysis within anarbitrary region-of-interest (ROI) of a segmentation network, which we appliedto assess the efficacy of our approaches in reducing model variance. As aresult, adding a rectum segmentation task reduced the model variance of therectal cancer segmentation network within tumor regions by a factor of 0.90;data augmentation further reduced the variance by a factor of 0.89. Theseapproaches also reduced the training duration by a factor of 0.96 and a furtherfactor of 0.78, respectively. Our approaches will improve the quality of rectalcancer staging by increasing the accuracy of its automatic demarcation and byproviding rectum boundary information since rectal cancer staging requires thedemarcation of both rectum and rectal cancer. Besides such clinical benefits,our method also enables segmentation networks to be assessed with bias-varianceanalysis within an arbitrary ROI, such as a cancerous region. ", "id2": "507", "id3": "None"}
{"id": "509", "content": "This paper tackles the problem of real-time semantic segmentation of highdefinition videos using a hybrid GPU / CPU approach. We propose an EfficientVideo Segmentation(EVS) pipeline that combines:  (i) On the CPU, a very fast optical flow method, that is used to exploit thetemporal aspect of the video and propagate semantic information from one frameto the next. It runs in parallel with the GPU.  (ii) On the GPU, two Convolutional Neural Networks: A main segmentationnetwork that is used to predict dense semantic labels from scratch, and aRefiner that is designed to improve predictions from previous frames with thehelp of a fast Inconsistencies Attention Module (IAM). The latter can identifyregions that cannot be propagated accurately.  We suggest several operating points depending on the desired frame rate andaccuracy. Our pipeline achieves accuracy levels competitive to the existingreal-time methods for semantic image segmentation(mIoU above 60%), whileachieving much higher frame rates. On the popular Cityscapes dataset with highresolution frames (2048 x 1024), the proposed operating points range from 80 to1000 Hz on a single GPU and CPU. ", "id2": "508", "id3": "None"}
{"id": "510", "content": "We propose a novel approach for image segmentation that combines NeuralOrdinary Differential Equations (NODEs) and the Level Set method. Our approachparametrizes the evolution of an initial contour with a NODE that implicitlylearns from data a speed function describing the evolution. In addition, forcases where an initial contour is not available and to alleviate the need forcareful choice or design of contour embedding functions, we propose aNODE-based method that evolves an image embedding into a dense per-pixelsemantic label space. We evaluate our methods on kidney segmentation (KiTS19)and on salient object detection (PASCAL-S, ECSSD and HKU-IS). In addition toimproving initial contours provided by deep learning models while using afraction of their number of parameters, our approach achieves F scores that arehigher than several state-of-the-art deep learning algorithms. ", "id2": "509", "id3": "None"}
{"id": "511", "content": "That most deep learning models are purely data driven is both a strength anda weakness. Given sufficient training data, the optimal model for a particularproblem can be learned. However, this is usually not the case and so insteadthe model is either learned from scratch from a limited amount of training dataor pre-trained on a different problem and then fine-tuned. Both of thesesituations are potentially suboptimal and limit the generalizability of themodel. Inspired by this, we investigate methods to inform or guide deeplearning models for geospatial image analysis to increase their performancewhen a limited amount of training data is available or when they are applied toscenarios other than which they were trained on. In particular, we exploit thefact that there are certain fundamental rules as to how things are distributedon the surface of the Earth and these rules do not vary substantially betweenlocations. Based on this, we develop a novel feature pooling method forconvolutional neural networks using Getis-Ord Gi* analysis from geostatistics.Experimental results show our proposed pooling function has significantlybetter generalization performance compared to a standard data-driven approachwhen applied to overhead image segmentation. ", "id2": "510", "id3": "None"}
{"id": "512", "content": "Recent advances in AI technology have made the forgery of digital images andvideos easier, and it has become significantly more difficult to identify suchforgeries. These forgeries, if disseminated with malicious intent, cannegatively impact social and political stability, and pose significant ethicaland legal challenges as well. Deepfake is a variant of auto-encoders that usedeep learning techniques to identify and exchange images of a persons face ina picture or film. Deepfake can result in an erosion of public trust in digitalimages and videos, which has far-reaching effects on political and socialstability. This study therefore proposes a solution for facial forgerydetection to determine if a picture or film has ever been processed byDeepfake. The proposed solution reaches detection efficiency by using therecently proposed separable convolutional neural network (CNN) and imagesegmentation. In addition, this study also examined how different imagesegmentation methods affect detection results. Finally, the ensemble model isused to improve detection capabilities. Experiment results demonstrated theexcellent performance of the proposed solution. ", "id2": "511", "id3": "None"}
{"id": "513", "content": "The minimal path model based on the Eikonal partial differential equation(PDE) has served as a fundamental tool for the applications of imagesegmentation and boundary detection in the passed three decades. However, theexisting minimal paths-based image segmentation approaches commonly rely on theimage boundary features, potentially limiting their performance in somesituations. In this paper, we introduce a new variational image segmentationmodel based on the minimal path framework and the Eikonal PDE, where theregion-based functional that defines the homogeneity criteria can be taken intoaccount for estimating the associated geodesic paths. This is done byestablishing a geodesic curve interpretation to the region-based active contourevolution problem. The image segmentation processing is carried out in aniterative manner in our approach. A crucial ingredient in each iteration is toconstruct an asymmetric Randers geodesic metric using a sufficiently smallvector field, such that a set of geodesic paths can be tracked from thegeodesic distance map which is the solution to an Eikonal PDE. The objectboundary can be delineated by the concatenation of the final geodesic paths. Weinvoke the Finsler variant of the fast marching method to estimate the geodesicdistance map, yielding an efficient implementation of the proposed Eikonalregion-based active contour model. Experimental results on both of thesynthetic and real images exhibit that our model indeed achieves encouragingsegmentation performance. ", "id2": "512", "id3": "None"}
{"id": "514", "content": "A novel multi-focus image fusion algorithm performed in spatial domain basedon similarity characteristics is proposed incorporating with regionsegmentation. In this paper, a new similarity measure is developed based on thestructural similarity (SSIM) index, which is more suitable for multi-focusimage segmentation. Firstly, the SSNSIM map is calculated between two inputimages. Then we segment the SSNSIM map using watershed method, and merge thesmall homogeneous regions with fuzzy c-means clustering algorithm (FCM). Forthree source images, a joint region segmentation method based on segmentationof two images is used to obtain the final segmentation result. Finally, thecorresponding segmented regions of the source images are fused according totheir average gradient. The performance of the image fusion method is evaluatedby several criteria including spatial frequency, average gradient, entropy,edge retention etc. The evaluation results indicate that the proposed method iseffective and has good visual perception. ", "id2": "513", "id3": "None"}
{"id": "515", "content": "There is active research targeting local image manipulations that can fooldeep neural networks (DNNs) into producing incorrect results. This paperexamines a type of global image manipulation that can produce similar adverseeffects. Specifically, we explore how strong color casts caused by incorrectlyapplied computational color constancy - referred to as white balance (WB) inphotography - negatively impact the performance of DNNs targeting imagesegmentation and classification. In addition, we discuss how existing imageaugmentation methods used to improve the robustness of DNNs are not well suitedfor modeling WB errors. To address this problem, a novel augmentation method isproposed that can emulate accurate color constancy degradation. We also explorepre-processing training and testing images with a recent WB correctionalgorithm to reduce the effects of incorrectly white-balanced images. Weexamine both augmentation and pre-processing strategies on different datasetsand demonstrate notable improvements on the CIFAR-10, CIFAR-100, and ADE20Kdatasets. ", "id2": "514", "id3": "None"}
{"id": "516", "content": "Semantic image segmentation plays a pivotal role in many vision applicationsincluding autonomous driving and medical image analysis. Most of the formerapproaches move towards enhancing the performance in terms of accuracy with alittle awareness of computational efficiency. In this paper, we introduceLiteSeg, a lightweight architecture for semantic image segmentation. In thiswork, we explore a new deeper version of Atrous Spatial Pyramid Pooling module(ASPP) and apply short and long residual connections, and depthwise separableconvolution, resulting in a faster and efficient model. LiteSeg architecture isintroduced and tested with multiple backbone networks as Darknet19, MobileNet,and ShuffleNet to provide multiple trade-offs between accuracy andcomputational cost. The proposed model LiteSeg, with MobileNetV2 as a backbonenetwork, achieves an accuracy of 67.81% mean intersection over union at 161frames per second with $640  times 360$ resolution on the Cityscapes dataset. ", "id2": "515", "id3": "None"}
{"id": "517", "content": "Vegetation is a relevant feature in the urban scenery and its awareness canbe measured in an image by the Green View Index (GVI). Previous approaches toestimate the GVI were based upon heuristics image processing approaches andrecently by deep learning networks (DLN). By leveraging some recent DLNarchitectures tuned to the image segmentation problem and exploiting aweighting strategy in the loss function (LF) we improved previously reportedresults in similar datasets. ", "id2": "516", "id3": "None"}
{"id": "518", "content": "Image segmentation is one of the most fundamental tasks of computer vision.In many practical applications, it is essential to properly evaluate thereliability of individual segmentation results. In this study, we propose anovel framework to provide the statistical significance of segmentation resultsin the form of p-values. Specifically, we consider a statistical hypothesistest for determining the difference between the object and the backgroundregions. This problem is challenging because the difference can be deceptivelylarge (called segmentation bias) due to the adaptation of the segmentationalgorithm to the data. To overcome this difficulty, we introduce a statisticalapproach called selective inference, and develop a framework to compute validp-values in which the segmentation bias is properly accounted for. Although theproposed framework is potentially applicable to various segmentationalgorithms, we focus in this paper on graph cut-based and threshold-basedsegmentation algorithms, and develop two specific methods to compute validp-values for the segmentation results obtained by these algorithms. We provethe theoretical validity of these two methods and demonstrate theirpracticality by applying them to segmentation problems for medical images. ", "id2": "517", "id3": "None"}
{"id": "519", "content": "Modern computer vision (CV) is often based on convolutional neural networks(CNNs) that excel at hierarchical feature extraction. The previous generationof CV approaches was often based on conditional random fields (CRFs) that excelat modeling flexible higher order interactions. As their benefits arecomplementary they are often combined. However, these approaches generally usemean-field approximations and thus, arguably, did not directly optimize thereal problem. Here we revisit dual-decomposition-based approaches to CRFoptimization, an alternative to the mean-field approximation. These algorithmscan efficiently and exactly solve sub-problems and directly optimize a convexupper bound of the real problem, providing optimality certificates on the way.Our approach uses a novel fixed-point iteration algorithm which enjoysdual-monotonicity, dual-differentiability and high parallelism. The wholesystem, CRF and CNN can thus be efficiently trained using back-propagation. Wedemonstrate the effectiveness of our system on semantic image segmentation,showing consistent improvement over baseline models. ", "id2": "518", "id3": "None"}
{"id": "520", "content": "Medical image segmentation is a fundamental task in medical image analysis.Despite that deep convolutional neural networks have gained stellar performancein this challenging task, they typically rely on large labeled datasets, whichhave limited their extension to customized applications. By revisiting thesuperiority of atlas based segmentation methods, we present a new framework ofOne-pass aligned Atlas Set for Images Segmentation (OASIS). To address theproblem of time-consuming iterative image registration used for atlas warping,the proposed method takes advantage of the power of deep learning to achieveone-pass image registration. In addition, by applying label constraint, OASISalso makes the registration process to be focused on the regions to besegmented for improving the performance of segmentation. Furthermore, insteadof using image based similarity for label fusion, which can be distracted bythe large background areas, we propose a novel strategy to compute the labelsimilarity based weights for label fusion. Our experimental results on thechallenging task of prostate MR image segmentation demonstrate that OASIS isable to significantly increase the segmentation performance compared to otherstate-of-the-art methods. ", "id2": "519", "id3": "None"}
{"id": "521", "content": "Online Normalization is a new technique for normalizing the hiddenactivations of a neural network. Like Batch Normalization, it normalizes thesample dimension. While Online Normalization does not use batches, it is asaccurate as Batch Normalization. We resolve a theoretical limitation of BatchNormalization by introducing an unbiased technique for computing the gradientof normalized activations. Online Normalization works with automaticdifferentiation by adding statistical normalization as a primitive. Thistechnique can be used in cases not covered by some other normalizers, such asrecurrent networks, fully connected networks, and networks with activationmemory requirements prohibitive for batching. We show its applications to imageclassification, image segmentation, and language modeling. We present formalproofs and experimental results on ImageNet, CIFAR, and PTB datasets. ", "id2": "520", "id3": "None"}
{"id": "522", "content": "LIDAR point clouds and RGB-images are both extremely essential for 3D objectdetection. So many state-of-the-art 3D detection algorithms dedicate in fusingthese two types of data effectively. However, their fusion methods based onBirds Eye View (BEV) or voxel format are not accurate. In this paper, wepropose a novel fusion approach named Point-based Attentive Cont-convFusion(PACF) module, which fuses multi-sensor features directly on 3D points.Except for continuous convolution, we additionally add a Point-Pooling and anAttentive Aggregation to make the fused features more expressive. Moreover,based on the PACF module, we propose a 3D multi-sensor multi-task networkcalled Pointcloud-Image RCNN(PI-RCNN as brief), which handles the imagesegmentation and 3D object detection tasks. PI-RCNN employs a segmentationsub-network to extract full-resolution semantic feature maps from images andthen fuses the multi-sensor features via powerful PACF module. Beneficial fromthe effectiveness of the PACF module and the expressive semantic features fromthe segmentation module, PI-RCNN can improve much in 3D object detection. Wedemonstrate the effectiveness of the PACF module and PI-RCNN on the KITTI 3DDetection benchmark, and our method can achieve state-of-the-art on the metricof 3D AP. ", "id2": "521", "id3": "None"}
{"id": "523", "content": "We present an image segmentation method that iteratively evolves a polygon.At each iteration, the vertices of the polygon are displaced based on the localvalue of a 2D shift map that is inferred from the input image via anencoder-decoder architecture. The main training loss that is used is thedifference between the polygon shape and the ground truth segmentation mask.The network employs a neural renderer to create the polygon from its vertices,making the process fully differentiable. We demonstrate that our methodoutperforms the state of the art segmentation networks and deep active contoursolutions in a variety of benchmarks, including medical imaging and aerialimages. Our code is available at https://github.com/shirgur/ACDRNet. ", "id2": "522", "id3": "None"}
{"id2": 1049, "id3": "522", "content": "We present an image segmentation method that iteratively evolves a polygon.At each iteration, the vertices of the polygon are displaced based on the localvalue of a 2D shift map that is inferred from the input image via anencoder-decoder architecture. The main training loss that is used is thedifference between the polygon shape and the ground truth segmentation mask.The network employs a neural renderer to create the polygon from its vertices,making the process fully differentiable. We demonstrate that our methodoutperforms the state of the art segmentation networks and deep active contoursolutions in a variety of benchmarks, including medical imaging and aerialimages. Our code is available at https://github.com/shirgur/ACDRNet."}
{"id": "524", "content": "Cutting and pasting image segments feels intuitive: the choice of sourcetemplates gives artists flexibility in recombining existing source material.Formally, this process takes an image set as input and outputs a collage of theset elements. Such selection from sets of source templates does not fit easilyin classical convolutional neural models requiring inputs of fixed size.Inspired by advances in attention and set-input machine learning, we present anovel architecture that can generate in one forward pass image collages ofsource templates using set-structured representations. This paper has thefollowing contributions: (i) a novel framework for image generation calledMemory Attentive Generation of Image Collages (MAGIC) which gives artists newways to create digital collages; (ii) from the machine-learning perspective, weshow a novel Generative Adversarial Networks (GAN) architecture that usesSet-Transformer layers and set-pooling to blend sets of random image samples -a hybrid non-parametric approach. ", "id2": "523", "id3": "None"}
{"id": "525", "content": "Semantic segmentation is one of the basic topics in computer vision, it aimsto assign semantic labels to every pixel of an image. Unbalanced semantic labeldistribution could have a negative influence on segmentation accuracy. In thispaper, we investigate using data augmentation approach to balance the semanticlabel distribution in order to improve segmentation performance. We proposeusing generative adversarial networks (GANs) to generate realistic images forimproving the performance of semantic segmentation networks. Experimentalresults show that the proposed method can not only improve segmentationperformance on those classes with low accuracy, but also obtain 1.3% to 2.1%increase in average segmentation accuracy. It shows that this augmentationmethod can boost accuracy and be easily applicable to any other segmentationmodels. ", "id2": "524", "id3": "None"}
{"id": "526", "content": "The medical image is characterized by the inter-class indistinction, highvariability, and noise, where the recognition of pixels is challenging. Unlikeprevious self-attention based methods that capture context information from onelevel, we reformulate the self-attention mechanism from the view of thehigh-order graph and propose a novel method, namely Hierarchical AttentionNetwork (HANet), to address the problem of medical image segmentation.Concretely, an HA module embedded in the HANet captures context informationfrom neighbors of multiple levels, where these neighbors are extracted from thehigh-order graph. In the high-order graph, there will be an edge between twonodes only if the correlation between them is high enough, which naturallyreduces the noisy attention information caused by the inter-classindistinction. The proposed HA module is robust to the variance of input andcan be flexibly inserted into the existing convolution neural networks. Weconduct experiments on three medical image segmentation tasks including opticdisc/cup segmentation, blood vessel segmentation, and lung segmentation.Extensive results show our method is more effective and robust than theexisting state-of-the-art methods. ", "id2": "525", "id3": "None"}
{"id2": 1050, "id3": "525", "content": "The medical image is characterized by the inter-class indistinction, highvariability, and noise, where the recognition of pixels is challenging. Unlikeprevious self-attention based methods that capture context information from onelevel, we reformulate the self-attention mechanism from the view of thehigh-order graph and propose a novel method, namely Hierarchical AttentionNetwork (HANet), to address the problem of medical image segmentation.Concretely, an HA module embedded in the HANet captures context informationfrom neighbors of multiple levels, where these neighbors are extracted from thehigh-order graph. In the high-order graph, there will be an edge between twonodes only if the correlation between them is high enough, which naturallyreduces the noisy attention information caused by the inter-classindistinction. The proposed HA module is robust to the variance of input andcan be flexibly inserted into the existing convolution neural networks. Weconduct experiments on three medical image segmentation tasks including opticdisc/cup segmentation, blood vessel segmentation, and lung segmentation.Extensive results show our method is more effective and robust than theexisting state-of-the-art methods."}
{"id": "527", "content": "Existing automatic 3D image segmentation methods usually fail to meet theclinic use. Many studies have explored an interactive strategy to improve theimage segmentation performance by iteratively incorporating user hints.However, the dynamic process for successive interactions is largely ignored. Wehere propose to model the dynamic process of iterative interactive imagesegmentation as a Markov decision process (MDP) and solve it with reinforcementlearning (RL). Unfortunately, it is intractable to use single-agent RL forvoxel-wise prediction due to the large exploration space. To reduce theexploration space to a tractable size, we treat each voxel as an agent with ashared voxel-level behavior strategy so that it can be solved with multi-agentreinforcement learning. An additional advantage of this multi-agent model is tocapture the dependency among voxels for segmentation task. Meanwhile, to enrichthe information of previous segmentations, we reserve the predictionuncertainty in the state space of MDP and derive an adjustment action spaceleading to a more precise and finer segmentation. In addition, to improve theefficiency of exploration, we design a relative cross-entropy gain-based rewardto update the policy in a constrained direction. Experimental results onvarious medical datasets have shown that our method significantly outperformsexisting state-of-the-art methods, with the advantage of fewer interactions anda faster convergence. ", "id2": "526", "id3": "None"}
{"id": "528", "content": "There has been a debate in 3D medical image segmentation on whether to use 2Dor 3D networks, where both pipelines have advantages and disadvantages. 2Dmethods enjoy a low inference time and greater transfer-ability while 3Dmethods are superior in performance for hard targets requiring contextualinformation. This paper investigates efficient 3D segmentation from anotherperspective, which uses 2D networks to mimic 3D segmentation. To compensate thelack of contextual information in 2D manner, we propose to thicken the 2Dnetwork inputs by feeding multiple slices as multiple channels into 2D networksand thus 3D contextual information is incorporated. We also put forward to useearly-stage multiplexing and slice sensitive attention to solve the confusionproblem of information loss which occurs when 2D networks face thickenedinputs. With this design, we achieve a higher performance while maintaining alower inference latency on a few abdominal organs from CT scans, in particularwhen the organ has a peculiar 3D shape and thus strongly requires contextualinformation, demonstrating our methods effectiveness and ability in capturing3D information. We also point out that thickened 2D inputs pave a new methodof 3D segmentation, and look forward to more efforts in this direction.Experiments on segmenting a few abdominal targets in particular blood vesselswhich require strong 3D contexts demonstrate the advantages of our approach. ", "id2": "527", "id3": "None"}
{"id": "529", "content": "Identify the cells nuclei is the important point for most medical analyses.To assist doctors finding the accurate cell nuclei location automatically ishighly demanded in the clinical practice. Recently, fully convolutional neuralnetwork (FCNs) serve as the back-bone in many image segmentation, like liverand tumer segmentation in medical field, human body block in technical filed.The cells nuclei identification task is also kind of image segmentation. Toachieve this, we prefer to use deep learning algorithms. we construct threegeneral frameworks, one is Mask Region-based Convolutional Neural Network (MaskRCNN), which has the high performance in many image segmentations, one isU-net, which has the high generalization performance on small dataset and theother is DenseUNet, which is mixture network architecture with Dense Net andU-net. we compare the performance of these three frameworks. And we evaluatedour method on the dataset of data science bowl 2018 challenge. For single modelwithout any ensemble, they all have good performance. ", "id2": "528", "id3": "None"}
{"id": "530", "content": "In this paper, we demonstrate the ability to discriminate between cultivatedmaize plant and grass or grass-like weed image segments using the contextsurrounding the image segments. While convolutional neural networks havebrought state of the art accuracies within object detection, errors arise whenobjects in different classes share similar features. This scenario often occurswhen objects in images are viewed at too small of a scale to discern distinctdifferences in features, causing images to be incorrectly classified orlocalized. To solve this problem, we will explore using context whenclassifying image segments. This technique involves feeding a convolutionalneural network a central square image along with a border of its directsurroundings at train and test times. This means that although images arelabelled at a smaller scale to preserve accurate localization, the networkclassifies the images and learns features that include the wider context. Wedemonstrate the benefits of this context technique in the object detection taskthrough a case study of grass (foxtail) and grass-like (yellow nutsedge) weeddetection in maize fields. In this standard situation, adding context alonenearly halved the error of the neural network from 7.1% to 4.3%. After only oneepoch with context, the network also achieved a higher accuracy than thenetwork without context did after 50 epochs. The benefits of using the contexttechnique are likely to particularly evident in agricultural contexts in whichparts (such as leaves) of several plants may appear similar when not takinginto account the context in which those parts appear. ", "id2": "529", "id3": "None"}
{"id": "531", "content": "The screening of baggage using X-ray scanners is now routine in aviationsecurity with automatic threat detection approaches, based on 3D X-ray computedtomography (CT) images, known as Automatic Threat Recognition (ATR) within theaviation security industry. These current strategies use pre-defined threatmaterial signatures in contrast to adaptability towards new and emerging threatsignatures. To address this issue, the concept of adaptive automatic threatrecognition (AATR) was proposed in previous work. In this paper, we present asolution to AATR based on such X-ray CT baggage scan imagery. This aims toaddress the issues of rapidly evolving threat signatures within the screeningrequirements. Ideally, the detection algorithms deployed within the securityscanners should be readily adaptable to different situations with varyingrequirements of threat characteristics (e.g., threat material, physicalproperties of objects). We tackle this issue using a novel adaptive machinelearning methodology with our solution consisting of a multi-scale 3D CT imagesegmentation algorithm, a multi-class support vector machine (SVM) classifierfor object material recognition and a strategy to enable the adaptability ofour approach. Experiments are conducted on both open and sequestered 3D CTbaggage image datasets specifically collected for the AATR study. Our proposedapproach performs well on both recognition and adaptation. Overall our approachcan achieve the probability of detection around 90% with a probability of falsealarm below 20%. Our AATR shows the capabilities of adapting to varying typesof materials, even the unknown materials which are not available in thetraining data, adapting to varying required probability of detection andadapting to varying scales of the threat object. ", "id2": "530", "id3": "None"}
{"id": "532", "content": "Image co-segmentation is important for its advantage of alleviating theill-pose nature of image segmentation through exploring the correlation betweenrelated images. Many automatic image co-segmentation algorithms have beendeveloped in the last decade, which are investigated comprehensively in thispaper. We firstly analyze visual/semantic cues for guiding imageco-segmentation, including object cues and correlation cues. Then we describethe traditional methods in three categories of object elements based, objectregions/contours based, common object model based. In the next part, deeplearning based methods are reviewed. Furthermore, widely used test datasets andevaluation criteria are introduced and the reported performances of thesurveyed algorithms are compared with each other. Finally, we discuss thecurrent challenges and possible future directions and conclude the paper.Hopefully, this comprehensive investigation will be helpful for the developmentof image co-segmentation technique. ", "id2": "531", "id3": "None"}
{"id": "533", "content": "Radar signals have been dramatically increasing in complexity, limiting thesource separation ability of traditional approaches. In this paper we propose aDeep Learning-based clustering method, which encodes concurrent signals intoimages, and, for the first time, tackles clustering with image segmentation.Novel loss functions are introduced to optimize a Neural Network to separatethe input pulses into pure and non-fragmented clusters. Outperforming a varietyof baselines, the proposed approach is capable of clustering inputs directlywith a Neural Network, in an end-to-end fashion. ", "id2": "532", "id3": "None"}
{"id": "534", "content": "Over the last decade, electron microscopy has improved up to a point thatgenerating high quality gigavoxel sized datasets only requires a few hours.Automated image analysis, particularly image segmentation, however, has notevolved at the same pace. Even though state-of-the-art methods such as U-Netand DeepLab have improved segmentation performance substantially, the requiredamount of labels remains too expensive. Active learning is the subfield inmachine learning that aims to mitigate this burden by selecting the samplesthat require labeling in a smart way. Many techniques have been proposed,particularly for image classification, to increase the steepness of learningcurves. In this work, we extend these techniques to deep CNN based imagesegmentation. Our experiments on three different electron microscopy datasetsshow that active learning can improve segmentation quality by 10 to 15% interms of Jaccard score compared to standard randomized sampling. ", "id2": "533", "id3": "None"}
{"id2": 1051, "id3": "533", "content": "Over the last decade, electron microscopy has improved up to a point thatgenerating high quality gigavoxel sized datasets only requires a few hours.Automated image analysis, particularly image segmentation, however, has notevolved at the same pace. Even though state-of-the-art methods such as U-Netand DeepLab have improved segmentation performance substantially, the requiredamount of labels remains too expensive. Active learning is the subfield inmachine learning that aims to mitigate this burden by selecting the samplesthat require labeling in a smart way. Many techniques have been proposed,particularly for image classification, to increase the steepness of learningcurves. In this work, we extend these techniques to deep CNN based imagesegmentation. Our experiments on three different electron microscopy datasetsshow that active learning can improve segmentation quality by 10 to 15% interms of Jaccard score compared to standard randomized sampling."}
{"id": "535", "content": "Convolutional neural networks (CNNs) have been widely and successfully usedfor medical image segmentation. However, CNNs are typically considered torequire large numbers of dedicated expert-segmented training volumes, which maybe limiting in practice. This work investigates whether clinically obtainedsegmentations which are readily available in picture archiving andcommunication systems (PACS) could provide a possible source of data to train aCNN for segmentation of organs-at-risk (OARs) in radiotherapy treatmentplanning. In such data, delineations of structures deemed irrelevant to thetarget clinical use may be lacking. To overcome this issue, we use multi-labelinstead of multi-class segmentation. We empirically assess how many clinicaldelineations would be sufficient to train a CNN for the segmentation of OARsand find that increasing the training set size beyond a limited number ofimages leads to sharply diminishing returns. Moreover, we find that by usingmulti-label segmentation, missing structures in the reference standard do nothave a negative effect on overall segmentation accuracy. These results indicatethat segmentations obtained in a clinical workflow can be used to train anaccurate OAR segmentation model. ", "id2": "534", "id3": "None"}
{"id2": 1052, "id3": "534", "content": "Convolutional neural networks (CNNs) have been widely and successfully usedfor medical image segmentation. However, CNNs are typically considered torequire large numbers of dedicated expert-segmented training volumes, which maybe limiting in practice. This work investigates whether clinically obtainedsegmentations which are readily available in picture archiving andcommunication systems (PACS) could provide a possible source of data to train aCNN for segmentation of organs-at-risk (OARs) in radiotherapy treatmentplanning. In such data, delineations of structures deemed irrelevant to thetarget clinical use may be lacking. To overcome this issue, we use multi-labelinstead of multi-class segmentation. We empirically assess how many clinicaldelineations would be sufficient to train a CNN for the segmentation of OARsand find that increasing the training set size beyond a limited number ofimages leads to sharply diminishing returns. Moreover, we find that by usingmulti-label segmentation, missing structures in the reference standard do nothave a negative effect on overall segmentation accuracy. These results indicatethat segmentations obtained in a clinical workflow can be used to train anaccurate OAR segmentation model."}
{"id": "536", "content": "Regular inspection of rail valves and engines is an important task to ensurethe safety and efficiency of railway networks around the globe. Over the pastdecade, computer vision and pattern recognition based techniques have gainedtraction for such inspection and defect detection tasks. An automatedend-to-end trained system can potentially provide a low-cost, high throughput,and cheap alternative to manual visual inspection of these components. However,such systems require a huge amount of defective images for networks tounderstand complex defects. In this paper, a multi-phase deep learning basedtechnique is proposed to perform accurate fault detection of rail-valves. Ourapproach uses a two-step method to perform high precision image segmentation ofrail-valves resulting in pixel-wise accurate segmentation. Thereafter, acomputer vision technique is used to identify faulty valves. We demonstratethat the proposed approach results in improved detection performance whencompared to current state-of-theart techniques used in fault detection. ", "id2": "535", "id3": "None"}
{"id": "537", "content": "We introduce a novel Deep Learning framework, which quantitatively estimatesimage segmentation quality without the need for human inspection or labeling.We refer to this method as a Quality Assurance Network -- QANet. Specifically,given an image and a proposed corresponding segmentation, obtained by anymethod including manual annotation, the QANet solves a regression problem inorder to estimate a predefined quality measure with respect to the unknownground truth. The QANet is by no means yet another segmentation method.Instead, it performs a multi-level, multi-feature comparison of animage-segmentation pair based on a unique network architecture, called theRibCage.  To demonstrate the strength of the QANet, we addressed the evaluation ofinstance segmentation using two different datasets from different domains,namely, high throughput live cell microscopy images from the Cell SegmentationBenchmark and natural images of plants from the Leaf Segmentation Challenge.While synthesized segmentations were used to train the QANet, it was tested onsegmentations obtained by publicly available methods that participated in thedifferent challenges. We show that the QANet accurately estimates the scores ofthe evaluated segmentations with respect to the hidden ground truth, aspublished by the challenges organizers.  The code is available at: TBD. ", "id2": "536", "id3": "None"}
{"id": "538", "content": "Many recent medical segmentation systems rely on powerful deep learningmodels to solve highly specific tasks. To maximize performance, it is standardpractice to evaluate numerous pipelines with varying model topologies,optimization parameters, pre- & postprocessing steps, and even model cascades.It is often not clear how the resulting pipeline transfers to different tasks.We propose a simple and thoroughly evaluated deep learning framework forsegmentation of arbitrary medical image volumes. The system requires notask-specific information, no human interaction and is based on a fixed modeltopology and a fixed hyperparameter set, eliminating the process of modelselection and its inherent tendency to cause method-level over-fitting. Thesystem is available in open source and does not require deep learning expertiseto use. Without task-specific modifications, the system performed better thanor similar to highly specialized deep learning methods across 3 separatesegmentation tasks. In addition, it ranked 5-th and 6-th in the first andsecond round of the 2018 Medical Segmentation Decathlon comprising another 10tasks. The system relies on multi-planar data augmentation which facilitatesthe application of a single 2D architecture based on the familiar U-Net.Multi-planar training combines the parameter efficiency of a 2D fullyconvolutional neural network with a systematic train- and test-timeaugmentation scheme, which allows the 2D model to learn a representation of the3D image volume that fosters generalization. ", "id2": "537", "id3": "None"}
{"id": "539", "content": "The Dice score and Jaccard index are commonly used metrics for the evaluationof segmentation tasks in medical imaging. Convolutional neural networks trainedfor image segmentation tasks are usually optimized for (weighted)cross-entropy. This introduces an adverse discrepancy between the learningoptimization objective (the loss) and the end target metric. Recent works incomputer vision have proposed soft surrogates to alleviate this discrepancy anddirectly optimize the desired metric, either through relaxations (soft-Dice,soft-Jaccard) or submodular optimization (Lov asz-softmax). The aim of thisstudy is two-fold. First, we investigate the theoretical differences in a riskminimization framework and question the existence of a weighted cross-entropyloss with weights theoretically optimized to surrogate Dice or Jaccard. Second,we empirically investigate the behavior of the aforementioned loss functionsw.r.t. evaluation with Dice score and Jaccard index on five medicalsegmentation tasks. Through the application of relative approximation bounds,we show that all surrogates are equivalent up to a multiplicative factor, andthat no optimal weighting of cross-entropy exists to approximate Dice orJaccard measures. We validate these findings empirically and show that, whileit is important to opt for one of the target metric surrogates rather than across-entropy-based loss, the choice of the surrogate does not make astatistical difference on a wide range of medical segmentation tasks. ", "id2": "538", "id3": "None"}
{"id": "540", "content": "The scarcity of labeled data often limits the application of supervised deeplearning techniques for medical image segmentation. This has motivated thedevelopment of semi-supervised techniques that learn from a mixture of labeledand unlabeled images. In this paper, we propose a novel semi-supervised methodthat, in addition to supervised learning on labeled training images, learns topredict segmentations consistent under a given class of transformations on bothlabeled and unlabeled images. More specifically, in this work we explorelearning equivariance to elastic deformations. We implement this through: 1) aSiamese architecture with two identical branches, each of which receives adifferently transformed image, and 2) a composite loss function with asupervised segmentation loss term and an unsupervised term that encouragessegmentation consistency between the predictions of the two branches. Weevaluate the method on a public dataset of chest radiographs with segmentationsof anatomical structures using 5-fold cross-validation. The proposed methodreaches significantly higher segmentation accuracy compared to supervisedlearning. This is due to learning transformation consistency on both labeledand unlabeled images, with the latter contributing the most. We achieve theperformance comparable to state-of-the-art chest X-ray segmentation methodswhile using substantially fewer labeled images. ", "id2": "539", "id3": "None"}
{"id2": 1053, "id3": "539", "content": "The scarcity of labeled data often limits the application of supervised deeplearning techniques for medical image segmentation. This has motivated thedevelopment of semi-supervised techniques that learn from a mixture of labeledand unlabeled images. In this paper, we propose a novel semi-supervised methodthat, in addition to supervised learning on labeled training images, learns topredict segmentations consistent under a given class of transformations on bothlabeled and unlabeled images. More specifically, in this work we explorelearning equivariance to elastic deformations. We implement this through: 1) aSiamese architecture with two identical branches, each of which receives adifferently transformed image, and 2) a composite loss function with asupervised segmentation loss term and an unsupervised term that encouragessegmentation consistency between the predictions of the two branches. Weevaluate the method on a public dataset of chest radiographs with segmentationsof anatomical structures using 5-fold cross-validation. The proposed methodreaches significantly higher segmentation accuracy compared to supervisedlearning. This is due to learning transformation consistency on both labeledand unlabeled images, with the latter contributing the most. We achieve theperformance comparable to state-of-the-art chest X-ray segmentation methodswhile using substantially fewer labeled images."}
{"id": "541", "content": "In this paper, we aim to improve the performance of semantic imagesegmentation in a semi-supervised setting in which training is effectuated witha reduced set of annotated images and additional non-annotated images. Wepresent a method based on an ensemble of deep segmentation models. Each modelis trained on a subset of the annotated data, and uses the non-annotated imagesto exchange information with the other models, similar to co-training. Even ifeach model learns on the same non-annotated images, diversity is preserved withthe use of adversarial samples. Our results show that this ability tosimultaneously train models, which exchange knowledge while preservingdiversity, leads to state-of-the-art results on two challenging medical imagedatasets. ", "id2": "540", "id3": "None"}
{"id": "542", "content": "State-of-the-art approaches for semantic segmentation rely on deepconvolutional neural networks trained on fully annotated datasets, that havebeen shown to be notoriously expensive to collect, both in terms of time andmoney. To remedy this situation, weakly supervised methods leverage other formsof supervision that require substantially less annotation effort, but theytypically present an inability to predict precise object boundaries due toapproximate nature of the supervisory signals in those regions. While greatprogress has been made in improving the performance, many of these weaklysupervised methods are highly tailored to their own specific settings. Thisraises challenges in reusing algorithms and making steady progress. In thispaper, we intentionally avoid such practices when tackling weakly supervisedsemantic segmentation. In particular, we train standard neural networks withpartial cross-entropy loss function for the labeled pixels and our proposedGated CRF loss for the unlabeled pixels. The Gated CRF loss is designed todeliver several important assets: 1) it enables flexibility in the kernelconstruction to mask out influence from undesired pixel positions; 2) itoffloads learning contextual relations to CNN and concentrates on semanticboundaries; 3) it does not rely on high-dimensional filtering and thus has asimple implementation. Throughout the paper we present the advantages of theloss function, analyze several aspects of weakly supervised training, and showthat our purist approach achieves state-of-the-art performance for bothclick-based and scribble-based annotations. ", "id2": "541", "id3": "None"}
{"id": "543", "content": "Generalization capability to unseen domains is crucial for machine learningmodels when deploying to real-world conditions. We investigate the challengingproblem of domain generalization, i.e., training a model on multi-domain sourcedata such that it can directly generalize to target domains with unknownstatistics. We adopt a model-agnostic learning paradigm with gradient-basedmeta-train and meta-test procedures to expose the optimization to domain shift.Further, we introduce two complementary losses which explicitly regularize thesemantic structure of the feature space. Globally, we align a derived softconfusion matrix to preserve general knowledge about inter-class relationships.Locally, we promote domain-independent class-specific cohesion and separationof sample features with a metric-learning component. The effectiveness of ourmethod is demonstrated with new state-of-the-art results on two common objectrecognition benchmarks. Our method also shows consistent improvement on amedical image segmentation task. ", "id2": "542", "id3": "None"}
{"id": "544", "content": "Semantic road region segmentation is a high-level task, which paves the waytowards road scene understanding. This paper presents a residual networktrained for semantic road segmentation. Firstly, we represent the projectionsof road disparities in the v-disparity map as a linear model, which can beestimated by optimizing the v-disparity map using dynamic programming. Thislinear model is then utilized to reduce the redundant information in the leftand right road images. The right image is also transformed into the leftperspective view, which greatly enhances the road surface similarity betweenthe two images. Finally, the processed stereo images and their disparity mapsare concatenated to create a set of 3D images, which are then utilized to trainour neural network. The experimental results illustrate that our networkachieves a maximum F1-measure of approximately 91.19% when analyzing the imagesfrom the KITTI road dataset. ", "id2": "543", "id3": "None"}
{"id": "545", "content": "We present a collection of 24 multiple object scenes each recorded under 18multiple light source illumination scenarios. The illuminants are varying indominant spectral colours, intensity and distance from the scene. We mainlyaddress the realistic scenarios for evaluation of computational colourconstancy algorithms, but also have aimed to make the data as general aspossible for computational colour science and computer vision. Along with theimages of the scenes, we provide spectral characteristics of the camera, lightsources and the objects and include pixel-by-pixel ground truth annotation ofuniformly coloured object surfaces thus making this useful for benchmarkingcolour-based image segmentation algorithms. The dataset is freely available athttps://github.com/visillect/mls-dataset. ", "id2": "544", "id3": "None"}
{"id": "546", "content": "Neural networks are becoming more and more popular for the analysis ofphysiological time-series. The most successful deep learning systems in thisdomain combine convolutional and recurrent layers to extract useful features tomodel temporal relations. Unfortunately, these recurrent models are difficultto tune and optimize. In our experience, they often require task-specificmodifications, which makes them challenging to use for non-experts. We proposeU-Time, a fully feed-forward deep learning approach to physiological timeseries segmentation developed for the analysis of sleep data. U-Time is atemporal fully convolutional network based on the U-Net architecture that wasoriginally proposed for image segmentation. U-Time maps sequential inputs ofarbitrary length to sequences of class labels on a freely chosen temporalscale. This is done by implicitly classifying every individual time-point ofthe input signal and aggregating these classifications over fixed intervals toform the final predictions. We evaluated U-Time for sleep stage classificationon a large collection of sleep electroencephalography (EEG) datasets. In allcases, we found that U-Time reaches or outperforms current state-of-the-artdeep learning models while being much more robust in the training process andwithout requiring architecture or hyperparameter adaptation across tasks. ", "id2": "545", "id3": "None"}
{"id": "547", "content": "Unsupervised video object segmentation has often been tackled by methodsbased on recurrent neural networks and optical flow. Despite their complexity,these kinds of approaches tend to favour short-term temporal dependencies andare thus prone to accumulating inaccuracies, which cause drift over time.Moreover, simple (static) image segmentation models, alone, can performcompetitively against these methods, which further suggests that the waytemporal dependencies are modelled should be reconsidered. Motivated by theseobservations, in this paper we explore simple yet effective strategies to modellong-term temporal dependencies. Inspired by the non-local operators of [70],we introduce a technique to establish dense correspondences between pixelembeddings of a reference anchor frame and the current one. This allows thelearning of pairwise dependencies at arbitrarily long distances withoutconditioning on intermediate frames. Without online supervision, our approachcan suppress the background and precisely segment the foreground object even inchallenging scenarios, while maintaining consistent performance over time. Witha mean IoU of $81.7 %$, our method ranks first on the DAVIS-2016 leaderboard ofunsupervised methods, while still being competitive against state-of-the-artonline semi-supervised approaches. We further evaluate our method on the FBMSdataset and the ViSal video saliency dataset, showing results competitive withthe state of the art. ", "id2": "546", "id3": "None"}
{"id": "548", "content": "Objective: Herein, a neural network-based liver segmentation algorithm isproposed, and its performance was evaluated using abdominal computed tomography(CT) images. Methods: A fully convolutional network was developed to overcomethe volumetric image segmentation problem. To guide a neural network toaccurately delineate a target liver object, the network was deeply supervisedby applying the adaptive self-supervision scheme to derive the essentialcontour, which acted as a complement with the global shape. The discriminativecontour, shape, and deep features were internally merged for the segmentationresults. Results and Conclusion: 160 abdominal CT images were used for trainingand validation. The quantitative evaluation of the proposed network wasperformed through an eight-fold cross-validation. The result showed that themethod, which uses the contour feature, segmented the liver more accuratelythan the state-of-the-art with a 2.13% improvement in the dice score.Significance: In this study, a new framework was introduced to guide a neuralnetwork and learn complementary contour features. The proposed neural networkdemonstrates that the guided contour features can significantly improve theperformance of the segmentation task. ", "id2": "547", "id3": "None"}
{"id": "549", "content": "With the advancement of remote-sensed imaging large volumes of very highresolution land cover images can now be obtained. Automation of objectrecognition in these 2D images, however, is still a key issue. High intra-classvariance and low inter-class variance in Very High Resolution (VHR) imageshamper the accuracy of prediction in object recognition tasks. Most successfultechniques in various computer vision tasks recently are based on deepsupervised learning. In this work, a deep Convolutional Neural Network (CNN)based on symmetric encoder-decoder architecture with skip connections isemployed for the 2D semantic segmentation of most common land cover objectclasses - impervious surface, buildings, low vegetation, trees and cars. Atrousconvolutions are employed to have large receptive field in the proposed CNNmodel. Further, the CNN outputs are post-processed using Fully ConnectedConditional Random Field (FCRF) model to refine the CNN pixel labelpredictions. The proposed CNN-FCRF model achieves an overall accuracy of 90.5%on the ISPRS Vaihingen Dataset. ", "id2": "548", "id3": "None"}
{"id": "550", "content": "Recently, Fully Convolutional Network (FCN) seems to be the go-toarchitecture for image segmentation, including semantic scene parsing. However,it is difficult for a generic FCN to discriminate pixels around the objectboundaries, thus FCN based methods may output parsing results with inaccurateboundaries. Meanwhile, level set based active contours are superior to theboundary estimation due to the sub-pixel accuracy that they achieve. However,they are quite sensitive to initial settings. To address these limitations, inthis paper we propose a novel Deep Multiphase Level Set (DMLS) method forsemantic scene parsing, which efficiently incorporates multiphase level setsinto deep neural networks. The proposed method consists of three modules, i.e.,recurrent FCNs, adaptive multiphase level set, and deeply supervised learning.More specifically, recurrent FCNs learn multi-level representations of inputimages with different contexts. Adaptive multiphase level set drives thediscriminative contour for each semantic class, which makes use of theadvantages of both global and local information. In each time-step of therecurrent FCNs, deeply supervised learning is incorporated for model training.Extensive experiments on three public benchmarks have shown that our proposedmethod achieves new state-of-the-art performances. ", "id2": "549", "id3": "None"}
{"id2": 1054, "id3": "549", "content": "Recently, Fully Convolutional Network (FCN) seems to be the go-toarchitecture for image segmentation, including semantic scene parsing. However,it is difficult for a generic FCN to discriminate pixels around the objectboundaries, thus FCN based methods may output parsing results with inaccurateboundaries. Meanwhile, level set based active contours are superior to theboundary estimation due to the sub-pixel accuracy that they achieve. However,they are quite sensitive to initial settings. To address these limitations, inthis paper we propose a novel Deep Multiphase Level Set (DMLS) method forsemantic scene parsing, which efficiently incorporates multiphase level setsinto deep neural networks. The proposed method consists of three modules, i.e.,recurrent FCNs, adaptive multiphase level set, and deeply supervised learning.More specifically, recurrent FCNs learn multi-level representations of inputimages with different contexts. Adaptive multiphase level set drives thediscriminative contour for each semantic class, which makes use of theadvantages of both global and local information. In each time-step of therecurrent FCNs, deeply supervised learning is incorporated for model training.Extensive experiments on three public benchmarks have shown that our proposedmethod achieves new state-of-the-art performances."}
{"id": "551", "content": "Deep neural networks enable highly accurate image segmentation, but requirelarge amounts of manually annotated data for supervised training. Few-shotlearning aims to address this shortcoming by learning a new class from a fewannotated support examples. We introduce, a novel few-shot framework, for thesegmentation of volumetric medical images with only a few annotated slices.Compared to other related works in computer vision, the major challenges arethe absence of pre-trained networks and the volumetric nature of medical scans.We address these challenges by proposing a new architecture for few-shotsegmentation that incorporates squeeze & excite blocks. Our two-armedarchitecture consists of a conditioner arm, which processes the annotatedsupport input and generates a task-specific representation. This representationis passed on to the segmenter arm that uses this information to segment the newquery image. To facilitate efficient interaction between the conditioner andthe segmenter arm, we propose to use channel squeeze & spatial excitationblocks - a light-weight computational module - that enables heavy interactionbetween both the arms with negligible increase in model complexity. Thiscontribution allows us to perform image segmentation without relying on apre-trained model, which generally is unavailable for medical scans.Furthermore, we propose an efficient strategy for volumetric segmentation byoptimally pairing a few slices of the support volume to all the slices of thequery volume. We perform experiments for organ segmentation on whole-bodycontrast-enhanced CT scans from the Visceral Dataset. Our proposed modeloutperforms multiple baselines and existing approaches with respect to thesegmentation accuracy by a significant margin. The source code is available athttps://github.com/abhi4ssj/few-shot-segmentation. ", "id2": "550", "id3": "None"}
{"id": "552", "content": "Automated fiber placement (AFP) is an advanced manufacturing technology thatincreases the rate of production of composite materials. At the same time, theneed for adaptable and fast inline control methods of such parts raises.Existing inspection systems make use of handcrafted filter chains and featuredetectors, tuned for a specific measurement methods by domain experts. Thesemethods hardly scale to new defects or different measurement devices. In thispaper, we propose to formulate AFP defect detection as an image segmentationproblem that can be solved in an end-to-end fashion using artificiallygenerated training data. We employ a probabilistic graphical model to generatetraining images and annotations. We then train a deep neural network based onrecent architectures designed for image segmentation. This leads to anappealing method that scales well with new defect types and measurement devicesand requires little real world data for training. ", "id2": "551", "id3": "None"}
{"id2": 1055, "id3": "551", "content": "Automated fiber placement (AFP) is an advanced manufacturing technology thatincreases the rate of production of composite materials. At the same time, theneed for adaptable and fast inline control methods of such parts raises.Existing inspection systems make use of handcrafted filter chains and featuredetectors, tuned for a specific measurement methods by domain experts. Thesemethods hardly scale to new defects or different measurement devices. In thispaper, we propose to formulate AFP defect detection as an image segmentationproblem that can be solved in an end-to-end fashion using artificiallygenerated training data. We employ a probabilistic graphical model to generatetraining images and annotations. We then train a deep neural network based onrecent architectures designed for image segmentation. This leads to anappealing method that scales well with new defect types and measurement devicesand requires little real world data for training."}
{"id": "553", "content": "This paper proposes the first known to us iris recognition methodologydesigned specifically for post-mortem samples. We propose to use deeplearning-based iris segmentation models to extract highly irregular iristexture areas in post-mortem iris images. We show how to use segmentation maskspredicted by neural networks in conventional, Gabor-based iris recognitionmethod, which employs circular approximations of the pupillary and limbic irisboundaries. As a whole, this method allows for a significant improvement inpost-mortem iris recognition accuracy over the methods designed only forante-mortem irises, including the academic OSIRIS and commercial IriCoreimplementations. The proposed method reaches the EER less than 1% for samplescollected up to 10 hours after death, when compared to 16.89% and 5.37% of EERobserved for OSIRIS and IriCore, respectively. For samples collected up to 369hours post-mortem, the proposed method achieves the EER 21.45%, while 33.59%and 25.38% are observed for OSIRIS and IriCore, respectively. Additionally, themethod is tested on a database of iris images collected from ophthalmologyclinic patients, for which it also offers an advantage over the two otheralgorithms. This work is the first step towards post-mortem-specific irisrecognition, which increases the chances of identification of deceased subjectsin forensic investigations. The new database of post-mortem iris imagesacquired from 42 subjects, as well as the deep learning-based segmentationmodels are made available along with the paper, to ensure all the resultspresented in this manuscript are reproducible. ", "id2": "552", "id3": "None"}
{"id": "554", "content": "The Active Contour Model (ACM) is a standard image analysis technique whosenumerous variants have attracted an enormous amount of research attentionacross multiple fields. Incorrectly, however, the ACMsdifferential-equation-based formulation and prototypical dependence on userinitialization have been regarded as being largely incompatible with therecently popular deep learning approaches to image segmentation. This paperintroduces the first tight unification of these two paradigms. In particular,we devise Deep Convolutional Active Contours (DCAC), a truly end-to-endtrainable image segmentation framework comprising a Convolutional NeuralNetwork (CNN) and an ACM with learnable parameters. The ACMs Eulerian energyfunctional includes per-pixel parameter maps predicted by the backbone CNN,which also initializes the ACM. Importantly, both the CNN and ACM componentsare fully implemented in TensorFlow, and the entire DCAC architecture isend-to-end automatically differentiable and backpropagation trainable withoutuser intervention. As a challenging test case, we tackle the problem ofbuilding instance segmentation in aerial images and evaluate DCAC on twopublicly available datasets, Vaihingen and Bing Huts. Our reseults demonstratethat, for building segmentation, the DCAC establishes a new state-of-the-artperformance by a wide margin. ", "id2": "553", "id3": "None"}
{"id": "555", "content": "Registration is a fundamental task in medical image analysis which can beapplied to several tasks including image segmentation, intra-operativetracking, multi-modal image alignment, and motion analysis. Popularregistration tools such as ANTs and NiftyReg optimize an objective function foreach pair of images from scratch which is time-consuming for large images withcomplicated deformation. Facilitated by the rapid progress of deep learning,learning-based approaches such as VoxelMorph have been emerging for imageregistration. These approaches can achieve competitive performance in afraction of a second on advanced GPUs. In this work, we construct a neuralregistration framework, called NeurReg, with a hybrid loss of displacementfields and data similarity, which substantially improves the currentstate-of-the-art of registrations. Within the framework, we simulate varioustransformations by a registration simulator which generates fixed image anddisplacement field ground truth for training. Furthermore, we design threesegmentation frameworks based on the proposed registration framework: 1)atlas-based segmentation, 2) joint learning of both segmentation andregistration tasks, and 3) multi-task learning with atlas-based segmentation asan intermediate feature. Extensive experimental results validate theeffectiveness of the proposed NeurReg framework based on various metrics: theendpoint error (EPE) of the predicted displacement field, mean square error(MSE), normalized local cross-correlation (NLCC), mutual information (MI), Dicecoefficient, uncertainty estimation, and the interpretability of thesegmentation. The proposed NeurReg improves registration accuracy with fastinference speed, which can greatly accelerate related medical image analysistasks. ", "id2": "554", "id3": "None"}
{"id": "556", "content": "Thesedays, Convolutional Neural Networks are widely used in semanticsegmentation. However, since CNN-based segmentation networks producelow-resolution outputs with rich semantic information, it is inevitable thatspatial details (e.g., small bjects and fine boundary information) ofsegmentation results will be lost. To address this problem, motivated by avariational approach to image segmentation (i.e., level set theory), we proposea novel loss function called the level set loss which is designed to refinespatial details of segmentation results. To deal with multiple classes in animage, we first decompose the ground truth into binary images. Note that eachbinary image consists of background and regions belonging to a class. Then weconvert level set functions into class probability maps and calculate theenergy for each class. The network is trained to minimize the weighted sum ofthe level set loss and the cross-entropy loss. The proposed level set lossimproves the spatial details of segmentation results in a time and memoryefficient way. Furthermore, our experimental results show that the proposedloss function achieves better performance than previous approaches. ", "id2": "555", "id3": "None"}
{"id": "557", "content": "While accelerators such as GPUs have limited memory, deep neural networks arebecoming larger and will not fit with the memory limitation of accelerators fortraining. We propose an approach to tackle this problem by rewriting thecomputational graph of a neural network, in which swap-out and swap-inoperations are inserted to temporarily store intermediate results on CPUmemory. In particular, we first revise the concept of a computational graph bydefining a concrete semantics for variables in a graph. We then formally showhow to derive swap-out and swap-in operations from an existing graph andpresent rules to optimize the graph. To realize our approach, we developed amodule in TensorFlow, named TFLMS. TFLMS is published as a pull request in theTensorFlow repository for contributing to the TensorFlow community. With TFLMS,we were able to train ResNet-50 and 3DUnet with 4.7x and 2x larger batch size,respectively. In particular, we were able to train 3DUNet using images of sizeof $192^3$ for image segmentation, which, without TFLMS, had been done only bydividing the images to smaller images, which affects the accuracy. ", "id2": "556", "id3": "None"}
{"id": "558", "content": "Overfitting in deep learning has been the focus of a number of recent works,yet its exact impact on the behavior of neural networks is not well understood.This study analyzes overfitting by examining how the distribution of logitsalters in relation to how much the model overfits. Specifically, we find thatwhen training with few data samples, the distribution of logit activations whenprocessing unseen test samples of an under-represented class tends to shifttowards and even across the decision boundary, while the over-represented classseems unaffected. In image segmentation, foreground samples are often heavilyunder-represented. We observe that sensitivity of the model drops as a resultof overfitting, while precision remains mostly stable. Based on our analysis,we derive asymmetric modifications of existing loss functions and regularizersincluding a large margin loss, focal loss, adversarial training and mixup,which specifically aim at reducing the shift observed when embedding unseensamples of the under-represented class. We study the case of binarysegmentation of brain tumor core and show that our proposed simplemodifications lead to significantly improved segmentation performance over thesymmetric variants. ", "id2": "557", "id3": "None"}
{"id": "559", "content": "A key limitation of deep convolutional neural networks (DCNN) based imagesegmentation methods is the lack of generalizability. Manually traced trainingimages are typically required when segmenting organs in a new imaging modalityor from distinct disease cohort. The manual efforts can be alleviated if themanually traced images in one imaging modality (e.g., MRI) are able to train asegmentation network for another imaging modality (e.g., CT). In this paper, wepropose an end-to-end synthetic segmentation network (SynSeg-Net) to train asegmentation network for a target imaging modality without having manuallabels. SynSeg-Net is trained by using (1) unpaired intensity images fromsource and target modalities, and (2) manual labels only from source modality.SynSeg-Net is enabled by the recent advances of cycle generative adversarialnetworks (CycleGAN) and DCNN. We evaluate the performance of the SynSeg-Net ontwo experiments: (1) MRI to CT splenomegaly synthetic segmentation forabdominal images, and (2) CT to MRI total intracranial volume syntheticsegmentation (TICV) for brain images. The proposed end-to-end approach achievedsuperior performance to two stage methods. Moreover, the SynSeg-Net achievedcomparable performance to the traditional segmentation network using targetmodality labels in certain scenarios. The source code of SynSeg-Net is publiclyavailable (https://github.com/MASILab/SynSeg-Net). ", "id2": "558", "id3": "None"}
{"id": "560", "content": "In this paper we propose a novel deep learning-based algorithm for biomedicalimage segmentation which uses a sequential attention mechanism able to shiftthe focus of attention across the image in a selective way, allowing subareaswhich are more difficult to classify to be processed at increased resolution.The spatial distribution of class information in each subarea is learned usinga retina-like representation where resolution decreases with distance from thecenter of attention. The final segmentation is achieved by averaging classpredictions over overlapping subareas, utilizing the power of ensemble learningto increase segmentation accuracy. Experimental results for semanticsegmentation task for which only a few training images are available show thata CNN using the proposed method outperforms both a patch-based classificationCNN and a fully convolutional-based method. ", "id2": "559", "id3": "None"}
{"id": "561", "content": "In this chapter, we give an overview of part of our previous work based onthe minimal path framework and the Eikonal partial differential equation (PDE).We show that by designing adequate Riemannian and Randers geodesic metrics theminimal paths can be utilized to search for solutions to almost all of theactive contour problems and to the Euler-Mumford elastica problem, which allowsto blend the advantages from minimal geodesic paths and those originalapproaches, i.e. the active contours and elastica curves. The proposed minimalpath-based models can be applied to deal with a broad variety of image analysistasks such as boundary detection, image segmentation and tubular structureextraction. The numerical implementations for the computation of minimal pathsare known to be quite efficient thanks to the Eikonal solvers such as theFinsler variant of the fast marching method. ", "id2": "560", "id3": "None"}
{"id": "562", "content": "Supervised semantic segmentation normally assumes the test data being in asimilar data domain as the training data. However, in practice, the domainmismatch between the training and unseen data could lead to a significantperformance drop. Obtaining accurate pixel-wise label for images in differentdomains is tedious and labor intensive, especially for histopathology images.In this paper, we propose a dual adaptive pyramid network (DAPNet) forhistopathological gland segmentation adapting from one stain domain to another.We tackle the domain adaptation problem on two levels: 1) the image-levelconsiders the differences of image color and style; 2) the feature-leveladdresses the spatial inconsistency between two domains. The two components areimplemented as domain classifiers with adversarial training. We evaluate ournew approach using two gland segmentation datasets with H&E and DAB-H stainsrespectively. The extensive experiments and ablation study demonstrate theeffectiveness of our approach on the domain adaptive segmentation task. We showthat the proposed approach performs favorably against other state-of-the-artmethods. ", "id2": "561", "id3": "None"}
{"id": "563", "content": "Prognostic tumor growth modeling via volumetric medical imaging observationscan potentially lead to better outcomes of tumor treatment and surgicalplanning. Recent advances of convolutional networks have demonstrated higheraccuracy than traditional mathematical models in predicting future tumorvolumes. This indicates that deep learning-based techniques may have greatpotentials on addressing such problem. However, current 2D patch-based modelingapproaches cannot make full use of the spatio-temporal imaging context of thetumors longitudinal 4D (3D + time) data. Moreover, they are incapable topredict clinically-relevant tumor properties, other than volumes. In thispaper, we exploit to formulate the tumor growth process through convolutionalLong Short-Term Memory (ConvLSTM) that extract tumors static imagingappearances and capture its temporal dynamic changes within a single network.We extend ConvLSTM into the spatio-temporal domain (ST-ConvLSTM) by jointlylearning the inter-slice 3D contexts and the longitudinal or temporal dynamicsfrom multiple patient studies. Our approach can incorporate other non-imagingpatient information in an end-to-end trainable manner. Experiments areconducted on the largest 4D longitudinal tumor dataset of 33 patients to date.Results validate that the ST-ConvLSTM produces a Dice score of 83.2%+-5.1% anda RVD of 11.2%+-10.8%, both significantly outperforming (p<0.05) other comparedmethods of linear model, ConvLSTM, and generative adversarial network (GAN)under the metric of predicting future tumor volumes. Additionally, our newmethod enables the prediction of both cell density and CT intensity numbers.Last, we demonstrate the generalizability of ST-ConvLSTM by employing it in 4Dmedical image segmentation task, which achieves an averaged Dice score of86.3+-1.2% for left-ventricle segmentation in 4D ultrasound with 3 seconds perpatient. ", "id2": "562", "id3": "None"}
{"id": "564", "content": "Three-dimensional medical image segmentation is one of the most importantproblems in medical image analysis and plays a key role in downstream diagnosisand treatment. Recent years, deep neural networks have made groundbreakingsuccess in medical image segmentation problem. However, due to the highvariance in instrumental parameters, experimental protocols, and subjectappearances, the generalization of deep learning models is often hindered bythe inconsistency in medical images generated by different machines andhospitals. In this work, we present StyleSegor, an efficient and easy-to-usestrategy to alleviate this inconsistency issue. Specifically, neural styletransfer algorithm is applied to unlabeled data in order to minimize thedifferences in image properties including brightness, contrast, texture, etc.between the labeled and unlabeled data. We also apply probabilistic adjustmenton the network output and integrate multiple predictions through ensemblelearning. On a publicly available whole heart segmentation benchmarking datasetfrom MICCAI HVSMR 2016 challenge, we have demonstrated an elevated diceaccuracy surpassing current state-of-the-art method and notably, an improvementof the total score by 29.91 %. StyleSegor is thus corroborated to be anaccurate tool for 3D whole heart segmentation especially on highly inconsistentdata, and is available at https://github.com/horsepurve/StyleSegor. ", "id2": "563", "id3": "None"}
{"id": "565", "content": "Recent advances in generative models and adversarial training have led to aflourishing image-to-image (I2I) translation literature. The current I2Itranslation approaches require training images from the two domains that areeither all paired (supervised) or all unpaired (unsupervised). In practice,obtaining paired training data in sufficient quantities is often very costlyand cumbersome. Therefore solutions that employ unpaired data, while lessaccurate, are largely preferred. In this paper, we aim to bridge the gapbetween supervised and unsupervised I2I translation, with application tosemantic image segmentation. We build upon pix2pix and CycleGAN,state-of-the-art seminal I2I translation techniques. We propose a method toselect (very few) paired training samples and achieve significant improvementsin both supervised and unsupervised I2I translation settings over randomselection. Further, we boost the performance by incorporating both (selected)paired and unpaired samples in the training process. Our experiments show thatan extremely weak supervised I2I translation solution using only one pairedtraining sample can achieve a quantitative performance much better than theunsupervised CycleGAN model, and comparable to that of the supervised pix2pixmodel trained on thousands of pairs. ", "id2": "564", "id3": "None"}
{"id": "566", "content": "This work proposes the use of Bayesian approximations of uncertainty fromdeep learning in a robot planner, showing that this produces more cautiousactions in safety-critical scenarios. The case study investigated is motivatedby a setup where an aerial robot acts as a scout for a ground robot. This isuseful when the below area is unknown or dangerous, with applications in spaceexploration, military, or search-and-rescue. Images taken from the aerial vieware used to provide a less obstructed map to guide the navigation of the roboton the ground. Experiments are conducted using a deep learning semantic imagesegmentation, followed by a path planner based on the resulting cost map, toprovide an empirical analysis of the proposed method. A comparison with similarapproaches is presented to portray the usefulness of certain techniques, orvariations within a technique, in similar experimental settings. The method isanalyzed to assess the impact of variations in the uncertainty extraction, aswell as the absence of an uncertainty metric, on the overall system with theuse of a defined metric which measures surprise to the planner. The analysis isperformed on multiple datasets, showing a similar trend of lower surprise whenuncertainty information is incorporated in the planning, given threshold valuesof the hyperparameters in the uncertainty extraction have been met. We findthat taking uncertainty into account leads to paths that could be 18% lessrisky on an average. ", "id2": "565", "id3": "None"}
{"id": "567", "content": "In real-world practice, medical images acquired in different phases possesscomplementary information,   em e.g. , radiologists often refer to botharterial and venous scans in order to make the diagnosis. However, in medicalimage analysis, fusing prediction from two phases is often difficult, because(i) there is a domain gap between two phases, and (ii) the semantic labels arenot pixel-wise corresponded even for images scanned from the same patient. Thispaper studies organ segmentation in two-phase CT scans. We propose PhaseCollaborative Network (PCN), an end-to-end framework that contains bothgenerative and discriminative modules. PCN can be mathematically explained toformulate phase-to-phase and data-to-label relations jointly. Experiments areperformed on a two-phase CT dataset, on which PCN outperforms the baselinesworking with one-phase data by a large margin, and we empirically verify thatthe gain comes from inter-phase collaboration. Besides, PCN transfers well totwo public single-phase datasets, demonstrating its potential applications. ", "id2": "566", "id3": "None"}
{"id": "568", "content": "The segmentation of digital images is one of the essential steps in imageprocessing or a computer vision system. It helps in separating the pixels intodifferent regions according to their intensity level. A large number ofsegmentation techniques have been proposed, and a few of them use complexcomputational operations. Among all, the most straightforward procedure thatcan be easily implemented is thresholding. In this paper, we present a uniqueheuristic approach for image segmentation that automatically determinesmultilevel thresholds by sampling the histogram of a digital image. Ourapproach emphasis on selecting a valley as optimal threshold values. Wedemonstrated that our approach outperforms the popular Otsus method in termsof CPU computational time. We demonstrated that our approach outperforms thepopular Otsus method in terms of CPU computational time. We observed a maximumspeed-up of 35.58x and a minimum speed-up of 10.21x on popular image processingbenchmarks. To demonstrate the correctness of our approach in determiningthreshold values, we compute PSNR, SSIM, and FSIM values to compare with thevalues obtained by Otsus method. This evaluation shows that our approach iscomparable and better in many cases as compared to well known Otsus method. ", "id2": "567", "id3": "None"}
{"id": "569", "content": "Fully convolutional neural networks (CNNs) have proven to be effective atrepresenting and classifying textural information, thus transforming imageintensity into output class masks that achieve semantic image segmentation. Inmedical image analysis, however, expert manual segmentation often relies on theboundaries of anatomical structures of interest. We propose boundary aware CNNsfor medical image segmentation. Our networks are designed to account for organboundary information, both by providing a special network edge branch andedge-aware loss terms, and they are trainable end-to-end. We validate theireffectiveness on the task of brain tumor segmentation using the BraTS 2018dataset. Our experiments reveal that our approach yields more accuratesegmentation results, which makes it promising for more extensive applicationto medical image segmentation. ", "id2": "568", "id3": "None"}
{"id": "570", "content": "Recent state-of-the-art image segmentation algorithms are mostly based ondeep neural networks, thanks to their high performance and fast computationtime. However, these methods are usually trained in a supervised manner, whichrequires large number of high quality ground-truth segmentation masks. On theother hand, classical image segmentation approaches such as level-set methodsare formulated in a self-supervised manner by minimizing energy functions suchas Mumford-Shah functional, so they are still useful to help generation ofsegmentation masks without labels. Unfortunately, these algorithms are usuallycomputationally expensive and often have limitation in semantic segmentation.In this paper, we propose a novel loss function based on Mumford-Shahfunctional that can be used in deep-learning based image segmentation withoutor with small labeled data. This loss function is based on the observation thatthe softmax layer of deep neural networks has striking similarity to thecharacteristic function in the Mumford-Shah functional. We show that the newloss function enables semi-supervised and unsupervised segmentation. Inaddition, our loss function can be also used as a regularized function toenhance supervised semantic segmentation algorithms. Experimental results onmultiple datasets demonstrate the effectiveness of the proposed method. ", "id2": "569", "id3": "None"}
{"id": "571", "content": "Convolutional neural networks (CNNs) for biomedical image analysis are oftenof very large size, resulting in high memory requirement and high latency ofoperations. Searching for an acceptable compressed representation of the baseCNN for a specific imaging application typically involves a series oftime-consuming training/validation experiments to achieve a good compromisebetween network size and accuracy. To address this challenge, we proposeCC-Net, a new image complexity-guided CNN compression scheme for biomedicalimage segmentation. Given a CNN model, CC-Net predicts the final accuracy ofnetworks of different sizes based on the average image complexity computed fromthe training data. It then selects a multiplicative factor for producing adesired network with acceptable network accuracy and size. Experiments showthat CC-Net is effective for generating compressed segmentation networks,retaining up to 95% of the base network segmentation accuracy and utilizingonly 0.1% of trainable parameters of the full-sized networks in the best case. ", "id2": "570", "id3": "None"}
{"id": "572", "content": "Image segmentation as a clustering problem is to identify pixel groups on animage without any preliminary labels available. It remains a challenge inmachine vision because of the variations in size and shape of image segments.Furthermore, determining the segment number in an image is NP-hard withoutprior knowledge of the image content. This paper presents an automatic colorimage pixel clustering scheme based on mussels wandering optimization. Byapplying an activation variable to determine the number of clusters along withthe cluster centers optimization, an image is segmented with minimal priorknowledge and human intervention. By revising the within- and between-class sumof squares ratio for random natural image contents, we provide a novel fitnessfunction for image pixel clustering tasks. Comprehensive empirical studies ofthe proposed scheme against other state-of-the-art competitors on syntheticdata and the ASD dataset have demonstrated the promising performance of theproposed scheme. ", "id2": "571", "id3": "None"}
{"id": "573", "content": "Yes, it can. Data augmentation is perhaps the oldest preprocessing step incomputer vision literature. Almost every computer vision model trained onimaging data uses some form of augmentation. In this paper, we use theinter-vertebral disk segmentation task alongside a deep residual U-Net as thelearning model, to explore the effectiveness of augmentation. In the extreme,we observed that a model trained on patches extracted from just one scan, witheach patch augmented 50 times; achieved a Dice score of 0.73 in a validationset of 40 cases. Qualitative evaluation indicated a clinically usablesegmentation algorithm, which appropriately segments regions of interest,alongside limited false positive specks. When the initial patches are extractedfrom nine scans the average Dice coefficient jumps to 0.86 and most of thefalse positives disappear. While this still falls short of state-of-the-artdeep learning based segmentation of discs reported in literature, qualitativeexamination reveals that it does yield segmentation, which can be amended byexpert clinicians with minimal effort to generate additional data for trainingimproved deep models. Extreme augmentation of training data, should thus beconstrued as a strategy for training deep learning based algorithms, when verylittle manually annotated data is available to work with. Models trained withextreme augmentation can then be used to accelerate the generation of manuallylabelled data. Hence, we show that extreme augmentation can be a valuable toolin addressing scaling up small imaging data sets to address medical imagesegmentation tasks. ", "id2": "572", "id3": "None"}
{"id": "574", "content": "In this work, we study the problem of training deep networks for semanticimage segmentation using only a fraction of annotated images, which maysignificantly reduce human annotation efforts. Particularly, we propose astrategy that exploits the unpaired image style transfer capabilities ofCycleGAN in semi-supervised segmentation. Unlike recent works using adversariallearning for semi-supervised segmentation, we enforce cycle consistency tolearn a bidirectional mapping between unpaired images and segmentation masks.This adds an unsupervised regularization effect that boosts the segmentationperformance when annotated data is limited. Experiments on three differentpublic segmentation benchmarks (PASCAL VOC 2012, Cityscapes and ACDC)demonstrate the effectiveness of the proposed method. The proposed modelachieves 2-4% of improvement with respect to the baseline and outperformsrecent approaches for this task, particularly in low labeled data regime. ", "id2": "573", "id3": "None"}
{"id": "575", "content": "In recent years, there has been remarkable progress in supervised imagesegmentation. Video segmentation is less explored, despite the temporaldimension being highly informative. Semantic labels, e.g. that cannot beaccurately detected in the current frame, may be inferred by incorporatinginformation from previous frames. However, video segmentation is challengingdue to the amount of data that needs to be processed and, more importantly, thecost involved in obtaining ground truth annotations for each frame. In thispaper, we tackle the issue of label scarcity by using consecutive frames of avideo, where only one frame is annotated. We propose a deep, end-to-endtrainable model which leverages temporal information in order to make use ofeasy to acquire unlabeled data. Our network architecture relies on a novelinterconnection of two components: a fully convolutional network to modelspatial information and temporal units that are employed at intermediate levelsof the convolutional network in order to propagate information through time.The main contribution of this work is the guidance of the temporal signalthrough the network. We show that only placing a temporal module between theencoder and decoder is suboptimal (baseline). Our extensive experiments on theCityScapes dataset indicate that the resulting model can leverage unlabeledtemporal frames and significantly outperform both the frame-by-frame imagesegmentation and the baseline approach. ", "id2": "574", "id3": "None"}
{"id2": 1056, "id3": "574", "content": "In recent years, there has been remarkable progress in supervised imagesegmentation. Video segmentation is less explored, despite the temporaldimension being highly informative. Semantic labels, e.g. that cannot beaccurately detected in the current frame, may be inferred by incorporatinginformation from previous frames. However, video segmentation is challengingdue to the amount of data that needs to be processed and, more importantly, thecost involved in obtaining ground truth annotations for each frame. In thispaper, we tackle the issue of label scarcity by using consecutive frames of avideo, where only one frame is annotated. We propose a deep, end-to-endtrainable model which leverages temporal information in order to make use ofeasy to acquire unlabeled data. Our network architecture relies on a novelinterconnection of two components: a fully convolutional network to modelspatial information and temporal units that are employed at intermediate levelsof the convolutional network in order to propagate information through time.The main contribution of this work is the guidance of the temporal signalthrough the network. We show that only placing a temporal module between theencoder and decoder is suboptimal (baseline). Our extensive experiments on theCityScapes dataset indicate that the resulting model can leverage unlabeledtemporal frames and significantly outperform both the frame-by-frame imagesegmentation and the baseline approach."}
{"id": "576", "content": "U-Net has been providing state-of-the-art performance in many medical imagesegmentation problems. Many modifications have been proposed for U-Net, such asattention U-Net, recurrent residual convolutional U-Net (R2-UNet), and U-Netwith residual blocks or blocks with dense connections. However, all thesemodifications have an encoder-decoder structure with skip connections, and thenumber of paths for information flow is limited. We propose LadderNet in thispaper, which can be viewed as a chain of multiple U-Nets. Instead of only onepair of encoder branch and decoder branch in U-Net, a LadderNet has multiplepairs of encoder-decoder branches, and has skip connections between every pairof adjacent decoder and decoder branches in each level. Inspired by the successof ResNet and R2-UNet, we use modified residual blocks where two convolutionallayers in one block share the same weights. A LadderNet has more paths forinformation flow because of skip connections and residual blocks, and can beviewed as an ensemble of Fully Convolutional Networks (FCN). The equivalence toan ensemble of FCNs improves segmentation accuracy, while the shared weightswithin each residual block reduce parameter number. Semantic segmentation isessential for retinal disease detection. We tested LadderNet on two benchmarkdatasets for blood vessel segmentation in retinal images, and achieved superiorperformance over methods in the literature. The implementation is provided url https://github.com/juntang-zhuang/LadderNet  ", "id2": "575", "id3": "None"}
{"id2": 1057, "id3": "575", "content": "U-Net has been providing state-of-the-art performance in many medical imagesegmentation problems. Many modifications have been proposed for U-Net, such asattention U-Net, recurrent residual convolutional U-Net (R2-UNet), and U-Netwith residual blocks or blocks with dense connections. However, all thesemodifications have an encoder-decoder structure with skip connections, and thenumber of paths for information flow is limited. We propose LadderNet in thispaper, which can be viewed as a chain of multiple U-Nets. Instead of only onepair of encoder branch and decoder branch in U-Net, a LadderNet has multiplepairs of encoder-decoder branches, and has skip connections between every pairof adjacent decoder and decoder branches in each level. Inspired by the successof ResNet and R2-UNet, we use modified residual blocks where two convolutionallayers in one block share the same weights. A LadderNet has more paths forinformation flow because of skip connections and residual blocks, and can beviewed as an ensemble of Fully Convolutional Networks (FCN). The equivalence toan ensemble of FCNs improves segmentation accuracy, while the shared weightswithin each residual block reduce parameter number. Semantic segmentation isessential for retinal disease detection. We tested LadderNet on two benchmarkdatasets for blood vessel segmentation in retinal images, and achieved superiorperformance over methods in the literature. The implementation is provided url https://github.com/juntang-zhuang/LadderNet"}
{"id": "577", "content": "In this paper, we propose a Customizable Architecture Search (CAS) approachto automatically generate a network architecture for semantic imagesegmentation. The generated network consists of a sequence of stackedcomputation cells. A computation cell is represented as a directed acyclicgraph, in which each node is a hidden representation (i.e., feature map) andeach edge is associated with an operation (e.g., convolution and pooling),which transforms data to a new layer. During the training, the CAS algorithmexplores the search space for an optimized computation cell to build a network.The cells of the same type share one architecture but with different weights.In real applications, however, an optimization may need to be conducted undersome constraints such as GPU time and model size. To this end, a costcorresponding to the constraint will be assigned to each operation. When anoperation is selected during the search, its associated cost will be added tothe objective. As a result, our CAS is able to search an optimized architecturewith customized constraints. The approach has been thoroughly evaluated onCityscapes and CamVid datasets, and demonstrates superior performance overseveral state-of-the-art techniques. More remarkably, our CAS achieves 72.3%mIoU on the Cityscapes dataset with speed of 108 FPS on an Nvidia TitanXp GPU. ", "id2": "576", "id3": "None"}
{"id": "578", "content": "Deep learning based medical image segmentation models usually require largedatasets with high-quality dense segmentations to train, which are verytime-consuming and expensive to prepare. One way to tackle this challenge is byusing the mixed-supervised learning framework, in which only a part of data isdensely annotated with segmentation label and the rest is weakly labeled withbounding boxes. The model is trained jointly in a multi-task learning setting.In this paper, we propose Mixed-Supervised Dual-Network (MSDN), a novelarchitecture which consists of two separate networks for the detection andsegmentation tasks respectively, and a series of connection modules between thelayers of the two networks. These connection modules are used to transferuseful information from the auxiliary detection task to help the segmentationtask. We propose to use a recent technique called Squeeze and Excitation inthe connection module to boost the transfer. We conduct experiments on twomedical image segmentation datasets. The proposed MSDN model outperformsmultiple baselines. ", "id2": "577", "id3": "None"}
{"id": "579", "content": "Data for Image segmentation models can be costly to obtain due to theprecision required by human annotators. We run a series of experiments showingthe effect of different kinds of Dropout training on the DeepLabv3+ Imagesegmentation model when trained using a small dataset. We find that whenappropriate forms of Dropout are applied in the right place in the modelarchitecture that non-insignificant improvement in Mean Intersection over Union(mIoU) score can be observed. In our best case, we find that applying Dropoutscheduling in conjunction with SpatialDropout improves baseline mIoU from 0.49to 0.59. This result shows that even where a model architecture makes extensiveuse of Batch Normalization, Dropout can still be an effective way of improvingperformance in low data situations. ", "id2": "578", "id3": "None"}
{"id2": 1058, "id3": "578", "content": "Data for Image segmentation models can be costly to obtain due to theprecision required by human annotators. We run a series of experiments showingthe effect of different kinds of Dropout training on the DeepLabv3+ Imagesegmentation model when trained using a small dataset. We find that whenappropriate forms of Dropout are applied in the right place in the modelarchitecture that non-insignificant improvement in Mean Intersection over Union(mIoU) score can be observed. In our best case, we find that applying Dropoutscheduling in conjunction with SpatialDropout improves baseline mIoU from 0.49to 0.59. This result shows that even where a model architecture makes extensiveuse of Batch Normalization, Dropout can still be an effective way of improvingperformance in low data situations."}
{"id": "580", "content": "Nowadays U-net-like FCNs predominate various biomedical image segmentationapplications and attain promising performance, largely due to their elegantarchitectures, e.g., symmetric contracting and expansive paths as well aslateral skip-connections. It remains a research direction to devise novelarchitectures to further benefit the segmentation. In this paper, we develop anACE-net that aims to enhance the feature representation and utilization byaugmenting the contracting and expansive paths. In particular, we augment thepaths by the recently proposed advanced techniques including ASPP, denseconnection and deep supervision mechanisms, and novel connections such asdirectly connecting the raw image to the expansive side. With theseaugmentations, ACE-net can utilize features from multiple sources, scales andreception fields to segment while still maintains a relative simplearchitecture. Experiments on two typical biomedical segmentation tasks validateits effectiveness, where highly competitive results are obtained in both taskswhile ACE-net still runs fast at inference. ", "id2": "579", "id3": "None"}
{"id": "581", "content": "Deep convolutional neural networks have driven substantial advancements inthe automatic understanding of images. Requiring a large collection of imagesand their associated annotations is one of the main bottlenecks limiting theadoption of deep networks. In the task of medical image segmentation, requiringpixel-level semantic annotations performed by human experts exacerbate thisdifficulty. This paper proposes a new framework to train a fully convolutionalsegmentation network from a large set of cheap unreliable annotations and asmall set of expert-level clean annotations. We propose a spatially adaptivereweighting approach to treat clean and noisy pixel-level annotationscommensurately in the loss function. We deploy a meta-learning approach toassign higher importance to pixels whose loss gradient direction is closer tothose of clean data. Our experiments on training the network using segmentationground truth corrupted with different levels of annotation noise show howspatial reweighting improves the robustness of deep networks to noisyannotations. ", "id2": "580", "id3": "None"}
{"id": "582", "content": "The performance of the state-of-the-art image segmentation methods heavilyrelies on the high-quality annotations, which are not easily affordable,particularly for medical data. To alleviate this limitation, in this study, wepropose a weakly supervised image segmentation method based on a deep geodesicprior. We hypothesize that integration of this prior information can reduce theadverse effects of weak labels in segmentation accuracy. Our proposed algorithmis based on a prior information, extracted from an auto-encoder, trained to mapobjects geodesic maps to their corresponding binary maps. The obtainedinformation is then used as an extra term in the loss function of thesegmentor. In order to show efficacy of the proposed strategy, we haveexperimented segmentation of cardiac substructures with clean and two levels ofnoisy labels (L1, L2). Our experiments showed that the proposed algorithmboosted the performance of baseline deep learning-based segmentation for bothclean and noisy labels by 4.4%, 4.6%(L1), and 6.3%(L2) in dice score,respectively. We also showed that the proposed method was more robust in thepresence of high-level noise due to the existence of shape priors. ", "id2": "581", "id3": "None"}
{"id": "583", "content": "In Computer Vision, edge detection is one of the favored approaches forfeature and object detection in images since it provides information abouttheir objects boundaries. Other region-based approaches use probabilisticanalysis such as clustering and Markov random fields, but those methods cannotbe used to analyze edges and their interaction. In fact, only imagesegmentation can produce regions based on edges, but it requires thresholdingby simply separating the regions into binary in-out information. Hence, thereis currently a gap between edge-based and region-based algorithms, since edgescannot be used to study the properties of a region and vice versa. Theobjective of this paper is to present a novel spatial probability analysis thatallows determining the probability of inclusion inside a set of partialcontours (strokes). To answer this objective, we developed a new approach thatuses electromagnetic convolutions and repulsion optimization to compute therequired probabilities. Hence, it becomes possible to generate a continuousspace of probability based only on the edge information, thus bridging the gapbetween the edge-based methods and the region-based methods. The developedmethod is consistent with the fundamental properties of inclusion probabilitiesand its results are validated by comparing an image with the probability-basedestimation given by our algorithm. The method can also be generalized to takeinto consideration the intensity of the edges or to be used for 3D shapes. Thisis the first documented method that allows computing a space of probabilitybased on interacting edges, which opens the path to broader applications suchas image segmentation and contour completion. ", "id2": "582", "id3": "None"}
{"id": "584", "content": "Long-term visual localization is the problem of estimating the camera pose ofa given query image in a scene whose appearance changes over time. It is animportant problem in practice, for example, encountered in autonomous driving.In order to gain robustness to such changes, long-term localization approachesoften use segmantic segmentations as an invariant scene representation, as thesemantic meaning of each scene part should not be affected by seasonal andother changes. However, these representations are typically not verydiscriminative due to the limited number of available classes. In this paper,we propose a new neural network, the Fine-Grained Segmentation Network (FGSN),that can be used to provide image segmentations with a larger number of labelsand can be trained in a self-supervised fashion. In addition, we show how FGSNscan be trained to output consistent labels across seasonal changes. Wedemonstrate through extensive experiments that integrating the fine-grainedsegmentations produced by our FGSNs into existing localization algorithms leadsto substantial improvements in localization performance. ", "id2": "583", "id3": "None"}
{"id": "585", "content": "Accurate segmentation of the prostate from magnetic resonance (MR) imagesprovides useful information for prostate cancer diagnosis and treatment.However, automated prostate segmentation from 3D MR images still faces severalchallenges. For instance, a lack of clear edge between the prostate and otheranatomical structures makes it challenging to accurately extract theboundaries. The complex background texture and large variation in size, shapeand intensity distribution of the prostate itself make segmentation evenfurther complicated. With deep learning, especially convolutional neuralnetworks (CNNs), emerging as commonly used methods for medical imagesegmentation, the difficulty in obtaining large number of annotated medicalimages for training CNNs has become much more pronounced that ever before.Since large-scale dataset is one of the critical components for the success ofdeep learning, lack of sufficient training data makes it difficult to fullytrain complex CNNs. To tackle the above challenges, in this paper, we propose aboundary-weighted domain adaptive neural network (BOWDA-Net). To make thenetwork more sensitive to the boundaries during segmentation, aboundary-weighted segmentation loss (BWL) is proposed. Furthermore, an advancedboundary-weighted transfer leaning approach is introduced to address theproblem of small medical imaging datasets. We evaluate our proposed model onthe publicly available MICCAI 2012 Prostate MR Image Segmentation (PROMISE12)challenge dataset. Our experimental results demonstrate that the proposed modelis more sensitive to boundary information and outperformed otherstate-of-the-art methods. ", "id2": "584", "id3": "None"}
{"id": "586", "content": "Parsing sketches via semantic segmentation is attractive but challenging,because (i) free-hand drawings are abstract with large variances in depictingobjects due to different drawing styles and skills; (ii) distorting lines drawnon the touchpad make sketches more difficult to be recognized; (iii) thehigh-performance image segmentation via deep learning technologies needsenormous annotated sketch datasets during the training stage. In this paper, wepropose a Sketch-target deep FCN Segmentation Network(SFSegNet) for automaticfree-hand sketch segmentation, labeling each sketch in a single object withmultiple parts. SFSegNet has an end-to-end network process between the inputsketches and the segmentation results, composed of 2 parts: (i) a modified deepFully Convolutional Network(FCN) using a reweighting strategy to ignorebackground pixels and classify which part each pixel belongs to; (ii) affinetransform encoders that attempt to canonicalize the shaking strokes. We trainour network with the dataset that consists of 10,000 annotated sketches, tofind an extensively applicable model to segment stokes semantically in oneground truth. Extensive experiments are carried out and segmentation resultsshow that our method outperforms other state-of-the-art networks. ", "id2": "585", "id3": "None"}
{"id": "587", "content": "We present a Deep Differentiable Simplex Layer (DDSL) for neural networks forgeometric deep learning. The DDSL is a differentiable layer compatible withdeep neural networks for bridging simplex mesh-based geometry representations(point clouds, line mesh, triangular mesh, tetrahedral mesh) with raster images(e.g., 2D/3D grids). The DDSL uses Non-Uniform Fourier Transform (NUFT) toperform differentiable, efficient, anti-aliased rasterization of simplex-basedsignals. We present a complete theoretical framework for the process as well asan efficient backpropagation algorithm. Compared to previous differentiablerenderers and rasterizers, the DDSL generalizes to arbitrary simplex degreesand dimensions. In particular, we explore its applications to 2D shapes andillustrate two applications of this method: (1) mesh editing and optimizationguided by neural network outputs, and (2) using DDSL for a differentiablerasterization loss to facilitate end-to-end training of polygon generators. Weare able to validate the effectiveness of gradient-based shape optimizationwith the example of airfoil optimization, and using the differentiablerasterization loss to facilitate end-to-end training, we surpass state of theart for polygonal image segmentation given ground-truth bounding boxes. ", "id2": "586", "id3": "None"}
{"id": "588", "content": "For the task of medical image segmentation, fully convolutional network (FCN)based architectures have been extensively used with various modifications. Arising trend in these architectures is to employ joint-learning of the targetregion with an auxiliary task, a method commonly known as multi-task learning.These approaches help impose smoothness and shape priors, which vanilla FCNapproaches do not necessarily incorporate. In this paper, we propose a novelplug-and-play module, which we term as Conv-MCD, which exploits structuralinformation in two ways - i) using the contour map and ii) using the distancemap, both of which can be obtained from ground truth segmentation maps with noadditional annotation costs. The key benefit of our module is the ease of itsaddition to any state-of-the-art architecture, resulting in a significantimprovement in performance with a minimal increase in parameters. Tosubstantiate the above claim, we conduct extensive experiments using 4state-of-the-art architectures across various evaluation metrics, and report asignificant increase in performance in relation to the base networks. Inaddition to the aforementioned experiments, we also perform ablative studiesand visualization of feature maps to further elucidate our approach. ", "id2": "587", "id3": "None"}
{"id2": 1059, "id3": "587", "content": "For the task of medical image segmentation, fully convolutional network (FCN)based architectures have been extensively used with various modifications. Arising trend in these architectures is to employ joint-learning of the targetregion with an auxiliary task, a method commonly known as multi-task learning.These approaches help impose smoothness and shape priors, which vanilla FCNapproaches do not necessarily incorporate. In this paper, we propose a novelplug-and-play module, which we term as Conv-MCD, which exploits structuralinformation in two ways - i) using the contour map and ii) using the distancemap, both of which can be obtained from ground truth segmentation maps with noadditional annotation costs. The key benefit of our module is the ease of itsaddition to any state-of-the-art architecture, resulting in a significantimprovement in performance with a minimal increase in parameters. Tosubstantiate the above claim, we conduct extensive experiments using 4state-of-the-art architectures across various evaluation metrics, and report asignificant increase in performance in relation to the base networks. Inaddition to the aforementioned experiments, we also perform ablative studiesand visualization of feature maps to further elucidate our approach."}
{"id": "589", "content": "Image segmentation is a primary task in many medical applications. Recently,many deep networks derived from U-Net have been extensively used in variousmedical image segmentation tasks. However, in most of the cases, networkssimilar to U-net produce coarse and non-smooth segmentations with lots ofdiscontinuities. To improve and refine the performance of U-Net like networks,we propose the use of parallel decoders which along with performing the maskpredictions also perform contour prediction and distance map estimation. Thecontour and distance map aid in ensuring smoothness in the segmentationpredictions. To facilitate joint training of three tasks, we propose a novelarchitecture called Psi-Net with a single encoder and three parallel decoders(thus having a shape of $ Psi$), one decoder to learns the segmentation maskprediction and other two decoders to learn the auxiliary tasks of contourdetection and distance map estimation. The learning of these auxiliary taskshelps in capturing the shape and the boundary information. We also propose anew joint loss function for the proposed architecture. The loss functionconsists of a weighted combination of Negative Log likelihood and Mean SquareError loss. We have used two publicly available datasets: 1) Origa dataset forthe task of optic cup and disc segmentation and 2) Endovis segment dataset forthe task of polyp segmentation to evaluate our model. We have conductedextensive experiments using our network to show our model gives better resultsin terms of segmentation, boundary and shape metrics. ", "id2": "588", "id3": "None"}
{"id": "590", "content": "India is an agriculture-dependent country. As we all know that farming is thebackbone of our country it is our responsibility to preserve the crops.However, we cannot stop the destruction of crops by natural calamities at leastwe have to try to protect our crops from diseases. To, detect a plant diseasewe need a fast automatic way. So, this paper presents a model to identify theparticular disease of plant leaves at early stages so that we can prevent ortake a remedy to stop spreading of the disease. This proposed model is madeinto five sessions. Image preprocessing includes the enhancement of the lowlight image done using inception modules in CNN. Low-resolution imageenhancement is done using an Adversarial Neural Network. This also includesConversion of RGB Image to YCrCb color space. Next, this paper presents amethodology for image segmentation which is an important aspect for identifyingthe disease symptoms. This segmentation is done using the genetic algorithm.Due to this process the segmentation of the leaf Image this helps in detectionof the leaf mage automatically and classifying. Texture extraction is doneusing the statistical model called GLCM and finally, the classification of thediseases is done using the SVM using Different Kernels with the high accuracy. ", "id2": "589", "id3": "None"}
{"id": "591", "content": "In order to completely separate objects with large sections of occludedboundaries in an image, we devise a new variational level set model for imagesegmentation combining the Chan-Vese model with elastica and landmarkconstraints. For computational efficiency, we design its Augmented LagrangianMethod (ALM) or Alternating Direction Method of Multiplier (ADMM) method byintroducing some auxiliary variables, Lagrange multipliers, and penaltyparameters. In each loop of alternating iterative optimization, thesub-problems of minimization can be easily solved via the Gauss-Seideliterative method and generalized soft thresholding formulas with projection,respectively. Numerical experiments show that the proposed model can not onlyrecover larger broken boundaries but can also improve segmentation efficiency,as well as decrease the dependence of segmentation on parameter tuning andinitialization. ", "id2": "590", "id3": "None"}
{"id": "592", "content": "Generative Adversarial Networks (GANs) have the capability of synthesizingimages, which have been successfully applied to medical image synthesis tasks.However, most of existing methods merely consider the global contextualinformation and ignore the fine foreground structures, e.g., vessel, skeleton,which may contain diagnostic indicators for medical image analysis. Inspired byhuman painting procedure, which is composed of stroking and color renderingsteps, we propose a Sketching-rendering Unconditional Generative AdversarialNetwork (SkrGAN) to introduce a sketch prior constraint to guide the medicalimage generation. In our SkrGAN, a sketch guidance module is utilized togenerate a high quality structural sketch from random noise, then a colorrender mapping is used to embed the sketch-based representations and resemblethe background appearances. Experimental results show that the proposed SkrGANachieves the state-of-the-art results in synthesizing images for various imagemodalities, including retinal color fundus, X-Ray, Computed Tomography (CT) andMagnetic Resonance Imaging (MRI). In addition, we also show that theperformances of medical image segmentation method have been improved by usingour synthesized images as data augmentation. ", "id2": "591", "id3": "None"}
{"id": "593", "content": "Dense prediction models are widely used for image segmentation. One importantchallenge is to sufficiently train these models to yield good generalizationsfor hard-to-learn pixels. A typical group of such hard-to-learn pixels areboundaries between instances. Many studies have proposed to give specificattention to learning the boundary pixels. They include designing multi-tasknetworks with an additional task of boundary prediction and increasing theweights of boundary pixels predictions in the loss function. Such strategiesrequire defining what to attend beforehand and incorporating this definedattention to the learning model. However, there may exist other groups ofhard-to-learn pixels and manually defining and incorporating the appropriateattention for each group may not be feasible. In order to provide a moreattainable and scalable solution, this paper proposes AttentionBoost, which isa new multi-attention learning model based on adaptive boosting. AttentionBoostdesigns a multi-stage network and introduces a new loss adjustment mechanismfor a dense prediction model to adaptively learn what to attend at each stagedirectly on image data without necessitating any prior definition about what toattend. This mechanism modulates the attention of each stage to correct themistakes of previous stages, by adjusting the loss weight of each pixelprediction separately with respect to how accurate the previous stages are onthis pixel. This mechanism enables AttentionBoost to learn different attentionsfor different pixels at the same stage, according to difficulty of learningthese pixels, as well as multiple attentions for the same pixel at differentstages, according to confidence of these stages on their predictions for thispixel. Using gland segmentation as a showcase application, our experimentsdemonstrate that AttentionBoost improves the results of its counterparts. ", "id2": "592", "id3": "None"}
{"id": "594", "content": "The throughput of electron microscopes has increased significantly in recentyears, enabling detailed analysis of cell morphology and ultrastructure.Analysis of neural circuits at single-synapse resolution remains the flagshiptarget of this technique, but applications to cell and developmental biologyare also starting to emerge at scale. The amount of data acquired in suchstudies makes manual instance segmentation, a fundamental step in many analysispipelines, impossible. While automatic segmentation approaches have improvedsignificantly thanks to the adoption of convolutional neural networks, theiraccuracy still lags behind human annotations and requires additional manualproof-reading. A major hindrance to further improvements is the limited fieldof view of the segmentation networks preventing them from exploiting theexpected cell morphology or other prior biological knowledge which humans useto inform their segmentation decisions. In this contribution, we show how suchdomain-specific information can be leveraged by expressing it as long-rangeinteractions in a graph partitioning problem known as the lifted multicutproblem. Using this formulation, we demonstrate significant improvement insegmentation accuracy for three challenging EM segmentation problems fromneuroscience and cell biology. ", "id2": "593", "id3": "None"}
{"id": "595", "content": "We tackle the problem of graph partitioning for image segmentation usingcorrelation clustering (CC), which we treat as an integer linear program (ILP).We reformulate optimization in the ILP so as to admit efficient optimizationvia Benders decomposition, a classic technique from operations research. OurBenders decomposition formulation has many subproblems, each associated with anode in the CC instances graph, which are solved in parallel. Each Benderssubproblem enforces the cycle inequalities corresponding to the negative weightedges attached to its corresponding node in the CC instance. We generateMagnanti-Wong Benders rows in addition to standard Benders rows, to accelerateoptimization. Our Benders decomposition approach provides a promising newavenue to accelerate optimization for CC, and allows for massiveparallelization. ", "id2": "594", "id3": "None"}
{"id": "596", "content": "Clustering is one of the major roles in data mining that is widelyapplication in pattern recognition and image segmentation. Fuzzy C-means (FCM)is the most used clustering algorithm that proven efficient, fast and easy toimplement, however, FCM uses the Euclidean distance that often leads toclustering errors, especially when handling multidimensional and noisy data. Inthe last few years, many distances metric have been proposed by researchers toimprove the performance of the FCM algorithms, and the majority of researcherspropose weighted distance. In this paper, we proposed Canberra WeightedDistance to improved performance of the FCM algorithm. The experimental resultusing the UCI data set show the proposed method is superior to the originalmethod and other clustering methods. ", "id2": "595", "id3": "None"}
{"id": "597", "content": "Separating and labeling each instance of a nucleus (instance-awaresegmentation) is the key challenge in segmenting single cell nuclei onfluorescence microscopy images. Deep Neural Networks can learn the implicittransformation of a nuclear image into a probability map indicating the classmembership of each pixel (nucleus or background), but the use ofpost-processing steps to turn the probability map into a labeled object mask iserror-prone. This especially accounts for nuclear images of tissue sections andnuclear images across varying tissue preparations. In this work, we aim toevaluate the performance of state-of-the-art deep learning architectures tosegment nuclei in fluorescence images of various tissue origins and samplepreparation types without post-processing. We compare architectures thatoperate on pixel to pixel translation and an architecture that operates onobject detection and subsequent locally applied segmentation. In addition, wepropose a novel strategy to create artificial images to extend the trainingset. We evaluate the influence of ground truth annotation quality, image scaleand segmentation complexity on segmentation performance. Results show thatthree out of four deep learning architectures (U-Net, U-Net with ResNet34backbone, Mask R-CNN) can segment fluorescent nuclear images on most of thesample preparation types and tissue origins with satisfactory segmentationperformance. Mask R-CNN, an architecture designed to address instance awaresegmentation tasks, outperforms other architectures. Equal nuclear mean size,consistent nuclear annotations and the use of artificially generated imagesresult in overall acceptable precision and recall across different tissues andsample preparation types. ", "id2": "596", "id3": "None"}
{"id": "598", "content": "We propose a novel semi-supervised image segmentation method thatsimultaneously optimizes a supervised segmentation and an unsupervisedreconstruction objectives. The reconstruction objective uses an attentionmechanism that separates the reconstruction of image areas corresponding todifferent classes. The proposed approach was evaluated on two applications:brain tumor and white matter hyperintensities segmentation. Our method, trainedon unlabeled and a small number of labeled images, outperformed supervised CNNstrained with the same number of images and CNNs pre-trained on unlabeled data.In ablation experiments, we observed that the proposed attention mechanismsubstantially improves segmentation performance. We explore two multi-tasktraining strategies: joint training and alternating training. Alternatingtraining requires fewer hyperparameters and achieves a better, more stableperformance than joint training. Finally, we analyze the features learned bydifferent methods and find that the attention mechanism helps to learn morediscriminative features in the deeper layers of encoders. ", "id2": "597", "id3": "None"}
{"id": "599", "content": "Deep learning methods have achieved promising performance in many areas, butthey are still struggling with noisy-labeled images during the trainingprocess. Considering that the annotation quality indispensably relies on greatexpertise, the problem is even more crucial in the medical image domain. How toeliminate the disturbance from noisy labels for segmentation tasks withoutfurther annotations is still a significant challenge. In this paper, weintroduce our label quality evaluation strategy for deep neural networksautomatically assessing the quality of each label, which is not explicitlyprovided, and training on clean-annotated ones. We propose a solution fornetwork automatically evaluating the relative quality of the labels in thetraining set and using good ones to tune the network parameters. We also designan overfitting control module to let the network maximally learn from theprecise annotations during the training process. Experiments on the publicbiomedical image segmentation dataset have proved the method outperformsbaseline methods and retains both high accuracy and good generalization atdifferent noise levels. ", "id2": "598", "id3": "None"}
{"id": "600", "content": "Most progress in semantic segmentation reports on daytime images taken underfavorable illumination conditions. We instead address the problem of semanticsegmentation of nighttime images and improve the state-of-the-art, by adaptingdaytime models to nighttime without using nighttime annotations. Moreover, wedesign a new evaluation framework to address the substantial uncertainty ofsemantics in nighttime images. Our central contributions are: 1) a curriculumframework to gradually adapt semantic segmentation models from day to night vialabeled synthetic images and unlabeled real images, both for progressivelydarker times of day, which exploits cross-time-of-day correspondences for thereal images to guide the inference of their labels; 2) a noveluncertainty-aware annotation and evaluation framework and metric for semanticsegmentation, designed for adverse conditions and including image regionsbeyond human recognition capability in the evaluation in a principled fashion;3) the Dark Zurich dataset, which comprises 2416 unlabeled nighttime and 2920unlabeled twilight images with correspondences to their daytime counterpartsplus a set of 151 nighttime images with fine pixel-level annotations createdwith our protocol, which serves as a first benchmark to perform our novelevaluation. Experiments show that our guided curriculum adaptationsignificantly outperforms state-of-the-art methods on real nighttime sets bothfor standard metrics and our uncertainty-aware metric. Furthermore, ouruncertainty-aware evaluation reveals that selective invalidation of predictionscan lead to better results on data with ambiguous content such as our nighttimebenchmark and profit safety-oriented applications which involve invalid inputs. ", "id2": "599", "id3": "None"}
{"id": "601", "content": "Accurate segmentation of the optic disc (OD) and cup (OC)in fundus imagesfrom different datasets is critical for glaucoma disease screening. Thecross-domain discrepancy (domain shift) hinders the generalization of deepneural networks to work on different domain datasets.In this work, we presentan unsupervised domain adaptation framework,called Boundary and Entropy-drivenAdversarial Learning (BEAL), to improve the OD and OC segmentation performance,especially on the ambiguous boundary regions. In particular, our proposed BEALframe-work utilizes the adversarial learning to encourage the boundaryprediction and mask probability entropy map (uncertainty map) of the targetdomain to be similar to the source ones, generating more accurate boundariesand suppressing the high uncertainty predictions of OD and OC segmentation. Weevaluate the proposed BEAL framework on two public retinal fundus imagedatasets (Drishti-GS and RIM-ONE-r3), and the experiment results demonstratethat our method outperforms the state-of-the-art unsupervised domain adaptationmethods. Codes will be available at https://github.com/EmmaW8/BEAL. ", "id2": "600", "id3": "None"}
{"id": "602", "content": "Deep convolutional neural networks (CNNs) are state-of-the-art for semanticimage segmentation, but typically require many labeled training samples.Obtaining 3D segmentations of medical images for supervised training isdifficult and labor intensive. Motivated by classical approaches for jointsegmentation and registration we therefore propose a deep learning frameworkthat jointly learns networks for image registration and image segmentation. Incontrast to previous work on deep unsupervised image registration, which showedthe benefit of weak supervision via image segmentations, our approach can useexisting segmentations when available and computes them via the segmentationnetwork otherwise, thereby providing the same registration benefit. Conversely,segmentation network training benefits from the registration, which essentiallyprovides a realistic form of data augmentation. Experiments on knee and brain3D magnetic resonance (MR) images show that our approach achieves largesimultaneous improvements of segmentation and registration accuracy (overindependently trained networks) and allows training high-quality models withvery limited training data. Specifically, in a one-shot-scenario (with only onemanually labeled image) our approach increases Dice scores (%) over anunsupervised registration network by 2.7 and 1.8 on the knee and brain imagesrespectively. ", "id2": "601", "id3": "None"}
{"id": "603", "content": "Segmentation is a fundamental task in medical image analysis. However, mostexisting methods focus on primary region extraction and ignore edgeinformation, which is useful for obtaining accurate segmentation. In thispaper, we propose a generic medical segmentation method, called Edge-aTtentionguidance Network (ET-Net), which embeds edge-attention representations to guidethe segmentation network. Specifically, an edge guidance module is utilized tolearn the edge-attention representations in the early encoding layers, whichare then transferred to the multi-scale decoding layers, fused using a weightedaggregation module. The experimental results on four segmentation tasks (i.e.,optic disc/cup and vessel segmentation in retinal images, and lung segmentationin chest X-Ray and CT images) demonstrate that preserving edge-attentionrepresentations contributes to the final segmentation accuracy, and ourproposed method outperforms current state-of-the-art segmentation methods. Thesource code of our method is available at https://github.com/ZzzJzzZ/ETNet. ", "id2": "602", "id3": "None"}
{"id": "604", "content": "Shape instantiation which predicts the 3D shape of a dynamic target from oneor more 2D images is important for real-time intra-operative navigation.Previously, a general shape instantiation framework was proposed with manualimage segmentation to generate a 2D Statistical Shape Model (SSM) and withKernel Partial Least Square Regression (KPLSR) to learn the relationshipbetween the 2D and 3D SSM for 3D shape prediction. In this paper, the two-stageshape instantiation is improved to be one-stage. PointOutNet with 19convolutional layers and three fully-connected layers is used as the networkstructure and Chamfer distance is used as the loss function to predict the 3Dtarget point cloud from a single 2D image. With the proposed one-stage shapeinstantiation algorithm, a spontaneous image-to-point cloud training andinference can be achieved. A dataset from 27 Right Ventricle (RV) subjects,indicating 609 experiments, were used to validate the proposed one-stage shapeinstantiation algorithm. An average point cloud-to-point cloud (PC-to-PC) errorof 1.72mm has been achieved, which is comparable to the PLSR-based (1.42mm) andKPLSR-based (1.31mm) two-stage shape instantiation algorithm. ", "id2": "603", "id3": "None"}
{"id": "605", "content": "This work addresses the task of open world semantic segmentation using RGBDsensing to discover new semantic classes over time. Although there are manytypes of objects in the real-word, current semantic segmentation methods make aclosed world assumption and are trained only to segment a limited number ofobject classes. Towards a more open world approach, we propose a novel methodthat incrementally learns new classes for image segmentation. The proposedsystem first segments each RGBD frame using both color and geometricinformation, and then aggregates that information to build a single segmenteddense 3D map of the environment. The segmented 3D map representation is a keycomponent of our approach as it is used to discover new object classes byidentifying coherent regions in the 3D map that have no semantic label. The useof coherent region in the 3D map as a primitive element, rather thantraditional elements such as surfels or voxels, also significantly reduces thecomputational complexity and memory use of our method. It thus leads tosemi-real-time performance at  10.7 Hz when incrementally updating the dense 3Dmap at every frame. Through experiments on the NYUDv2 dataset, we demonstratethat the proposed method is able to correctly cluster objects of both known andunseen classes. We also show the quantitative comparison with thestate-of-the-art supervised methods, the processing time of each step, and theinfluences of each component. ", "id2": "604", "id3": "None"}
{"id": "606", "content": "The task of medical image segmentation commonly involves an imagereconstruction step to convert acquired raw data to images before any analysis.However, noises, artifacts and loss of information due to the reconstructionprocess are almost inevitable, which compromises the final performance ofsegmentation. We present a novel learning framework that performs magneticresonance brain image segmentation directly from k-space data. The end-to-endframework consists of a unique task-driven attention module that recurrentlyutilizes intermediate segmentation estimation to facilitate image-domainfeature extraction from the raw data, thus closely bridging the reconstructionand the segmentation tasks. In addition, to address the challenge of manuallabeling, we introduce a novel workflow to generate labeled training data forsegmentation by exploiting imaging modality simulators and digital phantoms.Extensive experimental results show that the proposed method outperformsseveral state-of-the-art methods. ", "id2": "605", "id3": "None"}
{"id": "607", "content": "Convolutional Neural Network (CNN) based image segmentation has made greatprogress in recent years. However, video object segmentation remains achallenging task due to its high computational complexity. Most of the previousmethods employ a two-stream CNN framework to handle spatial and motion featuresseparately. In this paper, we propose an end-to-end encoder-decoder style 3DCNN to aggregate spatial and temporal information simultaneously for videoobject segmentation. To efficiently process video, we propose 3D separableconvolution for the pyramid pooling module and decoder, which dramaticallyreduces the number of operations while maintaining the performance. Moreover,we also extend our framework to video action segmentation by adding an extraclassifier to predict the action label for actors in videos. Extensiveexperiments on several video datasets demonstrate the superior performance ofthe proposed approach for action and object segmentation compared to thestate-of-the-art. ", "id2": "606", "id3": "None"}
{"id": "608", "content": "Training deep convolutional neural networks usually requires a large amountof labeled data. However, it is expensive and time-consuming to annotate datafor medical image segmentation tasks. In this paper, we present a noveluncertainty-aware semi-supervised framework for left atrium segmentation from3D MR images. Our framework can effectively leverage the unlabeled data byencouraging consistent predictions of the same input under differentperturbations. Concretely, the framework consists of a student model and ateacher model, and the student model learns from the teacher model byminimizing a segmentation loss and a consistency loss with respect to thetargets of the teacher model. We design a novel uncertainty-aware scheme toenable the student model to gradually learn from the meaningful and reliabletargets by exploiting the uncertainty information. Experiments show that ourmethod achieves high performance gains by incorporating the unlabeled data. Ourmethod outperforms the state-of-the-art semi-supervised methods, demonstratingthe potential of our framework for the challenging semi-supervised problems. ", "id2": "607", "id3": "None"}
{"id": "609", "content": "Semantic Segmentation is an important module for autonomous robots such asself-driving cars. The advantage of video segmentation approaches compared tosingle image segmentation is that temporal image information is considered, andtheir performance increases due to this. Hence, single image segmentationapproaches are extended by recurrent units such as convolutional LSTM(convLSTM) cells, which are placed at suitable positions in the basic networkarchitecture. However, a major critique of video segmentation approaches basedon recurrent neural networks is their large parameter count and theircomputational complexity, and so, their inference time of one video frame takesup to 66 percent longer than their basic version. Inspired by the success ofthe spatial and depthwise separable convolutional neural networks, wegeneralize these techniques for convLSTMs in this work, so that the number ofparameters and the required FLOPs are reduced significantly. Experiments ondifferent datasets show that the segmentation approaches using the proposed,modified convLSTM cells achieve similar or slightly worse accuracy, but are upto 15 percent faster on a GPU than the ones using the standard convLSTM cells.Furthermore, a new evaluation metric is introduced, which measures the amountof flickering pixels in the segmented video sequence. ", "id2": "608", "id3": "None"}
{"id": "610", "content": "Over the last two decades, deep learning has transformed the field ofcomputer vision. Deep convolutional networks were successfully applied to learndifferent vision tasks such as image classification, image segmentation, objectdetection and many more. By transferring the knowledge learned by deep modelson large generic datasets, researchers were further able to create fine-tunedmodels for other more specific tasks. Recently this idea was applied forregressing the absolute camera pose from an RGB image. Although the resultingaccuracy was sub-optimal, compared to classic feature-based solutions, thiseffort led to a surge of learning-based pose estimation methods. Here, wereview deep learning approaches for camera pose estimation. We describe keymethods in the field and identify trends aiming at improving the original deeppose regression solution. We further provide an extensive cross-comparison ofexisting learning-based pose estimators, together with practical notes on theirexecution for reproducibility purposes. Finally, we discuss emerging solutionsand potential future research directions. ", "id2": "609", "id3": "None"}
{"id": "611", "content": "In this paper, an stereo-based traversability analysis approach for allterrains in off-road mobile robotics, e.g. Unmanned Ground Vehicles (UGVs) isproposed. This approach reformulates the problem of terrain traversabilityanalysis into two main problems: (1) 3D terrain reconstruction and (2) terrainall surfaces detection and analysis. The proposed approach is using stereocamera for perception and 3D reconstruction of the terrain. In order to detectall the existing surfaces in the 3D reconstructed terrain as superpixelsurfaces (i.e. segments), an image segmentation technique is applied usinggeometry-based features (pixel-based surface normals). Having detected all thesurfaces, Superpixel Surface Traversability Analysis approach (SSTA) is appliedon all of the detected surfaces (superpixel segments) in order to classify thembased on their traversability index. The proposed SSTA approach is based on:(1) Superpixel surface normal and plane estimation, (2) Traversability analysisusing superpixel surface planes. Having analyzed all the superpixel surfacesbased on their traversability, these surfaces are finally classified into fivemain categories as following: traversable, semi-traversable, non-traversable,unknown and undecided. ", "id2": "610", "id3": "None"}
{"id2": 1060, "id3": "610", "content": "In this paper, an stereo-based traversability analysis approach for allterrains in off-road mobile robotics, e.g. Unmanned Ground Vehicles (UGVs) isproposed. This approach reformulates the problem of terrain traversabilityanalysis into two main problems: (1) 3D terrain reconstruction and (2) terrainall surfaces detection and analysis. The proposed approach is using stereocamera for perception and 3D reconstruction of the terrain. In order to detectall the existing surfaces in the 3D reconstructed terrain as superpixelsurfaces (i.e. segments), an image segmentation technique is applied usinggeometry-based features (pixel-based surface normals). Having detected all thesurfaces, Superpixel Surface Traversability Analysis approach (SSTA) is appliedon all of the detected surfaces (superpixel segments) in order to classify thembased on their traversability index. The proposed SSTA approach is based on:(1) Superpixel surface normal and plane estimation, (2) Traversability analysisusing superpixel surface planes. Having analyzed all the superpixel surfacesbased on their traversability, these surfaces are finally classified into fivemain categories as following: traversable, semi-traversable, non-traversable,unknown and undecided."}
{"id": "612", "content": "To improve the classification performance in the context of hyperspectralimage processing, many works have been developed based on two commonstrategies, namely the spatial-spectral information integration and theutilization of neural networks. However, both strategies typically require moretraining data than the classical algorithms, aggregating the shortage oflabeled samples. In this letter, we propose a novel framework that organicallycombines the spectrum-based deep metric learning model and the conditionalrandom field algorithm. The deep metric learning model is supervised by thecenter loss to produce spectrum-based features that gather more tightly inEuclidean space within classes. The conditional random field with Gaussian edgepotentials, which is firstly proposed for image segmentation tasks, isintroduced to give the pixel-wise classification over the hyperspectral imageby utilizing both the geographical distances between pixels and the Euclideandistances between the features produced by the deep metric learning model. Theproposed framework is trained by spectral pixels at the deep metric learningstage and utilizes the half handcrafted spatial features at the conditionalrandom field stage. This settlement alleviates the shortage of training data tosome extent. Experiments on two real hyperspectral images demonstrate theadvantages of the proposed method in terms of both classification accuracy andcomputation cost. ", "id2": "611", "id3": "None"}
{"id2": 1061, "id3": "611", "content": "To improve the classification performance in the context of hyperspectralimage processing, many works have been developed based on two commonstrategies, namely the spatial-spectral information integration and theutilization of neural networks. However, both strategies typically require moretraining data than the classical algorithms, aggregating the shortage oflabeled samples. In this letter, we propose a novel framework that organicallycombines the spectrum-based deep metric learning model and the conditionalrandom field algorithm. The deep metric learning model is supervised by thecenter loss to produce spectrum-based features that gather more tightly inEuclidean space within classes. The conditional random field with Gaussian edgepotentials, which is firstly proposed for image segmentation tasks, isintroduced to give the pixel-wise classification over the hyperspectral imageby utilizing both the geographical distances between pixels and the Euclideandistances between the features produced by the deep metric learning model. Theproposed framework is trained by spectral pixels at the deep metric learningstage and utilizes the half handcrafted spatial features at the conditionalrandom field stage. This settlement alleviates the shortage of training data tosome extent. Experiments on two real hyperspectral images demonstrate theadvantages of the proposed method in terms of both classification accuracy andcomputation cost."}
{"id": "613", "content": "Deep neural networks have achieved tremendous success in various fieldsincluding medical image segmentation. However, they have long been criticizedfor being a black-box, in that interpretation, understanding and correctingarchitectures is difficult as there is no general theory for deep neuralnetwork design. Previously, precision learning was proposed to fuse deeparchitectures and traditional approaches. Deep networks constructed in this waybenefit from the original known operator, have fewer parameters, and improvedinterpretability. However, they do not yield state-of-the-art performance inall applications. In this paper, we propose to analyze deep networks usingknown operators, by adopting a divide-and-conquer strategy to replace networkcomponents, whilst retaining its performance. The task of retinal vesselsegmentation is investigated for this purpose. We start with a high-performanceU-Net and show by step-by-step conversion that we are able to divide thenetwork into modules of known operators. The results indicate that acombination of a trainable guided filter and a trainable version of the Frangifilter yields a performance at the level of U-Net (AUC 0.974 vs. 0.972) with atremendous reduction in parameters (111,536 vs. 9,575). In addition, thetrained layers can be mapped back into their original algorithmicinterpretation and analyzed using standard tools of signal processing. ", "id2": "612", "id3": "None"}
{"id": "614", "content": "Deep learning approaches have achieved state-of-the-art performance incardiac magnetic resonance (CMR) image segmentation. However, most approacheshave focused on learning image intensity features for segmentation, whereas theincorporation of anatomical shape priors has received less attention. In thispaper, we combine a multi-task deep learning approach with atlas propagation todevelop a shape-constrained bi-ventricular segmentation pipeline for short-axisCMR volumetric images. The pipeline first employs a fully convolutional network(FCN) that learns segmentation and landmark localisation tasks simultaneously.The architecture of the proposed FCN uses a 2.5D representation, thus combiningthe computational advantage of 2D FCNs networks and the capability ofaddressing 3D spatial consistency without compromising segmentation accuracy.Moreover, the refinement step is designed to explicitly enforce a shapeconstraint and improve segmentation quality. This step is effective forovercoming image artefacts (e.g. due to different breath-hold positions andlarge slice thickness), which preclude the creation of anatomically meaningful3D cardiac shapes. The proposed pipeline is fully automated, due to networksability to infer landmarks, which are then used downstream in the pipeline toinitialise atlas propagation. We validate the pipeline on 1831 healthy subjectsand 649 subjects with pulmonary hypertension. Extensive numerical experimentson the two datasets demonstrate that our proposed method is robust and capableof producing accurate, high-resolution and anatomically smooth bi-ventricular3D models, despite the artefacts in input CMR volumes. ", "id2": "613", "id3": "None"}
{"id": "615", "content": "The machine learning community has been overwhelmed by a plethora of deeplearning based approaches. Many challenging computer vision tasks such asdetection, localization, recognition and segmentation of objects inunconstrained environment are being efficiently addressed by various types ofdeep neural networks like convolutional neural networks, recurrent networks,adversarial networks, autoencoders and so on. While there have been plenty ofanalytical studies regarding the object detection or recognition domain, manynew deep learning techniques have surfaced with respect to image segmentationtechniques. This paper approaches these various deep learning techniques ofimage segmentation from an analytical perspective. The main goal of this workis to provide an intuitive understanding of the major techniques that has madesignificant contribution to the image segmentation domain. Starting from someof the traditional image segmentation approaches, the paper progressesdescribing the effect deep learning had on the image segmentation domain.Thereafter, most of the major segmentation algorithms have been logicallycategorized with paragraphs dedicated to their unique contribution. With anample amount of intuitive explanations, the reader is expected to have animproved ability to visualize the internal dynamics of these processes. ", "id2": "614", "id3": "None"}
{"id": "616", "content": "Current state-of-the-art methods for image segmentation form a dense imagerepresentation where the color, shape and texture information are all processedtogether inside a deep CNN. This however may not be ideal as they contain verydifferent type of information relevant for recognition. Here, we propose a newtwo-stream CNN architecture for semantic segmentation that explicitly wiresshape information as a separate processing branch, i.e. shape stream, thatprocesses information in parallel to the classical stream. Key to thisarchitecture is a new type of gates that connect the intermediate layers of thetwo streams. Specifically, we use the higher-level activations in the classicalstream to gate the lower-level activations in the shape stream, effectivelyremoving noise and helping the shape stream to only focus on processing therelevant boundary-related information. This enables us to use a very shallowarchitecture for the shape stream that operates on the image-level resolution.Our experiments show that this leads to a highly effective architecture thatproduces sharper predictions around object boundaries and significantly boostsperformance on thinner and smaller objects. Our method achievesstate-of-the-art performance on the Cityscapes benchmark, in terms of both mask(mIoU) and boundary (F-score) quality, improving by 2% and 4% over strongbaselines. ", "id2": "615", "id3": "None"}
{"id": "617", "content": "Semantic segmentation is a crucial task in biomedical image processing, whichrecent breakthroughs in deep learning have allowed to improve. However, deeplearning methods in general are not yet widely used in practice since theyrequire large amount of data for training complex models. This is particularlychallenging for biomedical images, because data and ground truths are a scarceresource. Annotation efforts for biomedical images come with a real cost, sinceexperts have to manually label images at pixel-level on samples usuallycontaining many instances of the target anatomy (e.g. in histology samples:neurons, astrocytes, mitochondria, etc.). In this paper we provide a frameworkfor Deep Active Learning applied to a real-world scenario. Our framework relieson the U-Net architecture and overall uncertainty measure to suggest whichsample to annotate. It takes advantage of the uncertainty measure obtained bytaking Monte Carlo samples while using Dropout regularization scheme.Experiments were done on spinal cord and brain microscopic histology samples toperform a myelin segmentation task. Two realistic small datasets of 14 and 24images were used, from different acquisition settings (Serial Block-FaceElectron Microscopy and Transmitting Electron Microscopy) and showed that ourmethod reached a maximum Dice value after adding 3 uncertainty-selected samplesto the initial training set, versus 15 randomly-selected samples, therebysignificantly reducing the annotation effort. We focused on a plausiblescenario and showed evidence that this straightforward implementation achievesa high segmentation performance with very few labelled samples. We believe ourframework may benefit any biomedical researcher willing to obtain fast andaccurate image segmentation on their own dataset. The code is freely availableat https://github.com/neuropoly/deep-active-learning. ", "id2": "616", "id3": "None"}
{"id": "618", "content": "Although numerous improvements have been made in the field of imagesegmentation using convolutional neural networks, the majority of theseimprovements rely on training with larger datasets, model architecturemodifications, novel loss functions, and better optimizers. In this paper, wepropose a new segmentation performance boosting paradigm that relies onoptimally modifying the networks input instead of the network itself. Inparticular, we leverage the gradients of a trained segmentation network withrespect to the input to transfer it to a space where the segmentation accuracyimproves. We test the proposed method on three publicly available medical imagesegmentation datasets: the ISIC 2017 Skin Lesion Segmentation dataset, theShenzhen Chest X-Ray dataset, and the CVC-ColonDB dataset, for which our methodachieves improvements of 5.8%, 0.5%, and 4.8% in the average Dice scores,respectively. ", "id2": "617", "id3": "None"}
{"id": "619", "content": "Selective segmentation involves incorporating user input to partition animage into foreground and background, by discriminating between objects of asimilar type. Typically, such methods involve introducing additionalconstraints to generic segmentation approaches. However, we show that this isoften inconsistent with respect to common assumptions about the image. Theproposed method introduces a new fitting term that is more useful in practicethan the Chan-Vese framework. In particular, the idea is to define a term thatallows for the background to consist of multiple regions of inhomogeneity. Weprovide comparitive experimental results to alternative approaches todemonstrate the advantages of the proposed method, broadening the possibleapplication of these methods. ", "id2": "618", "id3": "None"}
{"id": "620", "content": "In the recent years, convolutional neural networks have transformed the fieldof medical image analysis due to their capacity to learn discriminative imagefeatures for a variety of classification and regression tasks. However,successfully learning these features requires a large amount of manuallyannotated data, which is expensive to acquire and limited by the availableresources of expert image analysts. Therefore, unsupervised, weakly-supervisedand self-supervised feature learning techniques receive a lot of attention,which aim to utilise the vast amount of available data, while at the same timeavoid or substantially reduce the effort of manual annotation. In this paper,we propose a novel way for training a cardiac MR image segmentation network, inwhich features are learnt in a self-supervised manner by predicting anatomicalpositions. The anatomical positions serve as a supervisory signal and do notrequire extra manual annotation. We demonstrate that this seemingly simple taskprovides a strong signal for feature learning and with self-supervisedlearning, we achieve a high segmentation accuracy that is better than orcomparable to a U-net trained from scratch, especially at a small data setting.When only five annotated subjects are available, the proposed method improvesthe mean Dice metric from 0.811 to 0.852 for short-axis image segmentation,compared to the baseline U-net. ", "id2": "619", "id3": "None"}
{"id": "621", "content": "With the recent advances in complex networks theory, graph-based techniquesfor image segmentation has attracted great attention recently. In order tosegment the image into meaningful connected components, this paper proposes animage segmentation general framework using complex networks based communitydetection algorithms. If we consider regions as communities, using communitydetection algorithms directly can lead to an over-segmented image. To addressthis problem, we start by splitting the image into small regions using aninitial segmentation. The obtained regions are used for building the complexnetwork. To produce meaningful connected components and detect homogeneouscommunities, some combinations of color and texture based features are employedin order to quantify the regions similarities. To sum up, the network ofregions is constructed adaptively to avoid many small regions in the image, andthen, community detection algorithms are applied on the resulting adaptivesimilarity matrix to obtain the final segmented image. Experiments areconducted on Berkeley Segmentation Dataset and four of the most influentialcommunity detection algorithms are tested. Experimental results have shown thatthe proposed general framework increases the segmentation performances comparedto some existing methods. ", "id2": "620", "id3": "None"}
{"id": "622", "content": "In this paper we test the use of a deep learning approach to automaticallycount Wandering Albatrosses in Very High Resolution (VHR) satellite imagery. Weuse a dataset of manually labelled imagery provided by the British AntarcticSurvey to train and develop our methods. We employ a U-Net architecture,designed for image segmentation, to simultaneously classify and localisepotential albatrosses. We aid training with the use of the Focal Losscriterion, to deal with extreme class imbalance in the dataset. Initial resultsachieve peak precision and recall values of approximately 80%. Finally weassess the models performance in relation to inter-observer variation, bycomparing errors against an image labelled by multiple observers. We concludemodel accuracy falls within the range of human counters. We hope that themethods will streamline the analysis of VHR satellite images, enabling morefrequent monitoring of a species which is of high conservation concern. ", "id2": "621", "id3": "None"}
{"id": "623", "content": "Extracting texts of various size and shape from images containing multipleobjects is an important problem in many contexts, especially, in connection toe-commerce, augmented reality assistance system in natural scene, etc. Theexisting works (based on only CNN) often perform sub-optimally when the imagecontains regions of high entropy having multiple objects. This paper presentsan end-to-end text detection strategy combining a segmentation algorithm and anensemble of multiple text detectors of different types to detect text in everyindividual image segments independently. The proposed strategy involves asuper-pixel based image segmenter which splits an image into multiple regions.A convolutional deep neural architecture is developed which works on each ofthe segments and detects texts of multiple shapes, sizes, and structures. Itoutperforms the competing methods in terms of coverage in detecting texts inimages especially the ones where the text of various types and sizes arecompacted in a small region along with various other objects. Furthermore, theproposed text detection method along with a text recognizer outperforms theexisting state-of-the-art approaches in extracting text from high entropyimages. We validate the results on a dataset consisting of product images on ane-commerce website. ", "id2": "622", "id3": "None"}
{"id": "624", "content": "Advances in the image-based diagnostics of complex biological andmanufacturing processes have brought unsupervised image segmentation to theforefront of enabling automated, on the fly decision making. However, mostexisting unsupervised segmentation approaches are either computationallycomplex or require manual parameter selection (e.g., flow capacities inmax-flow/min-cut segmentation). In this work, we present a fully unsupervisedsegmentation approach using a continuous max-flow formulation over the imagedomain while optimally estimating the flow parameters from the imagecharacteristics. More specifically, we show that the maximum a posterioriestimate of the image labels can be formulated as a continuous max-flow problemgiven the flow capacities are known. The flow capacities are then iterativelyobtained by employing a novel Markov random field prior over the image domain.We present theoretical results to establish the posterior consistency of theflow capacities. We compare the performance of our approach on two real-worldcase studies including brain tumor image segmentation and defect identificationin additively manufactured components using electron microscopic images.Comparative results with several state-of-the-art supervised as well asunsupervised methods suggest that the present method performs statisticallysimilar to the supervised methods, but results in more than 90% improvement inthe Dice score when compared to the state-of-the-art unsupervised methods. ", "id2": "623", "id3": "None"}
{"id2": 1062, "id3": "623", "content": "Advances in the image-based diagnostics of complex biological andmanufacturing processes have brought unsupervised image segmentation to theforefront of enabling automated, on the fly decision making. However, mostexisting unsupervised segmentation approaches are either computationallycomplex or require manual parameter selection (e.g., flow capacities inmax-flow/min-cut segmentation). In this work, we present a fully unsupervisedsegmentation approach using a continuous max-flow formulation over the imagedomain while optimally estimating the flow parameters from the imagecharacteristics. More specifically, we show that the maximum a posterioriestimate of the image labels can be formulated as a continuous max-flow problemgiven the flow capacities are known. The flow capacities are then iterativelyobtained by employing a novel Markov random field prior over the image domain.We present theoretical results to establish the posterior consistency of theflow capacities. We compare the performance of our approach on two real-worldcase studies including brain tumor image segmentation and defect identificationin additively manufactured components using electron microscopic images.Comparative results with several state-of-the-art supervised as well asunsupervised methods suggest that the present method performs statisticallysimilar to the supervised methods, but results in more than 90% improvement inthe Dice score when compared to the state-of-the-art unsupervised methods."}
{"id": "625", "content": "Convolutional neural networks (CNNs) show outstanding performance in manyimage processing problems, such as image recognition, object detection andimage segmentation. Semantic segmentation is a very challenging task thatrequires recognizing, understanding whats in the image in pixel level. Thoughthe state of the art has been greatly improved by CNNs, there is no explicitconnections between prediction of neighbouring pixels. That is, spatialregularity of the segmented objects is still a problem for CNNs. In this paper,we propose a method to add spatial regularization to the segmented objects. Inour method, the spatial regularization such as total variation (TV) can beeasily integrated into CNN network. It can help CNN find a better local optimumand make the segmentation results more robust to noise. We apply our proposedmethod to Unet and Segnet, which are well established CNNs for imagesegmentation, and test them on WBC, CamVid and SUN-RGBD datasets, respectively.The results show that the regularized networks not only could provide bettersegmentation results with regularization effect than the original ones but alsohave certain robustness to noise. ", "id2": "624", "id3": "None"}
{"id": "626", "content": "Deep learning has been used as a powerful tool for various tasks in computervision, such as image segmentation, object recognition and data generation. Akey part of end-to-end training is designing the appropriate encoder to extractspecific features from the input data. However, few encoders maintain thetopological properties of data, such as connection structures and globalcontours. In this paper, we introduce a Voronoi Diagram encoder based on convexset distance (CSVD) and apply it in edge encoding. The boundaries of Voronoicells is related to detected edges of structures and contours. The CSVD modelimproves contour extraction in CNN and structure generation in GAN. We alsoshow the experimental results and demonstrate that the proposed model has greatpotentiality in different visual problems where topology information should beinvolved. ", "id2": "625", "id3": "None"}
{"id": "627", "content": "To evaluate their performance, existing dehazing approaches generally rely ondistance measures between the generated image and its corresponding groundtruth. Despite its ability to produce visually good images, using pixel-basedor even perceptual metrics do not guarantee, in general, that the producedimage is fit for being used as input for low-level computer vision tasks suchas segmentation. To overcome this weakness, we are proposing a novel end-to-endapproach for image dehazing, fit for being used as input to an imagesegmentation procedure, while maintaining the visual quality of the generatedimages. Inspired by the success of Generative Adversarial Networks (GAN), wepropose to optimize the generator by introducing a discriminator network and aloss function that evaluates segmentation quality of dehazed images. Inaddition, we make use of a supplementary loss function that verifies that thevisual and the perceptual quality of the generated image are preserved in hazyconditions. Results obtained using the proposed technique are appealing, with afavorable comparison to state-of-the-art approaches when considering theperformance of segmentation algorithms on the hazy images. ", "id2": "626", "id3": "None"}
{"id": "628", "content": "Semantic segmentation is essentially important to biomedical image analysis.Many recent works mainly focus on integrating the Fully Convolutional Network(FCN) architecture with sophisticated convolution implementation and deepsupervision. In this paper, we propose to decompose the single segmentationtask into three subsequent sub-tasks, including (1) pixel-wise imagesegmentation, (2) prediction of the class labels of the objects within theimage, and (3) classification of the scene the image belonging to. While thesethree sub-tasks are trained to optimize their individual loss functions ofdifferent perceptual levels, we propose to let them interact by the task-taskcontext ensemble. Moreover, we propose a novel sync-regularization to penalizethe deviation between the outputs of the pixel-wise segmentation and the classprediction tasks. These effective regularizations help FCN utilize contextinformation comprehensively and attain accurate semantic segmentation, eventhough the number of the images for training may be limited in many biomedicalapplications. We have successfully applied our framework to three diverse 2D/3Dmedical image datasets, including Robotic Scene Segmentation Challenge 18(ROBOT18), Brain Tumor Segmentation Challenge 18 (BRATS18), and Retinal FundusGlaucoma Challenge (REFUGE18). We have achieved top-tier performance in allthree challenges. ", "id2": "627", "id3": "None"}
{"id": "629", "content": "One of the key drawbacks of 3D convolutional neural networks for segmentationis their memory footprint, which necessitates compromises in the networkarchitecture in order to fit into a given memory budget. Motivated by theRevNet for image classification, we propose a partially reversible U-Netarchitecture that reduces memory consumption substantially. The reversiblearchitecture allows us to exactly recover each layers outputs from thesubsequent layers ones, eliminating the need to store activations forbackpropagation. This alleviates the biggest memory bottleneck and enables verydeep (theoretically infinitely deep) 3D architectures. On the BraTS challengedataset, we demonstrate substantial memory savings. We further show that thefreed memory can be used for processing the whole field-of-view (FOV) insteadof patches. Increasing network depth led to higher segmentation accuracy whilegrowing the memory footprint only by a very small fraction, thanks to thepartially reversible architecture. ", "id2": "628", "id3": "None"}
{"id": "630", "content": "This paper presents a novel unsupervised domain adaptation framework, calledSynergistic Image and Feature Adaptation (SIFA), to effectively tackle theproblem of domain shift. Domain adaptation has become an important and hottopic in recent studies on deep learning, aiming to recover performancedegradation when applying the neural networks to new testing domains. Ourproposed SIFA is an elegant learning diagram which presents synergistic fusionof adaptations from both image and feature perspectives. In particular, wesimultaneously transform the appearance of images across domains and enhancedomain-invariance of the extracted features towards the segmentation task. Thefeature encoder layers are shared by both perspectives to grasp their mutualbenefits during the end-to-end learning procedure. Without using any annotationfrom the target domain, the learning of our unified model is guided byadversarial losses, with multiple discriminators employed from various aspects.We have extensively validated our method with a challenging application ofcross-modality medical image segmentation of cardiac structures. Experimentalresults demonstrate that our SIFA model recovers the degraded performance from17.2% to 73.0%, and outperforms the state-of-the-art methods by a significantmargin. ", "id2": "629", "id3": "None"}
{"id": "631", "content": "Deep learning based models, generally, require a large number of samples forappropriate training, a requirement that is difficult to satisfy in the medicalfield. This issue can usually be avoided with a proper initialization of theweights. On the task of medical image segmentation in general, two techniquesare oftentimes employed to tackle the training of a deep network $f_T$. Thefirst one consists in reusing some weights of a network $f_S$ pre-trained on alarge scale database ($e.g.$ ImageNet). This procedure, also known as$transfer$ $learning$, happens to reduce the flexibility when it comes to newnetwork design since $f_T$ is constrained to match some parts of $f_S$. Thesecond commonly used technique consists in working on image patches to benefitfrom the large number of available patches. This paper brings together thesetwo techniques and propose to train $arbitrarily$ $designed$ $networks$ thatsegment an image in one forward pass, with a focus on relatively smalldatabases. An experimental work have been carried out on the tasks of retinalblood vessel segmentation and the optic disc one, using four publicly availabledatabases. Furthermore, three types of network are considered, going from avery light weighted network to a densely connected one. The final results showthe efficiency of the proposed framework along with state of the art results onall the databases. ", "id2": "630", "id3": "None"}
{"id": "632", "content": "3D image segmentation is one of the most important and ubiquitous problems inmedical image processing. It provides detailed quantitative analysis foraccurate disease diagnosis, abnormal detection, and classification. Currentlydeep learning algorithms are widely used in medical image segmentation, mostalgorithms trained models with full annotated datasets. However, obtainingmedical image datasets is very difficult and expensive, and full annotation of3D medical image is a monotonous and time-consuming work. Partially labellinginformative slices in 3D images will be a great relief of manual annotation.Sample selection strategies based on active learning have been proposed in thefield of 2D image, but few strategies focus on 3D images. In this paper, wepropose a sparse annotation strategy based on attention-guided active learningfor 3D medical image segmentation. Attention mechanism is used to improvesegmentation accuracy and estimate the segmentation accuracy of each slice. Thecomparative experiments with three different strategies using datasets from thedeveloping human connectome project (dHCP) show that, our strategy only needs15% to 20% annotated slices in brain extraction task and 30% to 35% annotatedslices in tissue segmentation task to achieve comparative results as fullannotation. ", "id2": "631", "id3": "None"}
{"id": "633", "content": "In this paper, a neural architecture search (NAS) framework is proposed for3D medical image segmentation, to automatically optimize a neural architecturefrom a large design space. Our NAS framework searches the structure of eachlayer including neural connectivities and operation types in both of theencoder and decoder. Since optimizing over a large discrete architecture spaceis difficult due to high-resolution 3D medical images, a novel stochasticsampling algorithm based on a continuous relaxation is also proposed forscalable gradient based optimization. On the 3D medical image segmentationtasks with a benchmark dataset, an automatically designed architecture by theproposed NAS framework outperforms the human-designed 3D U-Net, and moreoverthis optimized architecture is well suited to be transferred for differenttasks. ", "id2": "632", "id3": "None"}
{"id": "634", "content": "Segmentation algorithms are prone to make topological errors on fine-scalestructures, e.g., broken connections. We propose a novel method that learns tosegment with correct topology. In particular, we design a continuous-valuedloss function that enforces a segmentation to have the same topology as theground truth, i.e., having the same Betti number. The proposedtopology-preserving loss function is differentiable and we incorporate it intoend-to-end training of a deep neural network. Our method achieves much betterperformance on the Betti number error, which directly accounts for thetopological correctness. It also performs superiorly on other topology-relevantmetrics, e.g., the Adjusted Rand Index and the Variation of Information. Weillustrate the effectiveness of the proposed method on a broad spectrum ofnatural and biomedical datasets. ", "id2": "633", "id3": "None"}
{"id": "635", "content": "Recent advances in deep learning for medical image segmentation demonstrateexpert-level accuracy. However, in clinically realistic environments, suchmethods have marginal performance due to differences in image domains,including different imaging protocols, device vendors and patient populations.Here we consider the problem of domain generalization, when a model is trainedonce, and its performance generalizes to unseen domains. Intuitively, within aspecific medical imaging modality the domain differences are smaller relativeto natural images domain variability. We rethink data augmentation for medical3D images and propose a deep stacked transformations (DST) approach for domaingeneralization. Specifically, a series of n stacked transformations are appliedto each image in each mini-batch during network training to account for thecontribution of domain-specific shifts in medical images. We comprehensivelyevaluate our method on three tasks: segmentation of whole prostate from 3D MRI,left atrial from 3D MRI, and left ventricle from 3D ultrasound. We demonstratethat when trained on a small source dataset, (i) on average, DST models onunseen datasets degrade only by 11% (Dice score change), compared to theconventional augmentation (degrading 39%) and CycleGAN-based domain adaptationmethod (degrading 25%); (ii) when evaluation on the same domain, DST is alsobetter albeit only marginally. (iii) When training on large-sized data, DST onunseen domains reaches performance of state-of-the-art fully supervised models.These findings establish a strong benchmark for the study of domaingeneralization in medical imaging, and can be generalized to the design ofrobust deep segmentation models for clinical deployment. ", "id2": "634", "id3": "None"}
{"id": "636", "content": "Adversarial learning has been proven to be effective for capturing long-rangeand high-level label consistencies in semantic segmentation. Unique to medicalimaging, capturing 3D semantics in an effective yet computationally efficientway remains an open problem. In this study, we address this computationalburden by proposing a novel projective adversarial network, called PAN, whichincorporates high-level 3D information through 2D projections. Furthermore, weintroduce an attention module into our framework that helps for a selectiveintegration of global information directly from our segmentor to ouradversarial network. For the clinical application we chose pancreassegmentation from CT scans. Our proposed framework achieved state-of-the-artperformance without adding to the complexity of the segmentor. ", "id2": "635", "id3": "None"}
{"id": "637", "content": "Encoder-decoder architectures are widely adopted for medical imagesegmentation tasks. With the lateral skip connection, the models can obtain andfuse both semantic and resolution information in deep layers to achieve moreaccurate segmentation performance. However, in many applications (e.g., blurryboundary images), these models often cannot precisely locate complex boundariesand segment tiny isolated parts. To solve this challenging problem, we firstlyanalyze why simple skip connections are not enough to help accurately locateindistinct boundaries and argue that it is due to the fuzzy information in theskip connection provided in the encoder layers. Then we propose asemantic-guided encoder feature learning strategy to learn both high resolutionand rich semantic encoder features so that we can more accurately locate theblurry boundaries, which can also enhance the network by selectively learningdiscriminative features. Besides, we further propose a soft contour constraintmechanism to model the blurry boundary detection. Experimental results on realclinical datasets show that our proposed method can achieve state-of-the-artsegmentation accuracy, especially for the blurry regions. Further analysis alsoindicates that our proposed network components indeed contribute to theimprovement of performance. Experiments on additional datasets validate thegeneralization ability of our proposed method. ", "id2": "636", "id3": "None"}
{"id": "638", "content": "Real-time semantic image segmentation on platforms subject to size, weightand power (SWaP) constraints is a key area of interest for air surveillance andinspection. In this work, we propose MAVNet: a small, light-weight, deep neuralnetwork for real-time semantic segmentation on micro Aerial Vehicles (MAVs).MAVNet, inspired by ERFNet, features 400 times fewer parameters and achievescomparable performance with some reference models in empirical experiments. Ourmodel achieves a trade-off between speed and accuracy, achieving up to 48 FPSon an NVIDIA 1080Ti and 9 FPS on the NVIDIA Jetson Xavier when processing highresolution imagery. Additionally, we provide two novel datasets that representchallenges in semantic segmentation for real-time MAV tracking andinfrastructure inspection tasks and verify MAVNet on these datasets. Ouralgorithm and datasets are made publicly available. ", "id2": "637", "id3": "None"}
{"id": "639", "content": "In this work, we explore the issue of the inter-annotator agreement fortraining and evaluating automated segmentation of skin lesions. We explore whatdifferent degrees of agreement represent, and how they affect different usecases for segmentation. We also evaluate how conditioning the ground truthsusing different (but very simple) algorithms may help to enhance agreement andmay be appropriate for some use cases. The segmentation of skin lesions is acornerstone task for automated skin lesion analysis, useful both as anend-result to locate/detect the lesions and as an ancillary task for lesionclassification. Lesion segmentation, however, is a very challenging task, duenot only to the challenge of image segmentation itself but also to thedifficulty in obtaining properly annotated data. Detecting accurately theborders of lesions is challenging even for trained humans, since, for manylesions, those borders are fuzzy and ill-defined. Using lesions and annotationsfrom the ISIC Archive, we estimate inter-annotator agreement for skin-lesionsegmentation and propose several simple procedures that may help to improveinter-annotator agreement if used to condition the ground truths. ", "id2": "638", "id3": "None"}
{"id": "640", "content": "Volumetric image segmentation with convolutional neural networks (CNNs)encounters several challenges, which are specific to medical images. Amongthese challenges are large volumes of interest, high class imbalances, anddifficulties in learning shape representations. To tackle these challenges, wepropose to improve over traditional CNN-based volumetric image segmentationthrough point-wise classification of point clouds. The sparsity of point cloudsallows processing of entire image volumes, balancing highly imbalancedsegmentation problems, and explicitly learning an anatomical shape. We buildupon PointCNN, a neural network proposed to process point clouds, and proposehere to jointly encode shape and volumetric information within the point cloudin a compact and computationally effective manner. We demonstrate how thisapproach can then be used to refine CNN-based segmentation, which yieldssignificantly improved results in our experiments on the difficult task ofperipheral nerve segmentation from magnetic resonance neurography images. Bysynthetic experiments, we further show the capability of our approach inlearning an explicit anatomical shape representation. ", "id2": "639", "id3": "None"}
{"id": "641", "content": "Deep learning (DL) approaches are state-of-the-art for many medical imagesegmentation tasks. They offer a number of advantages: they can be trained forspecific tasks, computations are fast at test time, and segmentation quality istypically high. In contrast, previously popular multi-atlas segmentation (MAS)methods are relatively slow (as they rely on costly registrations) and eventhough sophisticated label fusion strategies have been proposed, DL approachesgenerally outperform MAS. In this work, we propose a DL-based label fusionstrategy (VoteNet) which locally selects a set of reliable atlases whose labelsare then fused via plurality voting. Experiments on 3D brain MRI data show thatby selecting a good initial atlas set MAS with VoteNet significantlyoutperforms a number of other label fusion strategies as well as a direct DLsegmentation approach. We also provide an experimental analysis of the upperperformance bound achievable by our method. While unlikely achievable inpractice, this bound suggests room for further performance improvements.Lastly, to address the runtime disadvantage of standard MAS, all our resultsmake use of a fast DL registration approach. ", "id2": "640", "id3": "None"}
{"id": "642", "content": "Optical coherence tomography (OCT) is a non-invasive imaging modality whichis widely used in clinical ophthalmology. OCT images are capable of visualizingdeep retinal layers which is crucial for early diagnosis of retinal diseases.In this paper, we describe a comprehensive open-access database containing morethan 500 highresolution images categorized into different pathologicalconditions. The image classes include Normal (NO), Macular Hole (MH),Age-related Macular Degeneration (AMD), Central Serous Retinopathy (CSR), andDiabetic Retinopathy (DR). The images were obtained from a raster scan protocolwith a 2mm scan length and 512x1024 pixel resolution. We have also included 25normal OCT images with their corresponding ground truth delineations which canbe used for an accurate evaluation of OCT image segmentation. In addition, wehave provided a user-friendly GUI which can be used by clinicians for manual(and semi-automated) segmentation. ", "id2": "641", "id3": "None"}
{"id": "643", "content": "Methods that move towards less supervised scenarios are key for imagesegmentation, as dense labels demand significant human intervention. Generally,the annotation burden is mitigated by labeling datasets with weaker forms ofsupervision, e.g. image-level labels or bounding boxes. Another option aresemi-supervised settings, that commonly leverage a few strong annotations and ahuge number of unlabeled/weakly-labeled data. In this paper, we revisitsemi-supervised segmentation schemes and narrow down significantly theannotation budget (in terms of total labeling time of the training set)compared to previous approaches. With a very simple pipeline, we demonstratethat at low annotation budgets, semi-supervised methods outperform by a widemargin weakly-supervised ones for both semantic and instance segmentation. Ourapproach also outperforms previous semi-supervised works at a much reducedlabeling cost. We present results for the Pascal VOC benchmark and unify weaklyand semi-supervised approaches by considering the total annotation budget, thusallowing a fairer comparison between methods. ", "id2": "642", "id3": "None"}
{"id": "644", "content": "We present an end-to-end learned algorithm for seeded segmentation. Ourmethod is based on the Random Walker algorithm, where we predict the edgeweights of the underlying graph using a convolutional neural network. This canbe interpreted as learning context-dependent diffusivities for a lineardiffusion process. Besides calculating the exact gradient for optimizing thesediffusivities, we also propose simplifications that sparsely sample thegradient and still yield competitive results. The proposed method achieves thecurrently best results on a seeded version of the CREMI neuron segmentationchallenge. ", "id2": "643", "id3": "None"}
{"id": "645", "content": "In response to the growing importance of geospatial data, its analysisincluding semantic segmentation becomes an increasingly popular task incomputer vision today. Convolutional neural networks are powerful visual modelsthat yield hierarchies of features and practitioners widely use them to processremote sensing data. When performing remote sensing image segmentation,multiple instances of one class with precisely defined boundaries are often thecase, and it is crucial to extract those boundaries accurately. The accuracy ofsegments boundaries delineation influences the quality of the whole segmentedareas explicitly. However, widely-used segmentation loss functions such as BCE,IoU loss or Dice loss do not penalize misalignment of boundaries sufficiently.In this paper, we propose a novel loss function, namely a differentiablesurrogate of a metric accounting accuracy of boundary detection. We can use theloss function with any neural network for binary segmentation. We performedvalidation of our loss function with various modifications of UNet on asynthetic dataset, as well as using real-world data (ISPRS Potsdam, INRIA AIL).Trained with the proposed loss function, models outperform baseline methods interms of IoU score. ", "id2": "644", "id3": "None"}
{"id": "646", "content": "Automatic segmentation of organs-at-risk (OAR) in computed tomography (CT) isan essential part of planning effective treatment strategies to combat lung andesophageal cancer. Accurate segmentation of organs surrounding tumours helpsaccount for the variation in position and morphology inherent across patients,thereby facilitating adaptive and computer-assisted radiotherapy. Althoughmanual delineation of OARs is still highly prevalent, it is prone to errors dueto complex variations in the shape and position of organs across patients, andlow soft tissue contrast between neighbouring organs in CT images. Recently,deep convolutional neural networks (CNNs) have gained tremendous traction andachieved state-of-the-art results in medical image segmentation. In this paper,we propose a deep learning framework to segment OARs in thoracic CT images,specifically for the: heart, esophagus, trachea and aorta. Our approach employsdilated convolutions and aggregated residual connections in the bottleneck of aU-Net styled network, which incorporates global context and dense information.Our method achieved an overall Dice score of 91.57% on 20 unseen test samplesfrom the ISBI 2019 SegTHOR challenge. ", "id2": "645", "id3": "None"}
{"id": "647", "content": "Automated design of neural network architectures tailored for a specific taskis an extremely promising, albeit inherently difficult, avenue to explore.While most results in this domain have been achieved on image classificationand language modelling problems, here we concentrate on dense per-pixel tasks,in particular, semantic image segmentation using fully convolutional networks.In contrast to the aforementioned areas, the design choices of a fullyconvolutional network require several changes, ranging from the sort ofoperations that need to be used---e.g., dilated convolutions---to a solving ofa more difficult optimisation problem. In this work, we are particularlyinterested in searching for high-performance compact segmentationarchitectures, able to run in real-time using limited resources. To achievethat, we intentionally over-parameterise the architecture during the trainingtime via a set of auxiliary cells that provide an intermediate supervisorysignal and can be omitted during the evaluation phase. The design of theauxiliary cell is emitted by a controller, a neural network with the fixedstructure trained using reinforcement learning. More crucially, we demonstratehow to efficiently search for these architectures within limited time andcomputational budgets. In particular, we rely on a progressive strategy thatterminates non-promising architectures from being further trained, and onPolyak averaging coupled with knowledge distillation to speed-up theconvergence. Quantitatively, in 8 GPU-days our approach discovers a set ofarchitectures performing on-par with state-of-the-art among compact models onthe semantic segmentation, pose estimation and depth prediction tasks. Codewill be made available here: https://github.com/drsleep/nas-segm-pytorch ", "id2": "646", "id3": "None"}
{"id": "648", "content": "Since acquiring pixel-wise annotations for training convolutional neuralnetworks for semantic image segmentation is time-consuming, weakly supervisedapproaches that only require class tags have been proposed. In this work, wepropose another form of supervision, namely image captions as they can be foundon the Internet. These captions have two advantages. They do not requireadditional curation as it is the case for the clean class tags used by currentweakly supervised approaches and they provide textual context for the classespresent in an image. To leverage such textual context, we deploy a multi-modalnetwork that learns a joint embedding of the visual representation of the imageand the textual representation of the caption. The network estimates textactivation maps (TAMs) for class names as well as compound concepts, i.e.combinations of nouns and their attributes. The TAMs of compound conceptsdescribing classes of interest substantially improve the quality of theestimated class activation maps which are then used to train a network forsemantic segmentation. We evaluate our method on the COCO dataset where itachieves state of the art results for weakly supervised image segmentation. ", "id2": "647", "id3": "None"}
{"id": "649", "content": "Machine learning methods have achieved good performance and been widelyapplied in various real-world applications. They can learn the model adaptivelyand be better fit for special requirements of different tasks. Generally, agood machine learning system is composed of plentiful training data, a goodmodel training process, and an accurate inference. Many factors can affect theperformance of the machine learning process, among which the diversity of themachine learning process is an important one. The diversity can help eachprocedure to guarantee a total good machine learning: diversity of the trainingdata ensures that the training data can provide more discriminative informationfor the model, diversity of the learned model (diversity in parameters of eachmodel or diversity among different base models) makes each parameter/modelcapture unique or complement information and the diversity in inference canprovide multiple choices each of which corresponds to a specific plausiblelocal optimal result. Even though the diversity plays an important role inmachine learning process, there is no systematical analysis of thediversification in machine learning system. In this paper, we systematicallysummarize the methods to make data diversification, model diversification, andinference diversification in the machine learning process, respectively. Inaddition, the typical applications where the diversity technology improved themachine learning performance have been surveyed, including the remote sensingimaging tasks, machine translation, camera relocalization, image segmentation,object detection, topic modeling, and others. Finally, we discuss somechallenges of the diversity technology in machine learning and point out somedirections in future work. ", "id2": "648", "id3": "None"}
{"id": "650", "content": "Pap smear testing has been widely used for detecting cervical cancers basedon the morphology properties of cell nuclei in microscopic image. An accuratenuclei segmentation could thus improve the success rate of cervical cancerscreening. In this work, a method of automated cervical nuclei segmentationusing Deformable Multipath Ensemble Model (D-MEM) is proposed. The approachadopts a U-shaped convolutional network as a backbone network, in which denseblocks are used to transfer feature information more effectively. To increasethe flexibility of the model, we then use deformable convolution to deal withdifferent nuclei irregular shapes and sizes. To reduce the predictive bias, wefurther construct multiple networks with different settings, which form anensemble model. The proposed segmentation framework has achievedstate-of-the-art accuracy on Herlev dataset with Zijdenbos similarity index(ZSI) of 0.933, and has the potential to be extended for solving other medicalimage segmentation tasks. ", "id2": "649", "id3": "None"}
{"id": "651", "content": "This paper proposes the first, known to us, open source presentation attackdetection (PAD) solution to distinguish between authentic iris images (possiblywearing clear contact lenses) and irises with textured contact lenses. Thissoftware can serve as a baseline in various PAD evaluations, and also as anopen-source platform with an up-to-date reference method for iris PAD. Thesoftware is written in C++ and Python and uses only open source resources, suchas OpenCV. This method does not incorporate iris image segmentation, which maybe problematic for unknown fake samples. Instead, it makes a best guess tolocalize the rough position of the iris. The PAD-related features are extractedwith the Binary Statistical Image Features (BSIF), which are classified by anensemble of classifiers incorporating support vector machine, random forest andmultilayer perceptron. The models attached to the current software have beentrained with the NDCLD15 database and evaluated on the independent datasetsincluded in the LivDet-Iris 2017 competition. The software implements thefunctionality of retraining the classifiers with any database of authentic andattack images. The accuracy of the current version offered with this paperexceeds 99% when tested on subject-disjoint subsets of NDCLD15, and oscillatesaround 85% when tested on the LivDet-Iris 2017 benchmarks, which is on par withthe results obtained by the LivDet-Iris 2017 winner. ", "id2": "650", "id3": "None"}
{"id": "652", "content": "What did it feel like to walk through a city from the past? In this work, wedescribe Nostalgin (Nostalgia Engine), a method that can faithfully reconstructcities from historical images. Unlike existing work in city reconstruction, wefocus on the task of reconstructing 3D cities from historical images. Workingwith historical image data is substantially more difficult, as there aresignificantly fewer buildings available and the details of the cameraparameters which captured the images are unknown. Nostalgin can generate a citymodel even if there is only a single image per facade, regardless of viewpointor occlusions. To achieve this, our novel architecture combines imagesegmentation, rectification, and inpainting. We motivate our design decisionswith experimental analysis of individual components of our pipeline, and showthat we can improve on baselines in both speed and visual realism. Wedemonstrate the efficacy of our pipeline by recreating two 1940s Manhattan cityblocks. We aim to deploy Nostalgin as an open source platform where users cangenerate immersive historical experiences from their own photos. ", "id2": "651", "id3": "None"}
{"id": "653", "content": "Most of the semantic segmentation approaches have been developed for singleimage segmentation, and hence, video sequences are currently segmented byprocessing each frame of the video sequence separately. The disadvantage ofthis is that temporal image information is not considered, which improves theperformance of the segmentation approach. One possibility to include temporalinformation is to use recurrent neural networks. However, there are only a fewapproaches using recurrent networks for video segmentation so far. Theseapproaches extend the encoder-decoder network architecture of well-knownsegmentation approaches and place convolutional LSTM layers between encoder anddecoder. However, in this paper it is shown that this position is not optimal,and that other positions in the network exhibit better performance. Nowadays,state-of-the-art segmentation approaches rarely use the classicalencoder-decoder structure, but use multi-branch architectures. Thesearchitectures are more complex, and hence, it is more difficult to place therecurrent units at a proper position. In this work, the multi-brancharchitectures are extended by convolutional LSTM layers at different positionsand evaluated on two different datasets in order to find the best one. Itturned out that the proposed approach outperforms the pure CNN-based approachfor up to 1.6 percent. ", "id2": "652", "id3": "None"}
{"id": "654", "content": "Recent progress in biomedical image segmentation based on deep convolutionalneural networks (CNNs) has drawn much attention. However, its vulnerabilitytowards adversarial samples cannot be overlooked. This paper is the first onethat discovers that all the CNN-based state-of-the-art biomedical imagesegmentation models are sensitive to adversarial perturbations. This limits thedeployment of these methods in safety-critical biomedical fields. In thispaper, we discover that global spatial dependencies and global contextualinformation in a biomedical image can be exploited to defend againstadversarial attacks. To this end, non-local context encoder (NLCE) is proposedto model short- and long range spatial dependencies and encode global contextsfor strengthening feature activations by channel-wise attention. The NLCEmodules enhance the robustness and accuracy of the non-local context encodingnetwork (NLCEN), which learns robust enhanced pyramid feature representationswith NLCE modules, and then integrates the information across different levels.Experiments on both lung and skin lesion segmentation datasets havedemonstrated that NLCEN outperforms any other state-of-the-art biomedical imagesegmentation methods against adversarial attacks. In addition, NLCE modules canbe applied to improve the robustness of other CNN-based biomedical imagesegmentation methods. ", "id2": "653", "id3": "None"}
{"id2": 1063, "id3": "653", "content": "Recent progress in biomedical image segmentation based on deep convolutionalneural networks (CNNs) has drawn much attention. However, its vulnerabilitytowards adversarial samples cannot be overlooked. This paper is the first onethat discovers that all the CNN-based state-of-the-art biomedical imagesegmentation models are sensitive to adversarial perturbations. This limits thedeployment of these methods in safety-critical biomedical fields. In thispaper, we discover that global spatial dependencies and global contextualinformation in a biomedical image can be exploited to defend againstadversarial attacks. To this end, non-local context encoder (NLCE) is proposedto model short- and long range spatial dependencies and encode global contextsfor strengthening feature activations by channel-wise attention. The NLCEmodules enhance the robustness and accuracy of the non-local context encodingnetwork (NLCEN), which learns robust enhanced pyramid feature representationswith NLCE modules, and then integrates the information across different levels.Experiments on both lung and skin lesion segmentation datasets havedemonstrated that NLCEN outperforms any other state-of-the-art biomedical imagesegmentation methods against adversarial attacks. In addition, NLCE modules canbe applied to improve the robustness of other CNN-based biomedical imagesegmentation methods."}
{"id": "655", "content": "It is important to find the target as soon as possible for search and rescueoperations. Surveillance camera systems and unmanned aerial vehicles (UAVs) areused to support search and rescue. Automatic object detection is importantbecause a person cannot monitor multiple surveillance screens simultaneouslyfor 24 hours. Also, the object is often too small to be recognized by the humaneye on the surveillance screen. This study used UAVs around the Port of Houstonand fixed surveillance cameras to build an automatic target detection systemthat supports the US Coast Guard (USCG) to help find targets (e.g., personoverboard). We combined image segmentation, enhancement, and convolution neuralnetworks to reduce detection time to detect small targets. We compared theperformance between the auto-detection system and the human eye. Our systemdetected the target within 8 seconds, but the human eye detected the targetwithin 25 seconds. Our systems also used synthetic data generation and dataaugmentation techniques to improve target detection accuracy. This solution mayhelp the search and rescue operations of the first responders in a timelymanner. ", "id2": "654", "id3": "None"}
{"id2": 1064, "id3": "654", "content": "It is important to find the target as soon as possible for search and rescueoperations. Surveillance camera systems and unmanned aerial vehicles (UAVs) areused to support search and rescue. Automatic object detection is importantbecause a person cannot monitor multiple surveillance screens simultaneouslyfor 24 hours. Also, the object is often too small to be recognized by the humaneye on the surveillance screen. This study used UAVs around the Port of Houstonand fixed surveillance cameras to build an automatic target detection systemthat supports the US Coast Guard (USCG) to help find targets (e.g., personoverboard). We combined image segmentation, enhancement, and convolution neuralnetworks to reduce detection time to detect small targets. We compared theperformance between the auto-detection system and the human eye. Our systemdetected the target within 8 seconds, but the human eye detected the targetwithin 25 seconds. Our systems also used synthetic data generation and dataaugmentation techniques to improve target detection accuracy. This solution mayhelp the search and rescue operations of the first responders in a timelymanner."}
{"id": "656", "content": "Autonomous Micro Aerial Vehicles (MAVs) gained tremendous attention in recentyears. Autonomous flight in indoor requires a dense depth map for navigablespace detection which is the fundamental component for autonomous navigation.In this paper, we address the problem of reconstructing dense depth while adrone is hovering (small camera motion) in indoor scenes using alreadyestimated cameras and sparse point cloud obtained from a vSLAM. We start bysegmenting the scene based on sudden depth variation using sparse 3D points andintroduce a patch-based local plane fitting via energy minimization whichcombines photometric consistency and co-planarity with neighbouring patches.The method also combines a plane sweep technique for image segments havingalmost no sparse point for initialization. Experiments show, the proposedmethod produces better depth for indoor in artificial lighting condition,low-textured environment compared to earlier literature in small motion. ", "id2": "655", "id3": "None"}
{"id": "657", "content": "In the last few years, Deep Learning (DL) has been showing superiorperformance in different modalities of biomedical image analysis. Several DLarchitectures have been proposed for classification, segmentation, anddetection tasks in medical imaging and computational pathology. In this paper,we propose a new DL architecture, the NABLA-N network, with better featurefusion techniques in decoding units for dermoscopic image segmentation tasks.The NABLA-N network has several advances for segmentation tasks. First, thismodel ensures better feature representation for semantic segmentation with acombination of low to high-level feature maps. Second, this network showsbetter quantitative and qualitative results with the same or fewer networkparameters compared to other methods. In addition, the Inception RecurrentResidual Convolutional Neural Network (IRRCNN) model is used for skin cancerclassification. The proposed NABLA-N network and IRRCNN models are evaluatedfor skin cancer segmentation and classification on the benchmark datasets fromthe International Skin Imaging Collaboration 2018 (ISIC-2018). The experimentalresults show superior performance on segmentation tasks compared to theRecurrent Residual U-Net (R2U-Net). The classification model shows around 87%testing accuracy for dermoscopic skin cancer classification on ISIC2018dataset. ", "id2": "656", "id3": "None"}
{"id": "658", "content": "In this paper, we propose a novel iterative convolution-thresholding method(ICTM) that is applicable to a range of variational models for imagesegmentation. A variational model usually minimizes an energy functionalconsisting of a fidelity term and a regularization term. In the ICTM, theinterface between two different segment domains is implicitly represented bytheir characteristic functions. The fidelity term is then usually written as alinear functional of the characteristic functions and the regularized term isapproximated by a functional of characteristic functions in terms of heatkernel convolution. This allows us to design an iterativeconvolution-thresholding method to minimize the approximate energy. The methodis simple, efficient and enjoys the energy-decaying property. Numericalexperiments show that the method is easy to implement, robust and applicable tovarious image segmentation models. ", "id2": "657", "id3": "None"}
{"id": "659", "content": "Domain adaptation for semantic image segmentation is very necessary sincemanually labeling large datasets with pixel-level labels is expensive and timeconsuming. Existing domain adaptation techniques either work on limiteddatasets, or yield not so good performance compared with supervised learning.In this paper, we propose a novel bidirectional learning framework for domainadaptation of segmentation. Using the bidirectional learning, the imagetranslation model and the segmentation adaptation model can be learnedalternatively and promote to each other. Furthermore, we propose aself-supervised learning algorithm to learn a better segmentation adaptationmodel and in return improve the image translation model. Experiments show thatour method is superior to the state-of-the-art methods in domain adaptation ofsegmentation with a big margin. The source code is available athttps://github.com/liyunsheng13/BDL. ", "id2": "658", "id3": "None"}
{"id": "660", "content": "X-Ray image enhancement, along with many other medical image processingapplications, requires the segmentation of images into bone, soft tissue, andopen beam regions. We apply a machine learning approach to this problem,presenting an end-to-end solution which results in robust and efficientinference. Since medical institutions frequently do not have the resources toprocess and label the large quantity of X-Ray images usually needed for neuralnetwork training, we design an end-to-end solution for small datasets, whileachieving state-of-the-art results. Our implementation produces an overallaccuracy of 92%, F1 score of 0.92, and an AUC of 0.98, surpassing classicalimage processing techniques, such as clustering and entropy based methods,while improving upon the output of existing neural networks used forsegmentation in non-medical contexts. The code used for this project isavailable online. ", "id2": "659", "id3": "None"}
{"id": "661", "content": "Segmenting objects in images and separating sound sources in audio arechallenging tasks, in part because traditional approaches require large amountsof labeled data. In this paper we develop a neural network model for visualobject segmentation and sound source separation that learns from natural videosthrough self-supervision. The model is an extension of recently proposed workthat maps image pixels to sounds. Here, we introduce a learning approach todisentangle concepts in the neural networks, and assign semantic categories tonetwork feature channels to enable independent image segmentation and soundsource separation after audio-visual training on videos. Our evaluations showthat the disentangled model outperforms several baselines in semanticsegmentation and sound source separation. ", "id2": "660", "id3": "None"}
{"id2": 1065, "id3": "660", "content": "Segmenting objects in images and separating sound sources in audio arechallenging tasks, in part because traditional approaches require large amountsof labeled data. In this paper we develop a neural network model for visualobject segmentation and sound source separation that learns from natural videosthrough self-supervision. The model is an extension of recently proposed workthat maps image pixels to sounds. Here, we introduce a learning approach todisentangle concepts in the neural networks, and assign semantic categories tonetwork feature channels to enable independent image segmentation and soundsource separation after audio-visual training on videos. Our evaluations showthat the disentangled model outperforms several baselines in semanticsegmentation and sound source separation."}
{"id": "662", "content": "Cardiac image segmentation is a critical process for generating personalizedmodels of the heart and for quantifying cardiac performance parameters. Severalconvolutional neural network (CNN) architectures have been proposed to segmentthe heart chambers from cardiac cine MR images. Here we propose a multi-tasklearning (MTL)-based regularization framework for cardiac MR imagesegmentation. The network is trained to perform the main task of semanticsegmentation, along with a simultaneous, auxiliary task of pixel-wise distancemap regression. The proposed distance map regularizer is a decoder networkadded to the bottleneck layer of an existing CNN architecture, facilitating thenetwork to learn robust global features. The regularizer block is removed aftertraining, so that the original number of network parameters does not change. Weshow that the proposed regularization method improves both binary andmulti-class segmentation performance over the corresponding state-of-the-artCNN architectures on two publicly available cardiac cine MRI datasets,obtaining average dice coefficient of 0.84$ pm$0.03 and 0.91$ pm$0.04,respectively. Furthermore, we also demonstrate improved generalizationperformance of the distance map regularized network on cross-datasetsegmentation, showing as much as 42% improvement in myocardium Dice coefficientfrom 0.56$ pm$0.28 to 0.80$ pm$0.14. ", "id2": "661", "id3": "None"}
{"id2": 1066, "id3": "661", "content": "Cardiac image segmentation is a critical process for generating personalizedmodels of the heart and for quantifying cardiac performance parameters. Severalconvolutional neural network (CNN) architectures have been proposed to segmentthe heart chambers from cardiac cine MR images. Here we propose a multi-tasklearning (MTL)-based regularization framework for cardiac MR imagesegmentation. The network is trained to perform the main task of semanticsegmentation, along with a simultaneous, auxiliary task of pixel-wise distancemap regression. The proposed distance map regularizer is a decoder networkadded to the bottleneck layer of an existing CNN architecture, facilitating thenetwork to learn robust global features. The regularizer block is removed aftertraining, so that the original number of network parameters does not change. Weshow that the proposed regularization method improves both binary andmulti-class segmentation performance over the corresponding state-of-the-artCNN architectures on two publicly available cardiac cine MRI datasets,obtaining average dice coefficient of 0.84$ pm$0.03 and 0.91$ pm$0.04,respectively. Furthermore, we also demonstrate improved generalizationperformance of the distance map regularized network on cross-datasetsegmentation, showing as much as 42% improvement in myocardium Dice coefficientfrom 0.56$ pm$0.28 to 0.80$ pm$0.14."}
{"id": "663", "content": "Crack is one of the most common road distresses which may pose road safetyhazards. Generally, crack detection is performed by either certified inspectorsor structural engineers. This task is, however, time-consuming, subjective andlabor-intensive. In this paper, we propose a novel road crack detectionalgorithm based on deep learning and adaptive image segmentation. Firstly, adeep convolutional neural network is trained to determine whether an imagecontains cracks or not. The images containing cracks are then smoothed usingbilateral filtering, which greatly minimizes the number of noisy pixels.Finally, we utilize an adaptive thresholding method to extract the cracks fromroad surface. The experimental results illustrate that our network can classifyimages with an accuracy of 99.92%, and the cracks can be successfully extractedfrom the images using our proposed thresholding algorithm. ", "id2": "662", "id3": "None"}
{"id": "664", "content": "The quality of images captured in outdoor environments can be affected bypoor weather conditions such as fog, dust, and atmospheric scattering of otherparticles. This problem can bring extra challenges to high-level computervision tasks like image segmentation and object detection. However, previousstudies on image dehazing suffer from a huge computational workload andcorruption of the original image, such as over-saturation and halos. In thispaper, we present a novel image dehazing approach based on the optical modelfor haze images and regularized optimization. Specifically, we convert thenon-convex, bilinear problem concerning the unknown haze-free image and lighttransmission distribution to a convex, linear optimization problem byestimating the atmosphere light constant. Our method is further accelerated byintroducing a multilevel Haar wavelet transform. The optimization, instead, isapplied to the low frequency sub-band decomposition of the original image. Thisdimension reduction significantly improves the processing speed of our methodand exhibits the potential for real-time applications. Experimental resultsshow that our approach outperforms state-of-the-art dehazing algorithms interms of both image reconstruction quality and computational efficiency. Forimplementation details, source code can be publicly accessed viahttp://github.com/JiaxiHe/Image-and-Video-Dehazing. ", "id2": "663", "id3": "None"}
{"id": "665", "content": "In order to create an image segmentation method robust to lighting changes,two novel homogeneity criteria of an image region were studied. Both weredefined using the Logarithmic Image Processing (LIP) framework whose laws modellighting changes. The first criterion estimates the LIP-additive homogeneityand is based on the LIP-additive law. It is theoretically insensitive tolighting changes caused by variations of the camera exposure-time or sourceintensity. The second, the LIP-multiplicative homogeneity criterion, is basedon the LIP-multiplicative law and is insensitive to changes due to variationsof the object thickness or opacity. Each criterion is then applied in Revol andJourlins (1997) region growing method which is based on the homogeneity of animage region. The region growing method becomes therefore robust to thelighting changes specific to each criterion. Experiments on simulated and onreal images presenting lighting variations prove the robustness of the criteriato those variations. Compared to a state-of the art method based on the imagecomponent-tree, ours is more robust. These results open the way to numerousapplications where the lighting is uncontrolled or partially controlled. ", "id2": "664", "id3": "None"}
{"id": "666", "content": "Single Image Super Resolution (SISR) techniques based on Super ResolutionConvolutional Neural Networks (SRCNN) are applied to micro-computed tomography(  mu CT) images of sandstone and carbonate rocks. Digital rock imaging islimited by the capability of the scanning device resulting in trade-offsbetween resolution and field of view, and super resolution methods tested inthis study aim to compensate for these limits. SRCNN models SR-Resnet, EnhancedDeep SR (EDSR), and Wide-Activation Deep SR (WDSR) are used on the Digital RockSuper Resolution 1 (DRSRD1) Dataset of 4x downsampled images, comprising of2000 high resolution (800x800) raw micro-CT images of Bentheimer sandstone andEstaillades carbonate. The trained models are applied to the validation andtest data within the dataset and show a 3-5 dB rise in image quality comparedto bicubic interpolation, with all tested models performing within a 0.1 dBrange. Difference maps indicate that edge sharpness is completely recovered inimages within the scope of the trained model, with only high frequency noiserelated detail loss. We find that aside from generation of high-resolutionimages, a beneficial side effect of super resolution methods applied tosynthetically downgraded images is the removal of image noise while recoveringedgewise sharpness which is beneficial for the segmentation process. The modelis also tested against real low-resolution images of Bentheimer rock with imageaugmentation to account for natural noise and blur. The SRCNN method is shownto act as a preconditioner for image segmentation under these circumstanceswhich naturally leads to further future development and training of models thatsegment an image directly. Image restoration by SRCNN on the rock images is ofsignificantly higher quality than traditional methods and suggests SRCNNmethods are a viable processing step in a digital rock workflow. ", "id2": "665", "id3": "None"}
{"id": "667", "content": "Automated digital histopathology image segmentation is an important task tohelp pathologists diagnose tumors and cancer subtypes. For pathologicaldiagnosis of cancer subtypes, pathologists usually change the magnification ofwhole-slide images (WSI) viewers. A key assumption is that the importance ofthe magnifications depends on the characteristics of the input image, such ascancer subtypes. In this paper, we propose a novel semantic segmentationmethod, called Adaptive-Weighting-Multi-Field-of-View-CNN (AWMF-CNN), that canadaptively use image features from images with different magnifications tosegment multiple cancer subtype regions in the input image. The proposed methodaggregates several expert CNNs for images of different magnifications byadaptively changing the weight of each expert depending on the input image. Itleverages information in the images with different magnifications that might beuseful for identifying the subtypes. It outperformed other state-of-the-artmethods in experiments. ", "id2": "666", "id3": "None"}
{"id": "668", "content": "We propose and study a task we name panoptic segmentation (PS). Panopticsegmentation unifies the typically distinct tasks of semantic segmentation(assign a class label to each pixel) and instance segmentation (detect andsegment each object instance). The proposed task requires generating a coherentscene segmentation that is rich and complete, an important step towardreal-world vision systems. While early work in computer vision addressedrelated image/scene parsing tasks, these are not currently popular, possiblydue to lack of appropriate metrics or associated recognition challenges. Toaddress this, we propose a novel panoptic quality (PQ) metric that capturesperformance for all classes (stuff and things) in an interpretable and unifiedmanner. Using the proposed metric, we perform a rigorous study of both humanand machine performance for PS on three existing datasets, revealinginteresting insights about the task. The aim of our work is to revive theinterest of the community in a more unified view of image segmentation. ", "id2": "667", "id3": "None"}
{"id": "669", "content": "We address interactive full image annotation, where the goal is to accuratelysegment all object and stuff regions in an image. We propose an interactive,scribble-based annotation framework which operates on the whole image toproduce segmentations for all regions. This enables sharing scribblecorrections across regions, and allows the annotator to focus on the largesterrors made by the machine across the whole image. To realize this, we adaptMask-RCNN into a fast interactive segmentation framework and introduce aninstance-aware loss measured at the pixel-level in the full image canvas, whichlets predictions for nearby regions properly compete for space. Finally, wecompare to interactive single object segmentation on the COCO panoptic dataset.We demonstrate that our interactive full image segmentation approach leads to a5% IoU gain, reaching 90% IoU at a budget of four extreme clicks and fourcorrective scribbles per region. ", "id2": "668", "id3": "None"}
{"id": "670", "content": "Binary segmentation of volumetric images of porous media is a crucial steptowards gaining a deeper understanding of the factors governing biogeochemicalprocesses at minute scales. Contemporary work primarily revolves aroundprimitive techniques based on global or local adaptive thresholding that haveknown common drawbacks in image segmentation. Moreover, absence of a unifiedbenchmark prohibits quantitative evaluation, which further clouds the impact ofexisting methodologies. In this study, we tackle the issue on both fronts.Firstly, by drawing parallels with natural image segmentation, we propose anovel, and automatic segmentation technique, 3D Quantum Cuts (QCuts-3D)grounded on a state-of-the-art spectral clustering technique. Secondly, wecurate and present a publicly available dataset of 68 multiphase volumetricimages of porous media with diverse solid geometries, along with voxel-wiseground truth annotations for each constituting phase. We provide comparativeevaluations between QCuts-3D and the current state-of-the-art over this datasetacross a variety of evaluation metrics. The proposed systematic approachachieves a 26% increase in AUROC while achieving a substantial reduction of thecomputational complexity of the state-of-the-art competitors. Moreover,statistical analysis reveals that the proposed method exhibits significantrobustness against the compositional variations of porous media. ", "id2": "669", "id3": "None"}
{"id": "671", "content": "We consider the problem of referring image segmentation. Given an input imageand a natural language expression, the goal is to segment the object referredby the language expression in the image. Existing works in this area treat thelanguage expression and the input image separately in their representations.They do not sufficiently capture long-range correlations between these twomodalities. In this paper, we propose a cross-modal self-attention (CMSA)module that effectively captures the long-range dependencies between linguisticand visual features. Our model can adaptively focus on informative words in thereferring expression and important regions in the input image. In addition, wepropose a gated multi-level fusion module to selectively integrateself-attentive cross-modal features corresponding to different levels in theimage. This module controls the information flow of features at differentlevels. We validate the proposed approach on four evaluation datasets. Ourproposed approach consistently outperforms existing state-of-the-art methods. ", "id2": "670", "id3": "None"}
{"id": "672", "content": "Cloud based medical image analysis has become popular recently due to thehigh computation complexities of various deep neural network (DNN) basedframeworks and the increasingly large volume of medical images that need to beprocessed. It has been demonstrated that for medical images the transmissionfrom local to clouds is much more expensive than the computation in the cloudsitself. Towards this, 3D image compression techniques have been widely appliedto reduce the data traffic. However, most of the existing image compressiontechniques are developed around human vision, i.e., they are designed tominimize distortions that can be perceived by human eyes. In this paper we willuse deep learning based medical image segmentation as a vehicle and demonstratethat interestingly, machine and human view the compression quality differently.Medical images compressed with good quality w.r.t. human vision may result ininferior segmentation accuracy. We then design a machine vision oriented 3Dimage compression framework tailored for segmentation using DNNs. Our methodautomatically extracts and retains image features that are most important tothe segmentation. Comprehensive experiments on widely adopted segmentationframeworks with HVSMR 2016 challenge dataset show that our method can achievesignificantly higher segmentation accuracy at the same compression rate, ormuch better compression rate under the same segmentation accuracy, whencompared with the existing JPEG 2000 method. To the best of the authorsknowledge, this is the first machine vision guided medical image compressionframework for segmentation in the clouds. ", "id2": "671", "id3": "None"}
{"id": "673", "content": "Morphological reconstruction (MR) is often employed by seeded imagesegmentation algorithms such as watershed transform and power watershed as itis able to filter seeds (regional minima) to reduce over-segmentation. However,MR might mistakenly filter meaningful seeds that are required for generatingaccurate segmentation and it is also sensitive to the scale because asingle-scale structuring element is employed. In this paper, a novel adaptivemorphological reconstruction (AMR) operation is proposed that has threeadvantages. Firstly, AMR can adaptively filter useless seeds while preservingmeaningful ones. Secondly, AMR is insensitive to the scale of structuringelements because multiscale structuring elements are employed. Finally, AMR hastwo attractive properties: monotonic increasingness and convergence that helpseeded segmentation algorithms to achieve a hierarchical segmentation.Experiments clearly demonstrate that AMR is useful for improving algorithms ofseeded image segmentation and seed-based spectral segmentation. Compared toseveral state-of-the-art algorithms, the proposed algorithms provide bettersegmentation results requiring less computing time. Source code is available athttps://github.com/SUST-reynole/AMR. ", "id2": "672", "id3": "None"}
{"id": "674", "content": "Image segmentation is an important task in many medical applications. Methodsbased on convolutional neural networks attain state-of-the-art accuracy;however, they typically rely on supervised training with large labeleddatasets. Labeling medical images requires significant expertise and time, andtypical hand-tuned approaches for data augmentation fail to capture the complexvariations in such images.  We present an automated data augmentation method for synthesizing labeledmedical images. We demonstrate our method on the task of segmenting magneticresonance imaging (MRI) brain scans. Our method requires only a singlesegmented scan, and leverages other unlabeled scans in a semi-supervisedapproach. We learn a model of transformations from the images, and use themodel along with the labeled example to synthesize additional labeled examples.Each transformation is comprised of a spatial deformation field and anintensity change, enabling the synthesis of complex effects such as variationsin anatomy and image acquisition procedures. We show that training a supervisedsegmenter with these new examples provides significant improvements overstate-of-the-art methods for one-shot biomedical image segmentation. Our codeis available at https://github.com/xamyzhao/brainstorm. ", "id2": "673", "id3": "None"}
{"id": "675", "content": "Referring object detection and referring image segmentation are importanttasks that require joint understanding of visual information and naturallanguage. Yet there has been evidence that current benchmark datasets sufferfrom bias, and current state-of-the-art models cannot be easily evaluated ontheir intermediate reasoning process. To address these issues and complementsimilar efforts in visual question answering, we build CLEVR-Ref+, a syntheticdiagnostic dataset for referring expression comprehension. The preciselocations and attributes of the objects are readily available, and thereferring expressions are automatically associated with functional programs.The synthetic nature allows control over dataset bias (through samplingstrategy), and the modular programs enable intermediate reasoning ground truthwithout human annotators.  In addition to evaluating several state-of-the-art models on CLEVR-Ref+, wealso propose IEP-Ref, a module network approach that significantly outperformsother models on our dataset. In particular, we present two interesting andimportant findings using IEP-Ref: (1) the module trained to transform featuremaps into segmentation masks can be attached to any intermediate module toreveal the entire reasoning process step-by-step; (2) even if all training datahas at least one object referred, IEP-Ref can correctly predict no-foregroundwhen presented with false-premise referring expressions. To the best of ourknowledge, this is the first direct and quantitative proof that neural modulesbehave in the way they are intended. ", "id2": "674", "id3": "None"}
{"id": "676", "content": "Recently, Neural Architecture Search (NAS) has successfully identified neuralnetwork architectures that exceed human designed ones on large-scale imageclassification. In this paper, we study NAS for semantic image segmentation.Existing works often focus on searching the repeatable cell structure, whilehand-designing the outer network structure that controls the spatial resolutionchanges. This choice simplifies the search space, but becomes increasinglyproblematic for dense image prediction which exhibits a lot more network levelarchitectural variations. Therefore, we propose to search the network levelstructure in addition to the cell level structure, which forms a hierarchicalarchitecture search space. We present a network level search space thatincludes many popular designs, and develop a formulation that allows efficientgradient-based architecture search (3 P100 GPU days on Cityscapes images). Wedemonstrate the effectiveness of the proposed method on the challengingCityscapes, PASCAL VOC 2012, and ADE20K datasets. Auto-DeepLab, ourarchitecture searched specifically for semantic image segmentation, attainsstate-of-the-art performance without any ImageNet pretraining. ", "id2": "675", "id3": "None"}
{"id": "677", "content": "The prevalence of skin melanoma is rapidly increasing as well as the recordeddeath cases of its patients. Automatic image segmentation tools play animportant role in providing standardized computer-assisted analysis for skinmelanoma patients. Current state-of-the-art segmentation methods are based onfully convolutional neural networks, which utilize an encoder-decoder approach.However, these methods produce coarse segmentation masks due to the loss oflocation information during the encoding layers. Inspired by Pyramid SceneParsing Network (PSP-Net), we propose an encoder-decoder model that utilizespyramid pooling modules in the deep skip connections which aggregate the globalcontext and compensate for the lost spatial information. We trained andvalidated our approach using ISIC 2018: Skin Lesion Analysis Towards MelanomaDetection grand challenge dataset. Our approach showed a validation accuracywith a Jaccard index of 0.837, which outperforms U-Net. We believe that withthis reported reliable accuracy, this method can be introduced for clinicalpractice. ", "id2": "676", "id3": "None"}
{"id": "678", "content": "In applications of supervised learning applied to medical image segmentation,the need for large amounts of labeled data typically goes unquestioned. Inparticular, in the case of brain anatomy segmentation, hundreds or thousands ofweakly-labeled volumes are often used as training data. In this paper, we firstobserve that for many brain structures, a small number of training examples,(n=9), weakly labeled using Freesurfer 6.0, plus simple data augmentation,suffice as training data to achieve high performance, achieving an overall meanDice coefficient of $0.84  pm 0.12$ compared to Freesurfer over 28 brainstructures in T1-weighted images of $ approx 4000$ 9-10 year-olds from theAdolescent Brain Cognitive Development study. We then examine two varieties ofheteroscedastic network as a method for improving classification results. Anexisting proposal by Kendall and Gal, which uses Monte-Carlo inference to learnto predict the variance of each prediction, yields an overall mean Dice of$0.85  pm 0.14$ and showed statistically significant improvements over 25 brainstructures. Meanwhile a novel heteroscedastic network which directly learns theprobability that an example has been mislabeled yielded an overall mean Dice of$0.87  pm 0.11$ and showed statistically significant improvements over all butone of the brain structures considered. The loss function associated to thisnetwork can be interpreted as performing a form of learned label smoothing,where labels are only smoothed where they are judged to be uncertain. ", "id2": "677", "id3": "None"}
{"id": "679", "content": "We propose an active learning approach to image segmentation that exploitsgeometric priors to speed up and streamline the annotation process. It can beapplied for both background-foreground and multi-class segmentation tasks in 2Dimages and 3D image volumes. Our approach combines geometric smoothness priorsin the image space with more traditional uncertainty measures to estimate whichpixels or voxels are the most informative, and thus should to be annotatednext. For multi-class settings, we additionally introduce two novel criteriafor uncertainty. In the 3D case, we use the resulting uncertainty measure toselect voxels lying on a planar patch, which makes batch annotation much moreconvenient for the end user compared to the setting where voxels are randomlydistributed in a volume. The planar patch is found using a branch-and-boundalgorithm that looks for a 2D patch in a 3D volume where the most informativeinstances are located. We evaluate our approach on Electron Microscopy andMagnetic Resonance image volumes, as well as on regular images of horses andfaces. We demonstrate a substantial performance increase over other approachesthanks to the use of geometric priors. ", "id2": "678", "id3": "None"}
{"id": "680", "content": "Since the generative neural networks have made a breakthrough in the imagegeneration problem, lots of researches on their applications have been studiedsuch as image restoration, style transfer and image completion. However, therehas been few research generating objects in uncontrolled real-worldenvironments. In this paper, we propose a novel approach for vehicle imagegeneration in real-world scenes. Using a subnetwork based on a precedent workof image completion, our model makes the shape of an object. Details of objectsare trained by an additional colorization and refinement subnetwork, resultingin a better quality of generated objects. Unlike many other works, our methoddoes not require any segmentation layout but still makes a plausible vehicle inthe image. We evaluate our method by using images from Berkeley Deep Drive(BDD) and Cityscape datasets, which are widely used for object detection andimage segmentation problems. The adequacy of the generated images by theproposed method has also been evaluated using a widely utilized objectdetection algorithm and the FID score. ", "id2": "679", "id3": "None"}
{"id": "681", "content": "The design and performance of computer vision algorithms are greatlyinfluenced by the hardware on which they are implemented. CPUs, multi-coreCPUs, FPGAs and GPUs have inspired new algorithms and enabled existing ideas tobe realized. This is notably the case with GPUs, which has significantlychanged the landscape of computer vision research through deep learning. As theend of Moores law approaches, researchers and hardware manufacturers areexploring alternative hardware computing paradigms. Quantum computers are avery promising alternative and offer polynomial or even exponential speed-upsover conventional computing for some problems. This paper presents a novelapproach to image segmentation that uses new quantum computing hardware.Segmentation is formulated as a graph cut problem that can be mapped to thequantum approximate optimization algorithm (QAOA). This algorithm can beimplemented on current and near-term quantum computers. Encouraging results arepresented on artificial and medical imaging data. This represents an important,practical step towards leveraging quantum computers for computer vision. ", "id2": "680", "id3": "None"}
{"id": "682", "content": "Liver lesion segmentation is a difficult yet critical task for medical imageanalysis. Recently, deep learning based image segmentation methods haveachieved promising performance, which can be divided into three categories: 2D,2.5D and 3D, based on the dimensionality of the models. However, 2.5D and 3Dmethods can have very high complexity and 2D methods may not performsatisfactorily. To obtain competitive performance with low complexity, in thispaper, we propose a Feature-fusion Encoder-Decoder Network (FED-Net) based 2Dsegmentation model to tackle the challenging problem of liver lesionsegmentation from CT images. Our feature fusion method is based on theattention mechanism, which fuses high-level features carrying semanticinformation with low-level features having image details. Additionally, tocompensate for the information loss during the upsampling process, a denseupsampling convolution and a residual convolutional structure are proposed. Wetested our method on the dataset of MICCAI 2017 Liver Tumor Segmentation (LiTS)Challenge and achieved competitive results compared with other state-of-the-artmethods. ", "id2": "681", "id3": "None"}
{"id": "683", "content": "We propose a method to classify cardiac pathology based on a novel approachto extract image derived features to characterize the shape and motion of theheart. An original semi-supervised learning procedure, which makes efficientuse of a large amount of non-segmented images and a small amount of imagessegmented manually by experts, is developed to generate pixel-wise apparentflow between two time points of a 2D+t cine MRI image sequence. Combining theapparent flow maps and cardiac segmentation masks, we obtain a local apparentflow corresponding to the 2D motion of myocardium and ventricular cavities.This leads to the generation of time series of the radius and thickness ofmyocardial segments to represent cardiac motion. These time series of motionfeatures are reliable and explainable characteristics of pathological cardiacmotion. Furthermore, they are combined with shape-related features to classifycardiac pathologies. Using only nine feature values as input, we propose anexplainable, simple and flexible model for pathology classification. On ACDCtraining set and testing set, the model achieves 95% and 94% respectively asclassification accuracy. Its performance is hence comparable to that of thestate-of-the-art. Comparison with various other models is performed to outlinesome advantages of our model. ", "id2": "682", "id3": "None"}
{"id2": 1067, "id3": "682", "content": "We propose a method to classify cardiac pathology based on a novel approachto extract image derived features to characterize the shape and motion of theheart. An original semi-supervised learning procedure, which makes efficientuse of a large amount of non-segmented images and a small amount of imagessegmented manually by experts, is developed to generate pixel-wise apparentflow between two time points of a 2D+t cine MRI image sequence. Combining theapparent flow maps and cardiac segmentation masks, we obtain a local apparentflow corresponding to the 2D motion of myocardium and ventricular cavities.This leads to the generation of time series of the radius and thickness ofmyocardial segments to represent cardiac motion. These time series of motionfeatures are reliable and explainable characteristics of pathological cardiacmotion. Furthermore, they are combined with shape-related features to classifycardiac pathologies. Using only nine feature values as input, we propose anexplainable, simple and flexible model for pathology classification. On ACDCtraining set and testing set, the model achieves 95% and 94% respectively asclassification accuracy. Its performance is hence comparable to that of thestate-of-the-art. Comparison with various other models is performed to outlinesome advantages of our model."}
{"id": "684", "content": "This paper presents a new approach for relatively accurate brain region ofinterest (ROI) detection from dynamic susceptibility contrast (DSC) perfusionmagnetic resonance (MR) images of a human head with abnormal brain anatomy.Such images produce problems for automatic brain segmentation algorithms, andas a result, poor perfusion ROI detection affects both quantitativemeasurements and visual assessment of perfusion data. In the proposed approachimage segmentation is based on CUSUM filter usage that was adapted to beapplicable to process DSC perfusion MR images. The result of segmentation is abinary mask of brain ROI that is generated via usage of brain boundarylocation. Each point of the boundary between the brain and surrounding tissuesis detected as a change-point by CUSUM filter. Proposed adopted CUSUM filteroperates by accumulating the deviations between the observed and expectedintensities of image points at the time of moving on a trajectory. Motiontrajectory is created by the iterative change of movement direction inside thebackground region in order to reach brain region, and vice versa after boundarycrossing. Proposed segmentation approach was evaluated with Dice indexcomparing obtained results to the reference standard. Manually marked brainregion pixels (reference standard), as well as visual inspection of detectedwith CUSUM filter usage brain ROI, were provided by experienced radiologists.The results showed that proposed approach is suitable to be used for brain ROIdetection from DSC perfusion MR images of a human head with abnormal brainanatomy and can, therefore, be applied in the DSC perfusion data analysis. ", "id2": "683", "id3": "None"}
{"id2": 1068, "id3": "683", "content": "This paper presents a new approach for relatively accurate brain region ofinterest (ROI) detection from dynamic susceptibility contrast (DSC) perfusionmagnetic resonance (MR) images of a human head with abnormal brain anatomy.Such images produce problems for automatic brain segmentation algorithms, andas a result, poor perfusion ROI detection affects both quantitativemeasurements and visual assessment of perfusion data. In the proposed approachimage segmentation is based on CUSUM filter usage that was adapted to beapplicable to process DSC perfusion MR images. The result of segmentation is abinary mask of brain ROI that is generated via usage of brain boundarylocation. Each point of the boundary between the brain and surrounding tissuesis detected as a change-point by CUSUM filter. Proposed adopted CUSUM filteroperates by accumulating the deviations between the observed and expectedintensities of image points at the time of moving on a trajectory. Motiontrajectory is created by the iterative change of movement direction inside thebackground region in order to reach brain region, and vice versa after boundarycrossing. Proposed segmentation approach was evaluated with Dice indexcomparing obtained results to the reference standard. Manually marked brainregion pixels (reference standard), as well as visual inspection of detectedwith CUSUM filter usage brain ROI, were provided by experienced radiologists.The results showed that proposed approach is suitable to be used for brain ROIdetection from DSC perfusion MR images of a human head with abnormal brainanatomy and can, therefore, be applied in the DSC perfusion data analysis."}
{"id": "685", "content": "Over the past few years, deep learning techniques have achieved tremendoussuccess in many visual understanding tasks such as object detection, imagesegmentation, and caption generation. Despite this thriving in computer visionand natural language processing, deep learning has not yet shown significantimpact in robotics. Due to the gap between theory and application, there aremany challenges when applying the results of deep learning to the real roboticsystems. In this study, our long-term goal is to bridge the gap betweencomputer vision and robotics by developing visual methods that can be used inreal robots. In particular, this work tackles two fundamental visual problemsfor autonomous robotic manipulation: affordance detection and fine-grainedaction understanding. Theoretically, we propose different deep architectures tofurther improves the state of the art in each problem. Empirically, we showthat the outcomes of our proposed methods can be applied in real robots andallow them to perform useful manipulation tasks. ", "id2": "684", "id3": "None"}
{"id2": 1069, "id3": "684", "content": "Over the past few years, deep learning techniques have achieved tremendoussuccess in many visual understanding tasks such as object detection, imagesegmentation, and caption generation. Despite this thriving in computer visionand natural language processing, deep learning has not yet shown significantimpact in robotics. Due to the gap between theory and application, there aremany challenges when applying the results of deep learning to the real roboticsystems. In this study, our long-term goal is to bridge the gap betweencomputer vision and robotics by developing visual methods that can be used inreal robots. In particular, this work tackles two fundamental visual problemsfor autonomous robotic manipulation: affordance detection and fine-grainedaction understanding. Theoretically, we propose different deep architectures tofurther improves the state of the art in each problem. Empirically, we showthat the outcomes of our proposed methods can be applied in real robots andallow them to perform useful manipulation tasks."}
{"id": "686", "content": "In this paper we describe a new mobile architecture, MobileNetV2, thatimproves the state of the art performance of mobile models on multiple tasksand benchmarks as well as across a spectrum of different model sizes. We alsodescribe efficient ways of applying these mobile models to object detection ina novel framework we call SSDLite. Additionally, we demonstrate how to buildmobile semantic segmentation models through a reduced form of DeepLabv3 whichwe call Mobile DeepLabv3.  The MobileNetV2 architecture is based on an inverted residual structure wherethe input and output of the residual block are thin bottleneck layers oppositeto traditional residual models which use expanded representations in the inputan MobileNetV2 uses lightweight depthwise convolutions to filter features inthe intermediate expansion layer. Additionally, we find that it is important toremove non-linearities in the narrow layers in order to maintainrepresentational power. We demonstrate that this improves performance andprovide an intuition that led to this design. Finally, our approach allowsdecoupling of the input/output domains from the expressiveness of thetransformation, which provides a convenient framework for further analysis. Wemeasure our performance on Imagenet classification, COCO object detection, VOCimage segmentation. We evaluate the trade-offs between accuracy, and number ofoperations measured by multiply-adds (MAdd), as well as the number ofparameters ", "id2": "685", "id3": "None"}
{"id": "687", "content": "In this paper, we investigate how to learn a suitable representation ofsatellite image time series in an unsupervised manner by leveraging largeamounts of unlabeled data. Additionally , we aim to disentangle therepresentation of time series into two representations: a shared representationthat captures the common information between the images of a time series and anexclusive representation that contains the specific information of each imageof the time series. To address these issues, we propose a model that combines anovel component called cross-domain autoencoders with the variationalautoencoder (VAE) and generative ad-versarial network (GAN) methods. In orderto learn disentangled representations of time series, our model learns themultimodal image-to-image translation task. We train our model using satelliteimage time series from the Sentinel-2 mission. Several experiments are carriedout to evaluate the obtained representations. We show that these disentangledrepresentations can be very useful to perform multiple tasks such as imageclassification, image retrieval, image segmentation and change detection. ", "id2": "686", "id3": "None"}
{"id": "688", "content": "Image segmentation plays an essential role in medicine for both diagnosticand interventional tasks. Segmentation approaches are either manual,semi-automated or fully-automated. Manual segmentation offers full control overthe quality of the results, but is tedious, time consuming and prone tooperator bias. Fully automated methods require no human effort, but oftendeliver sub-optimal results without providing users with the means to makecorrections. Semi-automated approaches keep users in control of the results byproviding means for interaction, but the main challenge is to offer a goodtrade-off between precision and required interaction. In this paper we presenta deep learning (DL) based semi-automated segmentation approach that aims to bea smart interactive tool for region of interest delineation in medicalimages. We demonstrate its use for segmenting multiple organs on computedtomography (CT) of the abdomen. Our approach solves some of the most pressingclinical challenges: (i) it requires only one to a few user clicks to deliverexcellent 2D segmentations in a fast and reliable fashion; (ii) it cangeneralize to previously unseen structures and corner cases; (iii) itdelivers results that can be corrected quickly in a smart and intuitive way upto an arbitrary degree of precision chosen by the user and (iv) ensures highaccuracy. We present our approach and compare it to other techniques andprevious work to show the advantages brought by our method. ", "id2": "687", "id3": "None"}
{"id": "689", "content": "Dilated Convolutions have been shown to be highly useful for the task ofimage segmentation. By introducing gaps into convolutional filters, they enablethe use of larger receptive fields without increasing the original kernel size.Even though this allows for the inexpensive capturing of features at differentscales, the structure of the dilated convolutional filter leads to a loss ofinformation. We hypothesise that inexpensive modifications to DilatedConvolutional Neural Networks, such as additional averaging layers, couldovercome this limitation. In this project we test this hypothesis by evaluatingthe effect of these modifications for a state-of-the art image segmentationsystem and compare them to existing approaches with the same objective. Ourexperiments show that our proposed methods improve the performance of dilatedconvolutions for image segmentation. Crucially, our modifications achieve theseresults at a much lower computational cost than previous smoothing approaches. ", "id2": "688", "id3": "None"}
{"id": "690", "content": "Plant root research can provide a way to attain stress-tolerant crops thatproduce greater yield in a diverse array of conditions. Phenotyping roots insoil is often challenging due to the roots being difficult to access and theuse of time consuming manual methods. Rhizotrons allow visual inspection ofroot growth through transparent surfaces. Agronomists currently manually labelphotographs of roots obtained from rhizotrons using a line-intersect method toobtain root length density and rooting depth measurements which are essentialfor their experiments. We investigate the effectiveness of an automated imagesegmentation method based on the U-Net Convolutional Neural Network (CNN)architecture to enable such measurements. We design a data-set of 50 annotatedChicory (Cichorium intybus L.) root images which we use to train, validate andtest the system and compare against a baseline built using the Frangivesselness filter. We obtain metrics using manual annotations andline-intersect counts. Our results on the held out data show our proposedautomated segmentation system to be a viable solution for detecting andquantifying roots. We evaluate our system using 867 images for which we haveobtained line-intersect counts, attaining a Spearman rank correlation of 0.9748and an $r^2$ of 0.9217. We also achieve an $F_1$ of 0.7 when comparing theautomated segmentation to the manual annotations, with our automatedsegmentation system producing segmentations with higher quality than the manualannotations for large portions of the image. ", "id2": "689", "id3": "None"}
{"id": "691", "content": "Superpixels have become very popular in many computer vision applications.Nevertheless, they remain underexploited since the superpixel decomposition mayproduce irregular and non stable segmentation results due to the dependency tothe image content. In this paper, we first introduce a novel structure, asuperpixel-based patch, called SuperPatch. The proposed structure, based onsuperpixel neighborhood, leads to a robust descriptor since spatial informationis naturally included. The generalization of the PatchMatch method toSuperPatches, named SuperPatchMatch, is introduced. Finally, we propose aframework to perform fast segmentation and labeling from an image database, anddemonstrate the potential of our approach since we outperform, in terms ofcomputational cost and accuracy, the results of state-of-the-art methods onboth face labeling and medical image segmentation. ", "id2": "690", "id3": "None"}
{"id": "692", "content": "Different empirical models have been developed for cloud detection. There isa growing interest in using the ground-based sky/cloud images for this purpose.Several methods exist that perform binary segmentation of clouds. In thispaper, we propose to use a deep learning architecture (U-Net) to performmulti-label sky/cloud image segmentation. The proposed approach outperformsrecent literature by a large margin. ", "id2": "691", "id3": "None"}
{"id": "693", "content": "In this paper, we propose to tackle the problem of reducing discrepanciesbetween multiple domains referred to as multi-source domain adaptation andconsider it under the target shift assumption: in all domains we aim to solve aclassification problem with the same output classes, but with labelsproportions differing across them. This problem, generally ignored in the vastmajority papers on domain adaptation papers, is nevertheless critical inreal-world applications, and we theoretically show its impact on the adaptationsuccess. To address this issue, we design a method based on optimal transport,a theory that has been successfully used to tackle adaptation problems inmachine learning. Our method performs multi-source adaptation and target shiftcorrection simultaneously by learning the class probabilities of the unlabeledtarget sample and the coupling allowing to align two (or more) probabilitydistributions. Experiments on both synthetic and real-world data related tosatellite image segmentation task show the superiority of the proposed methodover the state-of-the-art. ", "id2": "692", "id3": "None"}
{"id": "694", "content": "Large-scale annotation of image segmentation datasets is often prohibitivelyexpensive, as it usually requires a huge number of worker hours to obtainhigh-quality results. Abundant and reliable data has been, however, crucial forthe advances on image understanding tasks achieved by deep learning models. Inthis paper, we introduce FreeLabel, an intuitive open-source web interface thatallows users to obtain high-quality segmentation masks with just a few freehandscribbles, in a matter of seconds. The efficacy of FreeLabel is quantitativelydemonstrated by experimental results on the PASCAL dataset as well as on adataset from the agricultural domain. Designed to benefit the computer visioncommunity, FreeLabel can be used for both crowdsourced or private annotationand has a modular structure that can be easily adapted for any image dataset. ", "id2": "693", "id3": "None"}
{"id": "695", "content": "A fully automated knee MRI segmentation method to study osteoarthritis (OA)was developed using a novel hierarchical set of random forests (RF) classifiersto learn the appearance of cartilage regions and their boundaries. Aneighborhood approximation forest is used first to provide contextual featureto the second-level RF classifier that also considers local features andproduces location-specific costs for the layered optimal graph imagesegmentation of multiple objects and surfaces (LOGISMOS) framework. Double echosteady state (DESS) MRIs used in this work originated from the OsteoarthritisInitiative (OAI) study. Trained on 34 MRIs with varying degrees of OA, theperformance of the learning-based method tested on 108 MRIs showed asignificant reduction in segmentation errors ( emph p $<$0.05) compared withthe conventional gradient-based and single-stage RF-learned costs. The 3DLOGISMOS was extended to longitudinal-3D (4D) to simultaneously segmentmultiple follow-up visits of the same patient. As such, data from alltime-points of the temporal sequence contribute information to a single optimalsolution that utilizes both spatial 3D and temporal contexts. 4D LOGISMOSvalidation on 108 MRIs from baseline and 12 month follow-up scans of 54patients showed a significant reduction in segmentation errors( emph p $<$0.01) compared to 3D. Finally, the potential of 4D LOGISMOS wasfurther explored on the same 54 patients using 5 annual follow-up scansdemonstrating a significant improvement of measuring cartilage thickness( emph p $<$0.01) compared to the sequential 3D approach. ", "id2": "694", "id3": "None"}
{"id": "696", "content": "We present a method to address the challenging problem of segmentation ofmulti-modality isointense infant brain MR images into white matter (WM), graymatter (GM), and cerebrospinal fluid (CSF). Our method is based oncontext-guided, multi-stream fully convolutional networks (FCN), which aftertraining, can directly map a whole volumetric data to its volume-wise labels.In order to alleviate the poten-tial gradient vanishing problem duringtraining, we designed multi-scale deep supervision. Furthermore, contextinfor-mation was used to further improve the performance of our method.Validated on the test data of the MICCAI 2017 Grand Challenge on 6-month infantbrain MRI segmentation (iSeg-2017), our method achieved an average Dice OverlapCoefficient of 95.4%, 91.6% and 89.6% for CSF, GM and WM, respectively. ", "id2": "695", "id3": "None"}
{"id": "697", "content": "Although convolutional neural networks (CNNs) currently dominate competitionson image segmentation, for neuroimaging analysis tasks, more classicalgenerative approaches based on mixture models are still used in practice toparcellate brains. To bridge the gap between the two, in this paper we proposea marriage between a probabilistic generative model, which has been shown to berobust to variability among magnetic resonance (MR) images acquired viadifferent imaging protocols, and a CNN. The link is in the prior distributionover the unknown tissue classes, which are classically modelled using a Markovrandom field. In this work we model the interactions among neighbouring pixelsby a type of recurrent CNN, which can encode more complex spatial interactions.We validate our proposed model on publicly available MR data, from differentcentres, and show that it generalises across imaging protocols. This resultdemonstrates a successful and principled inclusion of a CNN in a generativemodel, which in turn could be adapted by any probabilistic generative approachfor image segmentation. ", "id2": "696", "id3": "None"}
{"id": "698", "content": "We present an approach for fully automatic urinary bladder segmentation in CTimages with artificial neural networks in this study. Automatic medical imageanalysis has become an invaluable tool in the different treatment stages ofdiseases. Especially medical image segmentation plays a vital role, sincesegmentation is often the initial step in an image analysis pipeline. Sincedeep neural networks have made a large impact on the field of image processingin the past years, we use two different deep learning architectures to segmentthe urinary bladder. Both of these architectures are based on pre-trainedclassification networks that are adapted to perform semantic segmentation.Since deep neural networks require a large amount of training data,specifically images and corresponding ground truth labels, we furthermorepropose a method to generate such a suitable training data set from PositronEmission Tomography/Computed Tomography image data. This is done by applyingthresholding to the Positron Emission Tomography data for obtaining a groundtruth and by utilizing data augmentation to enlarge the dataset. In this study,we discuss the influence of data augmentation on the segmentation results, andcompare and evaluate the proposed architectures in terms of qualitative andquantitative segmentation performance. The results presented in this studyallow concluding that deep neural networks can be considered a promisingapproach to segment the urinary bladder in CT images. ", "id2": "697", "id3": "None"}
{"id": "699", "content": "Medical image segmentation is an important step in medical image analysis.With the rapid development of convolutional neural network in image processing,deep learning has been used for medical image segmentation, such as optic discsegmentation, blood vessel detection, lung segmentation, cell segmentation,etc. Previously, U-net based approaches have been proposed. However, theconsecutive pooling and strided convolutional operations lead to the loss ofsome spatial information. In this paper, we propose a context encoder network(referred to as CE-Net) to capture more high-level information and preservespatial information for 2D medical image segmentation. CE-Net mainly containsthree major components: a feature encoder module, a context extractor and afeature decoder module. We use pretrained ResNet block as the fixed featureextractor. The context extractor module is formed by a newly proposed denseatrous convolution (DAC) block and residual multi-kernel pooling (RMP) block.We applied the proposed CE-Net to different 2D medical image segmentationtasks. Comprehensive results show that the proposed method outperforms theoriginal U-Net method and other state-of-the-art methods for optic discsegmentation, vessel detection, lung segmentation, cell contour segmentationand retinal optical coherence tomography layer segmentation. ", "id2": "698", "id3": "None"}
{"id": "700", "content": "Difficult image segmentation problems, for instance left atrium MRI, can beaddressed by incorporating shape priors to find solutions that are consistentwith known objects. Nonetheless, a single multivariate Gaussian is not anadequate model in cases with significant nonlinear shape variation or where theprior distribution is multimodal. Nonparametric density estimation is moregeneral, but has a ravenous appetite for training samples and poses seriouschallenges in optimization, especially in high dimensional spaces. Here, wepropose a maximum-a-posteriori formulation that relies on a generative imagemodel by incorporating both local intensity and global shape priors. We usedeep autoencoders to capture the complex intensity distribution while avoidingthe careful selection of hand-crafted features. We formulate the shape prior asa mixture of Gaussians and learn the corresponding parameters in ahigh-dimensional shape space rather than pre-projecting onto a low-dimensionalsubspace. In segmentation, we treat the identity of the mixture component as alatent variable and marginalize it within a generalizedexpectation-maximization framework. We present a conditional maximization-basedscheme that alternates between a closed-form solution for component-specificshape parameters that provides a global update-based optimization strategy, andan intensity-based energy minimization that translates the global notion of anonlinear shape prior into a set of local penalties. We demonstrate ourapproach on the left atrial segmentation from gadolinium-enhanced MRI, which isuseful in quantifying the atrial geometry in patients with atrial fibrillation. ", "id2": "699", "id3": "None"}
{"id": "701", "content": "Accurate segmentation of brain tissue in magnetic resonance images (MRI) is adiffcult task due to different types of brain abnormalities. Using informationand features from multimodal MRI including T1, T1-weighted inversion recovery(T1-IR) and T2-FLAIR and differential geometric features including the Jacobiandeterminant(JD) and the curl vector(CV) derived from T1 modality can result ina more accurate analysis of brain images. In this paper, we use thedifferential geometric information including JD and CV as image characteristicsto measure the differences between different MRI images, which represent localsize changes and local rotations of the brain image, and we can use them as oneCNN channel with other three modalities (T1-weighted, T1-IR and T2-FLAIR) toget more accurate results of brain segmentation. We test this method on twodatasets including IBSR dataset and MRBrainS datasets based on the deepvoxelwise residual network, namely VoxResNet, and obtain excellent improvementover single modality or three modalities and increases averageDSC(Cerebrospinal Fluid (CSF), Gray Matter (GM) and White Matter (WM)) by about1.5% on the well-known MRBrainS18 dataset and about 2.5% on the IBSR dataset.Moreover, we discuss that one modality combined with its JD or CV informationcan replace the segmentation effect of three modalities, which can providemedical conveniences for doctor to diagnose because only to extract T1-modalityMRI image of patients. Finally, we also compare the segmentation performance ofour method in two networks, VoxResNet and U-Net network. The results showVoxResNet has a better performance than U-Net network with our method in brainMRI segmentation. We believe the proposed method can advance the performance inbrain segmentation and clinical diagnosis. ", "id2": "700", "id3": "None"}
{"id": "702", "content": "Human motion capture data has been widely used in data-driven characteranimation. In order to generate realistic, natural-looking motions, mostdata-driven approaches require considerable efforts of pre-processing,including motion segmentation and annotation. Existing (semi-) automaticsolutions either require hand-crafted features for motion segmentation or donot produce the semantic annotations required for motion synthesis and buildinglarge-scale motion databases. In addition, human labeled annotation datasuffers from inter- and intra-labeler inconsistencies by design. We propose asemi-automatic framework for semantic segmentation of motion capture data basedon supervised machine learning techniques. It first transforms a motion capturesequence into a motion image and applies a convolutional neural network forimage segmentation. Dilated temporal convolutions enable the extraction oftemporal information from a large receptive field. Our model outperforms twostate-of-the-art models for action segmentation, as well as a popular networkfor sequence modeling. Most of all, our method is very robust under noisy andinaccurate training labels and thus can handle human errors during the labelingprocess. ", "id2": "701", "id3": "None"}
{"id2": 1070, "id3": "701", "content": "Human motion capture data has been widely used in data-driven characteranimation. In order to generate realistic, natural-looking motions, mostdata-driven approaches require considerable efforts of pre-processing,including motion segmentation and annotation. Existing (semi-) automaticsolutions either require hand-crafted features for motion segmentation or donot produce the semantic annotations required for motion synthesis and buildinglarge-scale motion databases. In addition, human labeled annotation datasuffers from inter- and intra-labeler inconsistencies by design. We propose asemi-automatic framework for semantic segmentation of motion capture data basedon supervised machine learning techniques. It first transforms a motion capturesequence into a motion image and applies a convolutional neural network forimage segmentation. Dilated temporal convolutions enable the extraction oftemporal information from a large receptive field. Our model outperforms twostate-of-the-art models for action segmentation, as well as a popular networkfor sequence modeling. Most of all, our method is very robust under noisy andinaccurate training labels and thus can handle human errors during the labelingprocess."}
{"id": "703", "content": "The main obstacle to weakly supervised semantic image segmentation is thedifficulty of obtaining pixel-level information from coarse image-levelannotations. Most methods based on image-level annotations use localizationmaps obtained from the classifier, but these only focus on the smalldiscriminative parts of objects and do not capture precise boundaries.FickleNet explores diverse combinations of locations on feature maps created bygeneric deep neural networks. It selects hidden units randomly and then usesthem to obtain activation scores for image classification. FickleNet implicitlylearns the coherence of each location in the feature maps, resulting in alocalization map which identifies both discriminative and other parts ofobjects. The ensemble effects are obtained from a single network by selectingrandom hidden unit pairs, which means that a variety of localization maps aregenerated from a single image. Our approach does not require any additionaltraining steps and only adds a simple layer to a standard convolutional neuralnetwork; nevertheless it outperforms recent comparable techniques on the PascalVOC 2012 benchmark in both weakly and semi-supervised settings. ", "id2": "702", "id3": "None"}
{"id": "704", "content": "Recently, dense connections have attracted substantial attention in computervision because they facilitate gradient flow and implicit deep supervisionduring training. Particularly, DenseNet, which connects each layer to everyother layer in a feed-forward fashion, has shown impressive performances innatural image classification tasks. We propose HyperDenseNet, a 3D fullyconvolutional neural network that extends the definition of dense connectivityto multi-modal segmentation problems. Each imaging modality has a path, anddense connections occur not only between the pairs of layers within the samepath, but also between those across different paths. This contrasts with theexisting multi-modal CNN approaches, in which modeling several modalitiesrelies entirely on a single joint layer (or level of abstraction) for fusion,typically either at the input or at the output of the network. Therefore, theproposed network has total freedom to learn more complex combinations betweenthe modalities, within and in-between all the levels of abstraction, whichincreases significantly the learning representation. We report extensiveevaluations over two different and highly competitive multi-modal brain tissuesegmentation challenges, iSEG 2017 and MRBrainS 2013, with the former focusingon 6-month infant data and the latter on adult images. HyperDenseNet yieldedsignificant improvements over many state-of-the-art segmentation networks,ranking at the top on both benchmarks. We further provide a comprehensiveexperimental analysis of features re-use, which confirms the importance ofhyper-dense connections in multi-modal representation learning. Our code ispublicly available at https://www.github.com/josedolz/HyperDenseNet. ", "id2": "703", "id3": "None"}
{"id": "705", "content": "Supervised training a deep neural network aims to teach the network tomimic human visual perception that is represented by image-and-label pairs inthe training data. Superpixelized (SP) images are visually perceivable tohumans, but a conventionally trained deep learning model often performs poorlywhen working on SP images. To better mimic human visual perception, we think itis desirable for the deep learning model to be able to perceive not only rawimages but also SP images. In this paper, we propose a new superpixel-baseddata augmentation (SPDA) method for training deep learning models forbiomedical image segmentation. Our method applies a superpixel generationscheme to all the original training images to generate superpixelized images.The SP images thus obtained are then jointly used with the original trainingimages to train a deep learning model. Our experiments of SPDA on fourbiomedical image datasets show that SPDA is effective and can consistentlyimprove the performance of state-of-the-art fully convolutional networks forbiomedical image segmentation in 2D and 3D images. Additional studies alsodemonstrate that SPDA can practically reduce the generalization gap. ", "id2": "704", "id3": "None"}
{"id": "706", "content": "Various saliency detection algorithms from color images have been proposed tomimic eye fixation or attentive object detection response of human observersfor the same scenes. However, developments on hyperspectral imaging systemsenable us to obtain redundant spectral information of the observed scenes fromthe reflected light source from objects. A few studies using low-level featureson hyperspectral images demonstrated that salient object detection can beachieved. In this work, we proposed a salient object detection model onhyperspectral images by applying manifold ranking (MR) on self-supervisedConvolutional Neural Network (CNN) features (high-level features) fromunsupervised image segmentation task. Self-supervision of CNN continues untilclustering loss or saliency maps converges to a defined error between eachiteration. Finally, saliency estimations is done as the saliency map at lastiteration when the self-supervision procedure terminates with convergence.Experimental evaluations demonstrated that proposed saliency detectionalgorithm on hyperspectral images is outperforming state-of-the-artshyperspectral saliency models including the original MR based saliency model. ", "id2": "705", "id3": "None"}
{"id": "707", "content": "This work examines the use of a fully convolutional net (FCN) to find animage segment, given a pixel within this segment region. The net receives animage, a point in the image and a region of interest (RoI ) mask. The netoutput is a binary mask of the segment in which the point is located. Theregion where the segment can be found is contained within the input RoI mask.Full image segmentation can be achieved by running this net sequentially,region-by-region on the image, and stitching the output segments into a singlesegmentation map. This simple method addresses two major challenges of imagesegmentation: 1) Segmentation of unknown categories that were not included inthe training set. 2) Segmentation of both individual object instances (things)and non-objects (stuff), such as sky and vegetation. Hence, if the pointerpixel is located within a person in a group, the net will output a mask thatcovers that individual person; if the pointer point is located within the skyregion, the net returns the region of the sky in the image. This is true evenif no example for sky or person appeared in the training set. The net wastested and trained on the COCO panoptic dataset and achieved 67% IOU forsegmentation of familiar classes (that were part of the net training set) and53% IOU for segmentation of unfamiliar classes (that were not included in thetraining). ", "id2": "706", "id3": "None"}
{"id": "708", "content": "Dense 3D visual mapping estimates as many as possible pixel depths, for eachimage. This results in very dense point clouds that often contain redundant andnoisy information, especially for surfaces that are roughly planar, forinstance, the ground or the walls in the scene. In this paper we leverage onsemantic image segmentation to discriminate which regions of the scene requiresimplification and which should be kept at high level of details. We proposefour different point cloud simplification methods which decimate the perceivedpoint cloud by relying on class-specific local and global statistics stillmaintaining more points in the proximity of class boundaries to preserve theinfra-class edges and discontinuities. 3D dense model is obtained by fusing thepoint clouds in a 3D Delaunay Triangulation to deal with variable point clouddensity. In the experimental evaluation we have shown that, by leveraging onsemantics, it is possible to simplify the model and diminish the noiseaffecting the point clouds. ", "id2": "707", "id3": "None"}
{"id": "709", "content": "Segmentation of colorectal cancerous regions from 3D Magnetic Resonance (MR)images is a crucial procedure for radiotherapy which conventionally requiresaccurate delineation of tumour boundaries at an expense of labor, time andreproducibility. While deep learning based methods serve good baselines in 3Dimage segmentation tasks, small applicable patch size limits effectivereceptive field and degrades segmentation performance. In addition, Regions ofinterest (RoIs) localization from large whole volume 3D images serves as apreceding operation that brings about multiple benefits in terms of speed,target completeness, reduction of false positives. Distinct from sliding windowor non-joint localization-segmentation based models, we propose a novelmultitask framework referred to as 3D RoI-aware U-Net (3D RU-Net), for RoIlocalization and in-region segmentation where the two tasks share one backboneencoder network. With the region proposals from the encoder, we cropmulti-level RoI in-region features from the encoder to form a GPUmemory-efficient decoder for detailpreserving segmentation and thereforeenlarged applicable volume size and effective receptive field. To effectivelytrain the model, we designed a Dice formulated loss function for theglobal-to-local multi-task learning procedure. Based on the efficiency gains,we went on to ensemble models with different receptive fields to achieve evenhigher performance costing minor extra computational expensiveness. Extensiveexperiments were conducted on 64 cancerous cases with a four-foldcross-validation, and the results showed significant superiority in terms ofaccuracy and efficiency over conventional frameworks. In conclusion, theproposed method has a huge potential for extension to other 3D objectsegmentation tasks from medical images due to its inherent generalizability.The code for the proposed method is publicly available. ", "id2": "708", "id3": "None"}
{"id": "710", "content": "Optimal surface segmentation is a state-of-the-art method used forsegmentation of multiple globally optimal surfaces in volumetric datasets. Themethod is widely used in numerous medical image segmentation applications.However, nodes in the graph based optimal surface segmentation method typicallyencode uniformly distributed orthogonal voxels of the volume. Thus thesegmentation cannot attain an accuracy greater than a single unit voxel, i.e.the distance between two adjoining nodes in graph space. Segmentation accuracyhigher than a unit voxel is achievable by exploiting partial volume informationin the voxels which shall result in non-equidistant spacing between adjoininggraph nodes. This paper reports a generalized graph based multiple surfacesegmentation method with convex priors which can optimally segment the targetsurfaces in an irregularly sampled space. The proposed method allowsnon-equidistant spacing between the adjoining graph nodes to achieve subvoxelsegmentation accuracy by utilizing the partial volume information in thevoxels. The partial volume information in the voxels is exploited by computinga displacement field from the original volume data to identify thesubvoxel-accurate centers within each voxel resulting in non-equidistantspacing between the adjoining graph nodes. The smoothness of each surfacemodeled as a convex constraint governs the connectivity and regularity of thesurface. We employ an edge-based graph representation to incorporate thenecessary constraints and the globally optimal solution is obtained bycomputing a minimum s-t cut. The proposed method was validated on 10intravascular multi-frame ultrasound image datasets for subvoxel segmentationaccuracy. In all cases, the approach yielded highly accurate results. Ourapproach can be readily extended to higher-dimensional segmentations. ", "id2": "709", "id3": "None"}
{"id": "711", "content": "The encoder-decoder framework is state-of-the-art for offline semantic imagesegmentation. Since the rise in autonomous systems, real-time computation isincreasingly desirable. In this paper, we introduce fast segmentationconvolutional neural network (Fast-SCNN), an above real-time semanticsegmentation model on high resolution image data (1024x2048px) suited toefficient computation on embedded devices with low memory. Building on existingtwo-branch methods for fast segmentation, we introduce our learning todownsample module which computes low-level features for multiple resolutionbranches simultaneously. Our network combines spatial detail at high resolutionwith deep features extracted at lower resolution, yielding an accuracy of 68.0%mean intersection over union at 123.5 frames per second on Cityscapes. We alsoshow that large scale pre-training is unnecessary. We thoroughly validate ourmetric in experiments with ImageNet pre-training and the coarse labeled data ofCityscapes. Finally, we show even faster computation with competitive resultson subsampled inputs, without any network modifications. ", "id2": "710", "id3": "None"}
{"id": "712", "content": "Medical image segmentation being a substantial component of image processingplays a significant role to analyze gross anatomy, to locate an infirmity andto plan the surgical procedures. Segmentation of brain Magnetic ResonanceImaging (MRI) is of considerable importance for the accurate diagnosis.However, precise and accurate segmentation of brain MRI is a challenging task.Here, we present an efficient framework for segmentation of brain MR images.For this purpose, Gabor transform method is used to compute features of brainMRI. Then, these features are classified by using four different classifiersi.e., Incremental Supervised Neural Network (ISNN), K-Nearest Neighbor (KNN),Probabilistic Neural Network (PNN), and Support Vector Machine (SVM).Performance of these classifiers is investigated over different images of brainMRI and the variation in the performance of these classifiers is observed fordifferent brain tissues. Thus, we proposed a rule-based hybrid approach tosegment brain MRI. Experimental results show that the performance of theseclassifiers varies over each tissue MRI and the proposed rule-based hybridapproach exhibits better segmentation of brain MRI tissues. ", "id2": "711", "id3": "None"}
{"id": "713", "content": "In recent years Deep Learning has brought about a breakthrough in MedicalImage Segmentation. U-Net is the most prominent deep network in this regard,which has been the most popular architecture in the medical imaging community.Despite outstanding overall performance in segmenting multimodal medicalimages, from extensive experimentations on challenging datasets, we found outthat the classical U-Net architecture seems to be lacking in certain aspects.Therefore, we propose some modifications to improve upon the alreadystate-of-the-art U-Net model. Hence, following the modifications we develop anovel architecture MultiResUNet as the potential successor to the successfulU-Net architecture. We have compared our proposed architecture MultiResUNetwith the classical U-Net on a vast repertoire of multimodal medical images.Albeit slight improvements in the cases of ideal images, a remarkable gain inperformance has been attained for challenging images. We have evaluated ourmodel on five different datasets, each with their own unique challenges, andhave obtained a relative improvement in performance of 10.15%, 5.07%, 2.63%,1.41%, and 0.62% respectively. ", "id2": "712", "id3": "None"}
{"id": "714", "content": "We present a method for highly efficient landmark detection that combinesdeep convolutional neural networks with well established model-based fittingalgorithms. Motivated by established model-based fitting methods such as activeshapes, we use a PCA of the landmark positions to allow generative modeling offacial landmarks. Instead of computing the model parameters using iterativeoptimization, the PCA is included in a deep neural network using a novel layertype. The network predicts model parameters in a single forward pass, therebyallowing facial landmark detection at several hundreds of frames per second.Our architecture allows direct end-to-end training of a model-based landmarkdetection method and shows that deep neural networks can be used to reliablypredict model parameters directly without the need for an iterativeoptimization. The method is evaluated on different datasets for facial landmarkdetection and medical image segmentation. PyTorch code is freely available athttps://github.com/justusschock/shapenet ", "id2": "713", "id3": "None"}
{"id": "715", "content": "We propose a novel technique to incorporate attention within convolutionalneural networks using feature maps generated by a separate convolutionalautoencoder. Our attention architecture is well suited for incorporation withdeep convolutional networks. We evaluate our model on benchmark segmentationdatasets in skin cancer segmentation and lung lesion segmentation. Results showhighly competitive performance when compared with U-Net and its residualvariant. ", "id2": "714", "id3": "None"}
{"id": "716", "content": "Deep convolutional networks have achieved the state-of-the-art for semanticimage segmentation tasks. However, training these networks requires access todensely labeled images, which are known to be very expensive to obtain. On theother hand, the web provides an almost unlimited source of images annotated atthe image level. How can one utilize this much larger weakly annotated set fortasks that require dense labeling? Prior work often relied on localizationcues, such as saliency maps, objectness priors, bounding boxes etc., to addressthis challenging problem. In this paper, we propose a model that generatesauxiliary labels for each image, while simultaneously forcing the output of theCNN to satisfy the mean-field constraints imposed by a conditional randomfield. We show that one can enforce the CRF constraints by forcing thedistribution at each pixel to be close to the distribution of its neighbors.This is in stark contrast with methods that compute a recursive expansion ofthe mean-field distribution using a recurrent architecture and train theresultant distribution. Instead, the proposed model adds an extra loss term tothe output of the CNN, and hence, is faster than recursive implementations. Weachieve the state-of-the-art for weakly supervised semantic image segmentationon VOC 2012 dataset, assuming no manually labeled pixel level information isavailable. Furthermore, the incorporation of conditional random fields in CNNincurs little extra time during training. ", "id2": "715", "id3": "None"}
{"id": "717", "content": "The instance segmentation problem intends to precisely detect and delineateobjects in images. Most of the current solutions rely on deep convolutionalneural networks but despite this fact proposed solutions are very diverse. Somesolutions approach the problem as a network problem, where they use severalnetworks or specialize a single network to solve several tasks. A differentapproach tries to solve the problem as an annotation problem, where theinstance information is encoded in a mathematical representation. This workproposes a solution based in the DCME technique to solve the instancesegmentation with a single segmentation network. Different from others, thesegmentation network decoder is not specialized in a multi-task network.Instead, the network encoder is repurposed to classify image objects, reducingthe computational cost of the solution. ", "id2": "716", "id3": "None"}
{"id": "718", "content": "In the deep metric learning approach to image segmentation, a convolutionalnet densely generates feature vectors at the pixels of an image. Pairs offeature vectors are trained to be similar or different, depending on whetherthe corresponding pixels belong to same or different ground truth segments. Tosegment a new image, the feature vectors are computed and clustered. Bothempirically and theoretically, it is unclear whether or when deep metriclearning is superior to the more conventional approach of directly predictingan affinity graph with a convolutional net. We compare the two approaches usingbrain images from serial section electron microscopy images, which constitutean especially challenging example of instance segmentation. We first show thatseed-based postprocessing of the feature vectors, as originally proposed,produces inferior accuracy because it is difficult for the convolutional net topredict feature vectors that remain uniform across large objects. Then weconsider postprocessing by thresholding a nearest neighbor graph followed byconnected components. In this case, segmentations from a metric graph turnout to be competitive or even superior to segmentations from a directlypredicted affinity graph. To explain these findings theoretically, we invokethe property that the metric function satisfies the triangle inequality. Thenwe show with an example where this constraint suppresses noise, causingconnected components to more robustly segment a metric graph than anunconstrained affinity graph. ", "id2": "717", "id3": "None"}
{"id": "719", "content": "Despite the state-of-the-art performance for medical image segmentation, deepconvolutional neural networks (CNNs) have rarely provided uncertaintyestimations regarding their segmentation outputs, e.g., model (epistemic) andimage-based (aleatoric) uncertainties. In this work, we analyze these differenttypes of uncertainties for CNN-based 2D and 3D medical image segmentationtasks. We additionally propose a test-time augmentation-based aleatoricuncertainty to analyze the effect of different transformations of the inputimage on the segmentation output. Test-time augmentation has been previouslyused to improve segmentation accuracy, yet not been formulated in a consistentmathematical framework. Hence, we also propose a theoretical formulation oftest-time augmentation, where a distribution of the prediction is estimated byMonte Carlo simulation with prior distributions of parameters in an imageacquisition model that involves image transformations and noise. We compare andcombine our proposed aleatoric uncertainty with model uncertainty. Experimentswith segmentation of fetal brains and brain tumors from 2D and 3D MagneticResonance Images (MRI) showed that 1) the test-time augmentation-basedaleatoric uncertainty provides a better uncertainty estimation than calculatingthe test-time dropout-based model uncertainty alone and helps to reduceoverconfident incorrect predictions, and 2) our test-time augmentationoutperforms a single-prediction baseline and dropout-based multiplepredictions. ", "id2": "718", "id3": "None"}
{"id2": 1071, "id3": "718", "content": "Despite the state-of-the-art performance for medical image segmentation, deepconvolutional neural networks (CNNs) have rarely provided uncertaintyestimations regarding their segmentation outputs, e.g., model (epistemic) andimage-based (aleatoric) uncertainties. In this work, we analyze these differenttypes of uncertainties for CNN-based 2D and 3D medical image segmentationtasks. We additionally propose a test-time augmentation-based aleatoricuncertainty to analyze the effect of different transformations of the inputimage on the segmentation output. Test-time augmentation has been previouslyused to improve segmentation accuracy, yet not been formulated in a consistentmathematical framework. Hence, we also propose a theoretical formulation oftest-time augmentation, where a distribution of the prediction is estimated byMonte Carlo simulation with prior distributions of parameters in an imageacquisition model that involves image transformations and noise. We compare andcombine our proposed aleatoric uncertainty with model uncertainty. Experimentswith segmentation of fetal brains and brain tumors from 2D and 3D MagneticResonance Images (MRI) showed that 1) the test-time augmentation-basedaleatoric uncertainty provides a better uncertainty estimation than calculatingthe test-time dropout-based model uncertainty alone and helps to reduceoverconfident incorrect predictions, and 2) our test-time augmentationoutperforms a single-prediction baseline and dropout-based multiplepredictions."}
{"id": "720", "content": "We present a novel method to explicitly incorporate topological priorknowledge into deep learning based segmentation, which is, to our knowledge,the first work to do so. Our method uses the concept of persistent homology, atool from topological data analysis, to capture high-level topologicalcharacteristics of segmentation results in a way which is differentiable withrespect to the pixelwise probability of being assigned to a given class. Thetopological prior knowledge consists of the sequence of desired Betti numbersof the segmentation. As a proof-of-concept we demonstrate our approach byapplying it to the problem of left-ventricle segmentation of cardiac MR imagesof 500 subjects from the UK Biobank dataset, where we show that it improvessegmentation performance in terms of topological correctness withoutsacrificing pixelwise accuracy. ", "id2": "719", "id3": "None"}
{"id": "721", "content": "Current state-of-the-art deep learning segmentation methods have not yet madea broad entrance into the clinical setting in spite of high demand for suchautomatic methods. One important reason is the lack of reliability caused bymodels that fail unnoticed and often locally produce anatomically implausibleresults that medical experts would not make. This paper presents an automaticimage segmentation method based on (Bayesian) dilated convolutional networks(DCNN) that generate segmentation masks and spatial uncertainty maps for theinput image at hand. The method was trained and evaluated using segmentation ofthe left ventricle (LV) cavity, right ventricle (RV) endocardium and myocardium(Myo) at end-diastole (ED) and end-systole (ES) in 100 cardiac 2D MR scans fromthe MICCAI 2017 Challenge (ACDC). Combining segmentations and uncertainty mapsand employing a human-in-the-loop setting, we provide evidence that image areasindicated as highly uncertain regarding the obtained segmentation almostentirely cover regions of incorrect segmentations. The fused information can beharnessed to increase segmentation performance. Our results reveal that we canobtain valuable spatial uncertainty maps with low computational effort usingDCNNs. ", "id2": "720", "id3": "None"}
{"id": "722", "content": "Background: The trend towards large-scale studies including populationimaging poses new challenges in terms of quality control (QC). This is aparticular issue when automatic processing tools, e.g. image segmentationmethods, are employed to derive quantitative measures or biomarkers for lateranalyses. Manual inspection and visual QC of each segmentation isnt feasibleat large scale. However, its important to be able to automatically detect whena segmentation method fails so as to avoid inclusion of wrong measurements intosubsequent analyses which could lead to incorrect conclusions. Methods: Toovercome this challenge, we explore an approach for predicting segmentationquality based on Reverse Classification Accuracy, which enables us todiscriminate between successful and failed segmentations on a per-cases basis.We validate this approach on a new, large-scale manually-annotated set of 4,800cardiac magnetic resonance scans. We then apply our method to a large cohort of7,250 cardiac MRI on which we have performed manual QC. Results: We reportresults used for predicting segmentation quality metrics including DiceSimilarity Coefficient (DSC) and surface-distance measures. As initialvalidation, we present data for 400 scans demonstrating 99% accuracy forclassifying low and high quality segmentations using predicted DSC scores. Asfurther validation we show high correlation between real and predicted scoresand 95% classification accuracy on 4,800 scans for which manual segmentationswere available. We mimic real-world application of the method on 7,250 cardiacMRI where we show good agreement between predicted quality metrics and manualvisual QC scores. Conclusions: We show that RCA has the potential for accurateand fully automatic segmentation QC on a per-case basis in the context oflarge-scale population imaging as in the UK Biobank Imaging Study. ", "id2": "721", "id3": "None"}
{"id": "723", "content": "Recently, state-of-the-art results have been achieved in semanticsegmentation using fully convolutional networks (FCNs). Most of these networksemploy encoder-decoder style architecture similar to U-Net and are trained withimages and the corresponding segmentation maps as a pixel-wise classificationtask. Such frameworks only exploit class information by using the ground truthsegmentation maps. In this paper, we propose a multi-task learning frameworkwith the main aim of exploiting structural and spatial information along withthe class information. We modify the decoder part of the FCN to exploit classinformation and the structural information as well. We intend to do this whilealso keeping the parameters of the network as low as possible. We obtain thestructural information using either of the two ways: i) using the contour mapand ii) using the distance map, both of which can be obtained from ground truthsegmentation maps with no additional annotation costs. We also exploredifferent ways in which distance maps can be computed and study the effects ofdifferent distance maps on the segmentation performance. We also experimentextensively on two different medical image segmentation applications: i.e i)using color fundus images for optic disc and cup segmentation and ii) usingendoscopic images for polyp segmentation. Through our experiments, we reportresults comparable to, and in some cases performing better than the currentstate-of-the-art architectures and with an order of 2x reduction in the numberof parameters. ", "id2": "722", "id3": "None"}
{"id": "724", "content": "Image-to-image translation is a long-established and a difficult problem incomputer vision. In this paper we propose an adversarial based model forimage-to-image translation. The regular deep neural-network based methodsperform the task of image-to-image translation by comparing gram matrices andusing image segmentation which requires human intervention. Our generativeadversarial network based model works on a conditional probability approach.This approach makes the image translation independent of any local, global andcontent or style features. In our approach we use a bidirectionalreconstruction model appended with the affine transform factor that helps inconserving the content and photorealism as compared to other models. Theadvantage of using such an approach is that the image-to-image translation issemi-supervised, independant of image segmentation and inherits the propertiesof generative adversarial networks tending to produce realistic. This methodhas proven to produce better results than Multimodal UnsupervisedImage-to-image translation. ", "id2": "723", "id3": "None"}
{"id": "725", "content": "A class of vision problems, less commonly studied, consists of detectingobjects in imagery obtained from physics-based experiments. These objects canspan in 4D (x, y, z, t) and are visible as disturbances (caused due to physicalphenomena) in the image with background distribution being approximatelyuniform. Such objects, occasionally referred to as events, can be consideredas high energy blobs in the image. Unlike the images analyzed in conventionalvision problems, very limited features are associated with such events, andtheir shape, size and count can vary significantly. This poses a challenge onthe use of pre-trained models obtained from supervised approaches.  In this paper, we propose an unsupervised approach involving iterativeclustering based segmentation (ICS) which can detect target objects (events) inreal-time. In this approach, a test image is analyzed over several cycles, andone event is identified per cycle. Each cycle consists of the following steps:(1) image segmentation using a modified k-means clustering method, (2)elimination of empty (with no events) segments based on statistical analysis ofeach segment, (3) merging segments that overlap (correspond to same event), and(4) selecting the strongest event. These four steps are repeated until all theevents have been identified. The ICS approach consists of a fewhyper-parameters that have been chosen based on statistical study performedover a set of test images. The applicability of ICS method is demonstrated onseveral 2D and 3D test examples. ", "id2": "724", "id3": "None"}
{"id": "726", "content": "Ultrasound image compression by preserving speckle-based key information is achallenging task. In this paper, we introduce an ultrasound image compressionframework with the ability to retain realism of speckle appearance despiteachieving very high-density compression factors. The compressor employs atissue segmentation method, transmitting segments along with transducerfrequency, number of samples and image size as essential information requiredfor decompression. The decompressor is based on a convolutional network trainedto generate patho-realistic ultrasound images which convey essentialinformation pertinent to tissue pathology visible in the images. We demonstrategeneralizability of the building blocks using two variants to build thecompressor. We have evaluated the quality of decompressed images usingdistortion losses as well as perception loss and compared it with other off theshelf solutions. The proposed method achieves a compression ratio of $725:1$while preserving the statistical distribution of speckles. This enables imagesegmentation on decompressed images to achieve dice score of $0.89  pm 0.11$,which evidently is not so accurately achievable when images are compressed withcurrent standards like JPEG, JPEG 2000, WebP and BPG. We envision this framework to serve as a roadmap for speckle image compression standards. ", "id2": "725", "id3": "None"}
{"id": "727", "content": "The Encoder-Decoder architecture is a main stream deep learning model forbiomedical image segmentation. The encoder fully compresses the input andgenerates encoded features, and the decoder then produces dense predictionsusing encoded features. However, decoders are still under-explored in sucharchitectures. In this paper, we comprehensively study the state-of-the-artEncoder-Decoder architectures, and propose a new universal decoder, calledcascade decoder, to improve semantic segmentation accuracy. Our cascade decodercan be embedded into existing networks and trained altogether in an end-to-endfashion. The cascade decoder structure aims to conduct more effective decodingof hierarchically encoded features and is more compatible with common encodersthan the known decoders. We replace the decoders of state-of-the-art modelswith our cascade decoder for several challenging biomedical image segmentationtasks, and the considerable improvements achieved demonstrate the efficacy ofour new decoding method. ", "id2": "726", "id3": "None"}
{"id": "728", "content": "Photorealism is a complex concept that cannot easily be formulatedmathematically. Deep Photo Style Transfer is an attempt to transfer the styleof a reference image to a content image while preserving its photorealism. Thisis achieved by introducing a constraint that prevents distortions in thecontent image and by applying the style transfer independently for semanticallydifferent parts of the images. In addition, an automated segmentation processis presented that consists of a neural network based segmentation methodfollowed by a semantic grouping step. To further improve the results a measurefor image aesthetics is used and elaborated. If the content and the style imageare sufficiently similar, the result images look very realistic. With theautomation of the image segmentation the pipeline becomes completelyindependent from any user interaction, which allows for new applications. ", "id2": "727", "id3": "None"}
{"id": "729", "content": "Recent advances in deep learning methods have come to define thestate-of-the-art for many medical imaging applications, surpassing even humanjudgment in several tasks. Those models, however, when trained to reduce theempirical risk on a single domain, fail to generalize when applied to otherdomains, a very common scenario in medical imaging due to the variability ofimages and anatomical structures, even across the same imaging modality. Inthis work, we extend the method of unsupervised domain adaptation usingself-ensembling for the semantic segmentation task and explore multiple facetsof the method on a small and realistic publicly-available magnetic resonance(MRI) dataset. Through an extensive evaluation, we show that self-ensemblingcan indeed improve the generalization of the models even when using a smallamount of unlabelled data. ", "id2": "728", "id3": "None"}
{"id": "730", "content": "In recent years, deep learning has shown performance breakthroughs in manyapplications, such as image detection, image segmentation, pose estimation, andspeech recognition. However, this comes with a major concern: deep networkshave been found to be vulnerable to adversarial examples. Adversarial examplesare slightly modified inputs that are intentionally designed to cause amisclassification by the model. In the domains of images and speech, themodifications are so small that they are not seen or heard by humans, butnevertheless greatly affect the classification of the model.  Deep learning models have been successfully applied to malware detection. Inthis domain, generating adversarial examples is not straightforward, as smallmodifications to the bytes of the file could lead to significant changes in itsfunctionality and validity. We introduce a novel loss function for generatingadversarial examples specifically tailored for discrete input sets, such asexecutable bytes. We modify malicious binaries so that they would be detectedas benign, while preserving their original functionality, by injecting a smallsequence of bytes (payload) in the binary file. We applied this approach to anend-to-end convolutional deep learning malware detection model and show a highrate of detection evasion. Moreover, we show that our generated payload isrobust enough to be transferable within different locations of the same fileand across different files, and that its entropy is low and similar to that ofbenign data sections. ", "id2": "729", "id3": "None"}
{"id": "731", "content": "In order to identify and prevent tea leaf diseases effectively, convolutionneural network (CNN) was used to realize the image recognition of tea diseaseleaves. Firstly, image segmentation and data enhancement are used to preprocessthe images, and then these images were input into the network for training.Secondly, to reach a higher recognition accuracy of CNN, the learning rate anditeration numbers were adjusted frequently and the dropout was added properlyin the case of over-fitting. Finally, the experimental results show that therecognition accuracy of CNN is 93.75%, while the accuracy of SVM and BP neuralnetwork is 89.36% and 87.69% respectively. Therefore, the recognition algorithmbased on CNN is better in classification and can improve the recognitionefficiency of tea leaf diseases effectively. ", "id2": "730", "id3": "None"}
{"id": "732", "content": "Interactive image segmentation is a topic of many studies in imageprocessing. In a conventional approach, a user marks some pixels of theobject(s) of interest and background, and an algorithm propagates these labelsto the rest of the image. This paper presents a new graph-based method forinteractive segmentation with two stages. In the first stage, nodesrepresenting pixels are connected to their $k$-nearest neighbors to build acomplex network with the small-world property to propagate the labels quickly.In the second stage, a regular network in a grid format is used to refine thesegmentation on the object borders. Despite its simplicity, the proposed methodcan perform the task with high accuracy. Computer simulations are performedusing some real-world images to show its effectiveness in both two-classes andmulti-classes problems. It is also applied to all the images from the MicrosoftGrabCut dataset for comparison, and the segmentation accuracy is comparable tothose achieved by some state-of-the-art methods, while it is faster than them.In particular, it outperforms some recent approaches when the user input iscomposed only by a few scribbles draw over the objects. Its computationalcomplexity is only linear on the image size at the best-case scenario andlinearithmic in the worst case. ", "id2": "731", "id3": "None"}
{"id": "733", "content": "We propose a new self-supervised approach to image feature learning frommotion cue. This new approach leverages recent advances in deep learning in twodirections: 1) the success of training deep neural network in estimatingoptical flow in real data using synthetic flow data; and 2) emerging work inlearning image features from motion cues, such as optical flow. Building onthese, we demonstrate that image features can be learned in self-supervision byfirst training an optical flow estimator with synthetic flow data, and thenlearning image features from the estimated flows in real motion data. Wedemonstrate and evaluate this approach on an image segmentation task. Using thelearned image feature representation, the network performs significantly betterthan the ones trained from scratch in few-shot segmentation tasks. ", "id2": "732", "id3": "None"}
{"id": "734", "content": "Optimal decision making with limited or no information in stochasticenvironments where multiple agents interact is a challenging topic in the realmof artificial intelligence. Reinforcement learning (RL) is a popular approachfor arriving at optimal strategies by predicating stimuli, such as the rewardfor following a strategy, on experience. RL is heavily explored in thesingle-agent context, but is a nascent concept in multiagent problems. To thisend, I propose several principled model-free and partially model-basedreinforcement learning approaches for several multiagent settings. In the realmof normative reinforcement learning, I introduce scalable extensions to MonteCarlo exploring starts for partially observable Markov Decision Processes(POMDP), dubbed MCES-P, where I expand the theory and algorithm to themultiagent setting. I first examine MCES-P with probably approximately correct(PAC) bounds in the context of multiagent setting, showing MCESP+PAC holds inthe presence of other agents. I then propose a more sample-efficientmethodology for antagonistic settings, MCESIP+PAC. For cooperative settings, Iextend MCES-P to the Multiagent POMDP, dubbed MCESMP+PAC. I then explore theuse of reinforcement learning as a methodology in searching for optima inrealistic and latent model environments. First, I explore a parameterizedQ-learning approach in modeling humans learning to reason in an uncertain,multiagent environment. Next, I propose an implementation of MCES-P, along withimage segmentation, to create an adaptive team-based reinforcement learningtechnique to positively identify the presence of phenotypically-expressed waterand pathogen stress in crop fields. ", "id2": "733", "id3": "None"}
{"id": "735", "content": "This paper offers three new, open-source, deep learning-based irissegmentation methods, and the methodology how to use irregular segmentationmasks in a conventional Gabor-wavelet-based iris recognition. To train andvalidate the methods, we used a wide spectrum of iris images acquired bydifferent teams and different sensors and offered publicly, including datataken from CASIA-Iris-Interval-v4, BioSec, ND-Iris-0405, UBIRIS,Warsaw-BioBase-Post-Mortem-Iris v2.0 (post-mortem iris images), andND-TWINS-2009-2010 (iris images acquired from identical twins). This variedtraining data should increase the generalization capabilities of the proposedsegmentation techniques. In database-disjoint training and testing, we showthat deep learning-based segmentation outperforms the conventional (OSIRIS)segmentation in terms of Intersection over Union calculated between theobtained results and manually annotated ground-truth. Interestingly, theGabor-based iris matching is not always better when deep learning-basedsegmentation is used, and is on par with the method employing Daugmans basedsegmentation. ", "id2": "734", "id3": "None"}
{"id": "736", "content": "Preparation of high-quality datasets for the urban scene understanding is alabor-intensive task, especially, for datasets designed for the autonomousdriving applications. The application of the coarse ground truth (GT)annotations of these datasets without detriment to the accuracy of semanticimage segmentation (by the mean intersection over union - mIoU) could simplifyand speedup the dataset preparation and model fine tuning before its practicalapplication. Here the results of the comparative analysis for semanticsegmentation accuracy obtained by PSPNet deep learning architecture arepresented for fine and coarse annotated images from Cityscapes dataset. Twoscenarios were investigated: scenario 1 - the fine GT images for training andprediction, and scenario 2 - the fine GT images for training and the coarse GTimages for prediction. The obtained results demonstrated that for the mostimportant classes the mean accuracy values of semantic image segmentation forcoarse GT annotations are higher than for the fine GT ones, and the standarddeviation values are vice versa. It means that for some applications someunimportant classes can be excluded and the model can be tuned further for someclasses and specific regions on the coarse GT dataset without loss of theaccuracy even. Moreover, this opens the perspectives to use deep neuralnetworks for the preparation of such coarse GT datasets. ", "id2": "735", "id3": "None"}
{"id": "737", "content": "For medical image segmentation, most fully convolutional networks (FCNs) needstrong supervision through a large sample of high-quality dense segmentations,which is taxing in terms of costs, time and logistics involved. This burden ofannotation can be alleviated by exploiting weak inexpensive annotations such asbounding boxes and anatomical landmarks. However, it is very difficult to textit a priori  estimate the optimal balance between the number ofannotations needed for each supervision type that leads to maximum performancewith the least annotation cost. To optimize this cost-performance trade off, wepresent a budget-based cost-minimization framework in a mixed-supervisionsetting via dense segmentations, bounding boxes, and landmarks. We propose alinear programming (LP) formulation combined with uncertainty and similaritybased ranking strategy to judiciously select samples to be annotated next foroptimal performance. In the results section, we show that our proposed methodachieves comparable performance to state-of-the-art approaches withsignificantly reduced cost of annotations. ", "id2": "736", "id3": "None"}
{"id": "738", "content": "Recently there has been a lot of work on pruning filters from deepconvolutional neural networks (CNNs) with the intention of reducingcomputations.The key idea is to rank the filters based on a certain criterion(say, l1-norm) and retain only the top ranked filters. Once the low scoringfilters are pruned away the remainder of the network is fine tuned and is shownto give performance comparable to the original unpruned network. In this work,we report experiments which suggest that the comparable performance of thepruned network is not due to the specific criterion chosen but due to theinherent plasticity of deep neural networks which allows them to recover fromthe loss of pruned filters once the rest of the filters are fine-tuned.Specifically we show counter-intuitive results wherein by randomly pruning25-50% filters from deep CNNs we are able to obtain the same performance asobtained by using state-of-the-art pruning methods. We empirically validate ourclaims by doing an exhaustive evaluation with VGG-16 and ResNet-50. We alsoevaluate a real world scenario where a CNN trained on all 1000 ImageNet classesneeds to be tested on only a small set of classes at test time (say, onlyanimals). We create a new benchmark dataset from ImageNet to evaluate suchclass specific pruning and show that even here a random pruning strategy givesclose to state-of-the-art performance. Unlike existing approaches which mainlyfocus on the task of image classification, in this work we also report resultson object detection and image segmentation. We show that using a simple randompruning strategy we can achieve significant speed up in object detection (74%improvement in fps) while retaining the same accuracy as that of the originalFaster RCNN model. Similarly we show that the performance of a prunedSegmentation Network (SegNet) is actually very similar to that of the originalunpruned SegNet. ", "id2": "737", "id3": "None"}
{"id": "739", "content": "Convolutional Neural Networks (CNNs) have achieved state-of-the-artperformance in many different 2D medical image analysis tasks. In clinicalpractice, however, a large part of the medical imaging data available is in 3D.This has motivated the development of 3D CNNs for volumetric image segmentationin order to benefit from more spatial context. Due to GPU memory restrictionscaused by moving to fully 3D, state-of-the-art methods depend onsubvolume/patch processing and the size of the input patch is usually small,limiting the incorporation of larger context information for a betterperformance. In this paper, we propose a novel Holistic DecompositionConvolution (HDC), for an effective and efficient semantic segmentation ofvolumetric images. HDC consists of a periodic down-shuffling operation followedby a conventional 3D convolution. HDC has the advantage of significantlyreducing the size of the data for sub-sequential processing while using all theinformation available in the input irrespective of the down-shuffling factors.Results obtained from comprehensive experiments conducted on hip T1 MR imagesand intervertebral disc T2 MR images demonstrate the efficacy of the presentapproach. ", "id2": "738", "id3": "None"}
{"id": "740", "content": "This article suggests an algorithm of formation a training set for artificialneural network in case of image segmentation. The distinctive feature of thisalgorithm is that it using only one image for segmentation. The segmentationperforms using three-layer perceptron. The main method of the segmentation is amethod of region growing. Neural network is using for get a decision to includepixel into an area or not. Impulse noise is using for generation of a trainingset. Pixels damaged by noise are not related to the same region. Suggestedmethod has been tested with help of computer experiment in automatic andinteractive modes. ", "id2": "739", "id3": "None"}
{"id": "741", "content": "This paper tries to give a gentle introduction to deep learning in medicalimage processing, proceeding from theoretical foundations to applications. Wefirst discuss general reasons for the popularity of deep learning, includingseveral major breakthroughs in computer science. Next, we start reviewing thefundamental basics of the perceptron and neural networks, along with somefundamental theory that is often omitted. Doing so allows us to understand thereasons for the rise of deep learning in many application domains. Obviouslymedical image processing is one of these areas which has been largely affectedby this rapid progress, in particular in image detection and recognition, imagesegmentation, image registration, and computer-aided diagnosis. There are alsorecent trends in physical simulation, modelling, and reconstruction that haveled to astonishing results. Yet, some of these approaches neglect priorknowledge and hence bear the risk of producing implausible results. Theseapparent weaknesses highlight current limitations of deep learning. However, wealso briefly discuss promising approaches that might be able to resolve theseproblems in the future. ", "id2": "740", "id3": "None"}
{"id": "742", "content": "This paper presents a learning to learn approach to figure-ground imagesegmentation. By exploring webly-abundant images of specific visual effects,our method can effectively learn the visual-effect internal representations inan unsupervised manner and uses this knowledge to differentiate the figure fromthe ground in an image. Specifically, we formulate the meta-learning process asa compositional image editing task that learns to imitate a certain visualeffect and derive the corresponding internal representation. Such a generativeprocess can help instantiate the underlying figure-ground notion and enablesthe system to accomplish the intended image segmentation. Whereas existinggenerative methods are mostly tailored to image synthesis or style transfer,our approach offers a flexible learning mechanism to model a general concept offigure-ground segmentation from unorganized images that have no explicitpixel-level annotations. We validate our approach via extensive experiments onsix datasets to demonstrate that the proposed model can be end-to-end trainedwithout ground-truth pixel labeling yet outperforms the existing methods ofunsupervised segmentation tasks. ", "id2": "741", "id3": "None"}
{"id": "743", "content": "Deep neural network models used for medical image segmentation are largebecause they are trained with high-resolution three-dimensional (3D) images.Graphics processing units (GPUs) are widely used to accelerate the trainings.However, the memory on a GPU is not large enough to train the models. A popularapproach to tackling this problem is patch-based method, which divides a largeimage into small patches and trains the models with these small patches.However, this method would degrade the segmentation quality if a target objectspans multiple patches. In this paper, we propose a novel approach for 3Dmedical image segmentation that utilizes the data-swapping, which swaps outintermediate data from GPU memory to CPU memory to enlarge the effective GPUmemory size, for training high-resolution 3D medical images without patching.We carefully tuned parameters in the data-swapping method to obtain the besttraining performance for 3D U-Net, a widely used deep neural network model formedical image segmentation. We applied our tuning to train 3D U-Net withfull-size images of 192 x 192 x 192 voxels in brain tumor dataset. As a result,communication overhead, which is the most important issue, was reduced by17.1%. Compared with the patch-based method for patches of 128 x 128 x 128voxels, our training for full-size images achieved improvement on the mean Dicescore by 4.48% and 5.32 % for detecting whole tumor sub-region and tumor coresub-region, respectively. The total training time was reduced from 164 hours to47 hours, resulting in 3.53 times of acceleration. ", "id2": "742", "id3": "None"}
{"id": "744", "content": "Interactive image segmentation algorithms rely on the user to provideannotations as the guidance. When the task of interactive segmentation isperformed on a small touchscreen device, the requirement of providing preciseannotations could be cumbersome to the user. We design an efficient seedproposal method that actively proposes annotation seeds for the user to label.The user only needs to check which ones of the query seeds are inside theregion of interest (ROI). We enforce the sparsity and diversity criteria on theselection of the query seeds. At each round of interaction the user is onlypresented with a small number of informative query seeds that are far apartfrom each other. As a result, we are able to derive a user friendly interactionmechanism for annotation on small touchscreen devices. The user merely has toswipe through on the ROI-relevant query seeds, which should be easy since thosegestures are commonly used on a touchscreen. The performance of our algorithmis evaluated on six publicly available datasets. The evaluation results showthat our algorithm achieves high segmentation accuracy, with short responsetime and less user feedback. ", "id2": "743", "id3": "None"}
{"id2": 1072, "id3": "743", "content": "Interactive image segmentation algorithms rely on the user to provideannotations as the guidance. When the task of interactive segmentation isperformed on a small touchscreen device, the requirement of providing preciseannotations could be cumbersome to the user. We design an efficient seedproposal method that actively proposes annotation seeds for the user to label.The user only needs to check which ones of the query seeds are inside theregion of interest (ROI). We enforce the sparsity and diversity criteria on theselection of the query seeds. At each round of interaction the user is onlypresented with a small number of informative query seeds that are far apartfrom each other. As a result, we are able to derive a user friendly interactionmechanism for annotation on small touchscreen devices. The user merely has toswipe through on the ROI-relevant query seeds, which should be easy since thosegestures are commonly used on a touchscreen. The performance of our algorithmis evaluated on six publicly available datasets. The evaluation results showthat our algorithm achieves high segmentation accuracy, with short responsetime and less user feedback."}
{"id": "745", "content": "We propose an end-to-end learning framework for segmenting generic objects inboth images and videos. Given a novel image or video, our approach produces apixel-level mask for all object-like regions---even for object categoriesnever seen during training. We formulate the task as a structured predictionproblem of assigning an object/background label to each pixel, implementedusing a deep fully convolutional network. When applied to a video, our modelfurther incorporates a motion stream, and the network learns to combine bothappearance and motion and attempts to extract all prominent objects whetherthey are moving or not. Beyond the core model, a second contribution of ourapproach is how it leverages varying strengths of training annotations.Pixel-level annotations are quite difficult to obtain, yet crucial for traininga deep network approach for segmentation. Thus we propose ways to exploitweakly labeled data for learning dense foreground segmentation. For images, weshow the value in mixing object category examples with image-level labelstogether with relatively few images with boundary-level annotations. For video,we show how to bootstrap weakly annotated videos together with the networktrained for image segmentation. Through experiments on multiple challengingimage and video segmentation benchmarks, our method offers consistently strongresults and improves the state-of-the-art for fully automatic segmentation ofgeneric (unseen) objects. In addition, we demonstrate how our approach benefitsimage retrieval and image retargeting, both of which flourish when given ourhigh-quality foreground maps. Code, models, and videos areat:http://vision.cs.utexas.edu/projects/pixelobjectness/ ", "id2": "744", "id3": "None"}
{"id": "746", "content": "Markov Random Fields (MRFs) are a popular model for several patternrecognition and reconstruction problems in robotics and computer vision.Inference in MRFs is intractable in general and related work resorts toapproximation algorithms. Among those techniques, semidefinite programming(SDP) relaxations have been shown to provide accurate estimates while scalingpoorly with the problem size and being typically slow for practicalapplications. Our first contribution is to design a dual ascent method to solvestandard SDP relaxations that takes advantage of the geometric structure of theproblem to speed up computation. This technique, named Dual Ascent RiemannianStaircase (DARS), is able to solve large problem instances in seconds. Oursecond contribution is to develop a second and faster approach. The backbone ofthis second approach is a novel SDP relaxation combined with a fast andscalable solver based on smooth Riemannian optimization. We show that thisapproach, named Fast Unconstrained SEmidefinite Solver (FUSES), can solve largeproblems in milliseconds. Contrarily to local MRF solvers, e.g., loopy beliefpropagation, our approaches do not require an initial guess. Moreover, weleverage recent results from optimization theory to provide per-instancesub-optimality guarantees. We demonstrate the proposed approaches inmulti-class image segmentation problems. Extensive experimental evidence showsthat (i) FUSES and DARS produce near-optimal solutions, attaining an objectivewithin 0.1% of the optimum, (ii) FUSES and DARS are remarkably faster thangeneral-purpose SDP solvers, and FUSES is more than two orders of magnitudefaster than DARS while attaining similar solution quality, (iii) FUSES isfaster than local search methods while being a global solver. ", "id2": "745", "id3": "None"}
{"id": "747", "content": "Fully convolutional deep neural networks have been asserted to be fast andprecise frameworks with great potential in image segmentation. One of the majorchallenges in training such networks raises when data is unbalanced, which iscommon in many medical imaging applications such as lesion segmentation wherelesion class voxels are often much lower in numbers than non-lesion voxels. Atrained network with unbalanced data may make predictions with high precisionand low recall, being severely biased towards the non-lesion class which isparticularly undesired in most medical applications where FNs are moreimportant than FPs. Various methods have been proposed to address this problem,more recently similarity loss functions and focal loss. In this work we trainedfully convolutional deep neural networks using an asymmetric similarity lossfunction to mitigate the issue of data imbalance and achieve much bettertradeoff between precision and recall. To this end, we developed a 3DFC-DenseNet with large overlapping image patches as input and an asymmetricsimilarity loss layer based on Tversky index (using Fbeta scores). We usedlarge overlapping image patches as inputs for intrinsic and extrinsic dataaugmentation, a patch selection algorithm, and a patch prediction fusionstrategy using B-spline weighted soft voting to account for the uncertainty ofprediction in patch borders. We applied this method to MS lesion segmentationbased on two different datasets of MSSEG and ISBI longitudinal MS lesionsegmentation challenge, where we achieved top performance in both challenges.Our network trained with focal loss ranked first according to the ISBIchallenge overall score and resulted in the lowest reported lesion falsepositive rate among all submitted methods. Our network trained with theasymmetric similarity loss led to the lowest surface distance and the bestlesion true positive rate. ", "id2": "746", "id3": "None"}
{"id": "748", "content": "The graph Laplacian is a standard tool in data science, machine learning, andimage processing. The corresponding matrix inherits the complex structure ofthe underlying network and is in certain applications densely populated. Thismakes computations, in particular matrix-vector products, with the graphLaplacian a hard task. A typical application is the computation of a number ofits eigenvalues and eigenvectors. Standard methods become infeasible as thenumber of nodes in the graph is too large. We propose the use of the fastsummation based on the nonequispaced fast Fourier transform (NFFT) to performthe dense matrix-vector product with the graph Laplacian fast without everforming the whole matrix. The enormous flexibility of the NFFT algorithm allowsus to embed the accelerated multiplication into Lanczos-based eigenvaluesroutines or iterative linear system solvers and even consider other than thestandard Gaussian kernels. We illustrate the feasibility of our approach on anumber of test problems from image segmentation to semi-supervised learningbased on graph-based PDEs. In particular, we compare our approach with theNystr om method. Moreover, we present and test an enhanced, hybrid version ofthe Nystr om method, which internally uses the NFFT. ", "id2": "747", "id3": "None"}
{"id": "749", "content": "Weakly-supervised instance segmentation, which could greatly save labor andtime cost of pixel mask annotation, has attracted increasing attention inrecent years. The commonly used pipeline firstly utilizes conventional imagesegmentation methods to automatically generate initial masks and then use themto train an off-the-shelf segmentation network in an iterative way. However,the initial generated masks usually contains a notable proportion of invalidmasks which are mainly caused by small object instances. Directly using theseinitial masks to train segmentation model is harmful for the performance. Toaddress this problem, we propose a hybrid network in this paper. In ourarchitecture, there is a principle segmentation network which is used to handlethe normal samples with valid generated masks. In addition, a complementarybranch is added to handle the small and dim objects without valid masks.Experimental results indicate that our method can achieve significantlyperformance improvement both on the small object instances and large ones, andoutperforms all state-of-the-art methods. ", "id2": "748", "id3": "None"}
{"id": "750", "content": "Partial differential equations (PDEs) are indispensable for modeling manyphysical phenomena and also commonly used for solving image processing tasks.In the latter area, PDE-based approaches interpret image data asdiscretizations of multivariate functions and the output of image processingalgorithms as solutions to certain PDEs. Posing image processing problems inthe infinite dimensional setting provides powerful tools for their analysis andsolution. Over the last few decades, the reinterpretation of classical imageprocessing problems through the PDE lens has been creating multiple celebratedapproaches that benefit a vast area of tasks including image segmentation,denoising, registration, and reconstruction.  In this paper, we establish a new PDE-interpretation of a class of deepconvolutional neural networks (CNN) that are commonly used to learn fromspeech, image, and video data. Our interpretation includes convolution residualneural networks (ResNet), which are among the most promising approaches fortasks such as image classification having improved the state-of-the-artperformance in prestigious benchmark challenges. Despite their recentsuccesses, deep ResNets still face some critical challenges associated withtheir design, immense computational costs and memory requirements, and lack ofunderstanding of their reasoning.  Guided by well-established PDE theory, we derive three new ResNetarchitectures that fall into two new classes: parabolic and hyperbolic CNNs. Wedemonstrate how PDE theory can provide new insights and algorithms for deeplearning and demonstrate the competitiveness of three new CNN architecturesusing numerical experiments. ", "id2": "749", "id3": "None"}
{"id2": 1073, "id3": "749", "content": "Partial differential equations (PDEs) are indispensable for modeling manyphysical phenomena and also commonly used for solving image processing tasks.In the latter area, PDE-based approaches interpret image data asdiscretizations of multivariate functions and the output of image processingalgorithms as solutions to certain PDEs. Posing image processing problems inthe infinite dimensional setting provides powerful tools for their analysis andsolution. Over the last few decades, the reinterpretation of classical imageprocessing problems through the PDE lens has been creating multiple celebratedapproaches that benefit a vast area of tasks including image segmentation,denoising, registration, and reconstruction. In this paper, we establish a new PDE-interpretation of a class of deepconvolutional neural networks (CNN) that are commonly used to learn fromspeech, image, and video data. Our interpretation includes convolution residualneural networks (ResNet), which are among the most promising approaches fortasks such as image classification having improved the state-of-the-artperformance in prestigious benchmark challenges. Despite their recentsuccesses, deep ResNets still face some critical challenges associated withtheir design, immense computational costs and memory requirements, and lack ofunderstanding of their reasoning. Guided by well-established PDE theory, we derive three new ResNetarchitectures that fall into two new classes: parabolic and hyperbolic CNNs. Wedemonstrate how PDE theory can provide new insights and algorithms for deeplearning and demonstrate the competitiveness of three new CNN architecturesusing numerical experiments."}
{"id": "751", "content": "3D image segmentation plays an important role in biomedical image analysis.Many 2D and 3D deep learning models have achieved state-of-the-art segmentationperformance on 3D biomedical image datasets. Yet, 2D and 3D models have theirown strengths and weaknesses, and by unifying them together, one may be able toachieve more accurate results. In this paper, we propose a new ensemblelearning framework for 3D biomedical image segmentation that combines themerits of 2D and 3D models. First, we develop a fully convolutional networkbased meta-learner to learn how to improve the results from 2D and 3D models(base-learners). Then, to minimize over-fitting for our sophisticatedmeta-learner, we devise a new training method that uses the results of thebase-learners as multiple versions of ground truths. Furthermore, since ournew meta-learner training scheme does not depend on manual annotation, it canutilize abundant unlabeled 3D image data to further improve the model.Extensive experiments on two public datasets (the HVSMR 2016 Challenge datasetand the mouse piriform cortex dataset) show that our approach is effectiveunder fully-supervised, semi-supervised, and transductive settings, and attainssuperior performance over state-of-the-art image segmentation methods. ", "id2": "750", "id3": "None"}
{"id": "752", "content": "The most recent fast and accurate image segmentation methods are built uponfully convolutional deep neural networks. In this paper, we propose new deeplearning strategies for DenseNets to improve segmenting images with subtledifferences in intensity values and features. We aim to segment brain tissue oninfant brain MRI at about 6 months of age where white matter and gray matter ofthe developing brain show similar T1 and T2 relaxation times, thus appear tohave similar intensity values on both T1- and T2-weighted MRI scans. Braintissue segmentation at this age is, therefore, very challenging. To this end,we propose an exclusive multi-label training strategy to segment the mutuallyexclusive brain tissues with similarity loss functions that automaticallybalance the training based on class prevalence. Using our proposed trainingstrategy based on similarity loss functions and patch prediction fusion wedecrease the number of parameters in the network, reduce the complexity of thetraining process focusing the attention on less number of tasks, whilemitigating the effects of data imbalance between labels and inaccuracies nearpatch borders. By taking advantage of these strategies we were able to performfast image segmentation (90 seconds per 3D volume), using a network with lessparameters than many state-of-the-art networks, overcoming issues such as3Dvs2D training and large vs small patch size selection, while achieving thetop performance in segmenting brain tissue among all methods tested in firstand second round submissions of the isointense infant brain MRI segmentation(iSeg) challenge according to the official challenge test results. Our proposedstrategy improves the training process through balanced training and byreducing its complexity while providing a trained model that works for any sizeinput image and is fast and more accurate than many state-of-the-art methods. ", "id2": "751", "id3": "None"}
{"id": "753", "content": "Radiological imaging offers effective measurement of anatomy, which is usefulin disease diagnosis and assessment. Previous study has shown that the leftatrial wall remodeling can provide information to predict treatment outcome inatrial fibrillation. Nevertheless, the segmentation of the left atrialstructures from medical images is still very time-consuming. Current advancesin neural network may help creating automatic segmentation models that reducethe workload for clinicians. In this preliminary study, we propose automated,two-stage, three-dimensional U-Nets with convolutional neural network, for thechallenging task of left atrial segmentation. Unlike previous two-dimensionalimage segmentation methods, we use 3D U-Nets to obtain the heart cavitydirectly in 3D. The dual 3D U-Net structure consists of, a first U-Net tocoarsely segment and locate the left atrium, and a second U-Net to accuratelysegment the left atrium under higher resolution. In addition, we introduce aContour loss based on additional distance information to adjust the finalsegmentation. We randomly split the data into training datasets (80 subjects)and validation datasets (20 subjects) to train multiple models, with differentaugmentation setting. Experiments show that the average Dice coefficients forvalidation datasets are around 0.91 - 0.92, the sensitivity around 0.90-0.94and the specificity 0.99. Compared with traditional Dice loss, models trainedwith Contour loss in general offer smaller Hausdorff distance with similar Dicecoefficient, and have less connected components in predictions. Finally, weintegrate several trained models in an ensemble prediction to segment testingdatasets. ", "id2": "752", "id3": "None"}
{"id": "754", "content": "Many seemingly unrelated computer vision tasks can be viewed as a specialcase of image decomposition into separate layers. For example, imagesegmentation (separation into foreground and background layers); transparentlayer separation (into reflection and transmission layers); Image dehazing(separation into a clear image and a haze map), and more. In this paper wepropose a unified framework for unsupervised layer decomposition of a singleimage, based on coupled Deep-image-Prior (DIP) networks. It was shown[Ulyanov et al] that the structure of a single DIP generator network issufficient to capture the low-level statistics of a single image. We show thatcoupling multiple such DIPs provides a powerful tool for decomposing imagesinto their basic components, for a wide variety of applications. Thiscapability stems from the fact that the internal statistics of a mixture oflayers is more complex than the statistics of each of its individualcomponents. We show the power of this approach for Image-Dehazing, Fg/BgSegmentation, Watermark-Removal, Transparency Separation in images and video,and more. These capabilities are achieved in a totally unsupervised way, withno training examples other than the input image/video itself. ", "id2": "753", "id3": "None"}
{"id": "755", "content": "While deep learning has achieved significant advances in accuracy for medicalimage segmentation, its benefits for deformable image registration have so farremained limited to reduced computation times. Previous work has either focusedon replacing the iterative optimization of distance and smoothness terms withCNN-layers or using supervised approaches driven by labels. Our method is thefirst to combine the complementary strengths of global semantic information(represented by segmentation labels) and local distance metrics that help alignsurrounding structures. We demonstrate significant higher Dice scores (of86.5 %) for deformable cardiac image registration compared to classicregistration (79.0 %) as well as label-driven deep learning frameworks(83.4 %). ", "id2": "754", "id3": "None"}
{"id": "756", "content": "Radiologist is doctors doctor, biomedical image segmentation plays acentral role in quantitative analysis, clinical diagnosis, and medicalintervention. In the light of the fully convolutional networks (FCN) and U-Net,deep convolutional networks (DNNs) have made significant contributions inbiomedical image segmentation applications. In this paper, based on U-Net, wepropose MDUnet, a multi-scale densely connected U-net for biomedical imagesegmentation. we propose three different multi-scale dense connections for Ushaped architectures encoder, decoder and across them. The highlights of ourarchitecture is directly fuses the neighboring different scale feature mapsfrom both higher layers and lower layers to strengthen feature propagation incurrent layer. Which can largely improves the information flow encoder, decoderand across them. Multi-scale dense connections, which means containing shorterconnections between layers close to the input and output, also makes muchdeeper U-net possible. We adopt the optimal model based on the experiment andpropose a novel Multi-scale Dense U-Net (MDU-Net) architecture withquantization. Which reduce overfitting in MDU-Net for better accuracy. Weevaluate our purpose model on the MICCAI 2015 Gland Segmentation dataset(GlaS). The three multi-scale dense connections improve U-net performance by upto 1.8% on test A and 3.5% on test B in the MICCAI Gland dataset. Meanwhile theMDU-net with quantization achieves the superiority over U-Net performance by upto 3% on test A and 4.1% on test B. ", "id2": "755", "id3": "None"}
{"id": "757", "content": "Accurate segmentation of different sub-regions of gliomas includingperitumoral edema, necrotic core, enhancing and non-enhancing tumor core frommultimodal MRI scans has important clinical relevance in diagnosis, prognosisand treatment of brain tumors. However, due to the highly heterogeneousappearance and shape, segmentation of the sub-regions is very challenging.Recent development using deep learning models has proved its effectiveness inthe past several brain segmentation challenges as well as other semantic andmedical image segmentation problems. Most models in brain tumor segmentationuse a 2D/3D patch to predict the class label for the center voxel and variantpatch sizes and scales are used to improve the model performance. However, ithas low computation efficiency and also has limited receptive field. U-Net is awidely used network structure for end-to-end segmentation and can be used onthe entire image or extracted patches to provide classification labels over theentire input voxels so that it is more efficient and expect to yield betterperformance with larger input size. Furthermore, instead of picking the bestnetwork structure, an ensemble of multiple models, trained on different datasetor different hyper-parameters, can generally improve the segmentationperformance. In this study we propose to use an ensemble of 3D U-Nets withdifferent hyper-parameters for brain tumor segmentation. Preliminary resultsshowed effectiveness of this model. In addition, we developed a linear modelfor survival prediction using extracted imaging and non-imaging features,which, despite the simplicity, can effectively reduce overfitting andregression errors. ", "id2": "756", "id3": "None"}
{"id": "758", "content": "Segmentation of magnetic resonance (MR) images is a fundamental step in manymedical imaging-based applications. The recent implementation of deepconvolutional neural networks (CNNs) in image processing has been shown to havesignificant impacts on medical image segmentation. Network training ofsegmentation CNNs typically requires images and paired annotation datarepresenting pixel-wise tissue labels referred to as masks. However, thesupervised training of highly efficient CNNs with deeper structure and morenetwork parameters requires a large number of training images and paired tissuemasks. Thus, there is great need to develop a generalized CNN-basedsegmentation method which would be applicable for a wide variety of MR imagedatasets with different tissue contrasts. The purpose of this study was todevelop and evaluate a generalized CNN-based method for fully-automatedsegmentation of different MR image datasets using a single set of annotatedtraining data. A technique called cycle-consistent generative adversarialnetwork (CycleGAN) is applied as the core of the proposed method to performimage-to-image translation between MR image datasets with different tissuecontrasts. A joint segmentation network is incorporated into the adversarialnetwork to obtain additional segmentation functionality. The proposed methodwas evaluated for segmenting bone and cartilage on two clinical knee MR imagedatasets acquired at our institution using only a single set of annotated datafrom a publicly available knee MR image dataset. The new technique may furtherimprove the applicability and efficiency of CNN-based segmentation of medicalimages while eliminating the need for large amounts of annotated training data. ", "id2": "757", "id3": "None"}
{"id": "759", "content": "Increasing the mini-batch size for stochastic gradient descent offerssignificant opportunities to reduce wall-clock training time, but there are avariety of theoretical and systems challenges that impede the widespreadsuccess of this technique. We investigate these issues, with an emphasis ontime to convergence and total computational cost, through an extensiveempirical analysis of network training across several architectures and problemdomains, including image classification, image segmentation, and languagemodeling. Although it is common practice to increase the batch size in order tofully exploit available computational resources, we find a substantially morenuanced picture. Our main finding is that across a wide range of networkarchitectures and problem domains, increasing the batch size beyond a certainpoint yields no decrease in wall-clock time to convergence for  emph either train or test loss. This batch size is usually substantially below the capacityof current systems. We show that popular training strategies for large batchsize optimization begin to fail before we can populate all available computeresources, and we show that the point at which these methods break down dependsmore on attributes like model architecture and data complexity than it doesdirectly on the size of the dataset. ", "id2": "758", "id3": "None"}
{"id": "760", "content": "Convolutional neural networks (CNNs) have become increasingly popular forsolving a variety of computer vision tasks, ranging from image classificationto image segmentation. Recently, autonomous vehicles have created a demand fordepth information, which is often obtained using hardware sensors such as Lightdetection and ranging (LIDAR). Although it can provide precise distancemeasurements, most LIDARs are still far too expensive to sell in mass-producedconsumer vehicles, which has motivated methods to generate depth informationfrom commodity automotive sensors like cameras.  In this paper, we propose an approach called Deep Sensor Cloning (DSC). Theidea is to use Convolutional Neural Networks in conjunction with inexpensivesensors to replicate the 3D point-clouds that are created by expensive LIDARs.To accomplish this, we develop a new dataset (DSDepth) and a new family of CNNarchitectures (DSCnets). While previous tasks such as KITTI depth predictionuse an interpolated RGB-D images as ground-truth for training, we instead useDSCnets to directly predict LIDAR point-clouds. When we compare the output ofour models to a $75,000 LIDAR, we find that our most accurate DSCnet achieves arelative error of 5.77% using a single camera and 4.69% using stereo cameras. ", "id2": "759", "id3": "None"}
{"id": "761", "content": "This paper presents a novel framework in which video/image segmentation andlocalization are cast into a single optimization problem that integratesinformation from low level appearance cues with that of high level localizationcues in a very weakly supervised manner. The proposed framework leverages tworepresentations at different levels, exploits the spatial relationship betweenbounding boxes and superpixels as linear constraints and simultaneouslydiscriminates between foreground and background at bounding box and superpixellevel. Different from previous approaches that mainly rely on discriminativeclustering, we incorporate a foreground model that minimizes the histogramdifference of an object across all image frames. Exploiting the geometricrelation between the superpixels and bounding boxes enables the transfer ofsegmentation cues to improve localization output and vice-versa. Inclusion ofthe foreground model generalizes our discriminative framework to video datawhere the background tends to be similar and thus, not discriminative. Wedemonstrate the effectiveness of our unified framework on the YouTube Objectvideo dataset, Internet Object Discovery dataset and Pascal VOC 2007. ", "id2": "760", "id3": "None"}
{"id": "762", "content": "We consider the structured-output prediction problem through probabilisticapproaches and generalize the perturb-and-MAP framework to more challengingweighted Hamming losses, which are crucial in applications. While in principleour approach is a straightforward marginalization, it requires solving manyrelated MAP inference problems. We show that for log-supermodular pairwisemodels these operations can be performed efficiently using the machinery ofdynamic graph cuts. We also propose to use double stochastic gradient descent,both on the data and on the perturbations, for efficient learning. Ourframework can naturally take weak supervision (e.g., partial labels) intoaccount. We conduct a set of experiments on medium-scale character recognitionand image segmentation, showing the benefits of our algorithms. ", "id2": "761", "id3": "None"}
{"id": "763", "content": "In this work, we propose a special cascade network for image segmentation,which is based on the U-Net networks as building blocks and the idea of theiterative refinement. The model was mainly applied to achieve higherrecognition quality for the task of finding borders of the optic disc and cup,which are relevant to the presence of glaucoma. Compared to a single U-Net andthe state-of-the-art methods for the investigated tasks, very high segmentationquality has been achieved without a need for increasing the volume of datasets.Our experiments include comparison with the best-known methods on publiclyavailable databases DRIONS-DB, RIM-ONE v.3, DRISHTI-GS, and evaluation on aprivate data set collected in collaboration with University of California SanFrancisco Medical School. The analysis of the architecture details ispresented, and it is argued that the model can be employed for a broad scope ofimage segmentation problems of similar nature. ", "id2": "762", "id3": "None"}
{"id": "764", "content": "In multi-organ segmentation of abdominal CT scans, most existing fullysupervised deep learning algorithms require lots of voxel-wise annotations,which are usually difficult, expensive, and slow to obtain. In comparison,massive unlabeled 3D CT volumes are usually easily accessible. Currentmainstream works to address the semi-supervised biomedical image segmentationproblem are mostly graph-based. By contrast, deep network based semi-supervisedlearning methods have not drawn much attention in this field. In this work, wepropose Deep Multi-Planar Co-Training (DMPCT), whose contributions can bedivided into two folds: 1) The deep model is learned in a co-training stylewhich can mine consensus information from multiple planes like the sagittal,coronal, and axial planes; 2) Multi-planar fusion is applied to generate morereliable pseudo-labels, which alleviates the errors occurring in thepseudo-labels and thus can help to train better segmentation networks.Experiments are done on our newly collected large dataset with 100 unlabeledcases as well as 210 labeled cases where 16 anatomical structures are manuallyannotated by four radiologists and confirmed by a senior expert. The resultssuggest that DMPCT significantly outperforms the fully supervised method bymore than 4% especially when only a small set of annotations is used. ", "id2": "763", "id3": "None"}
{"id": "765", "content": "Brain image segmentation is used for visualizing and quantifying anatomicalstructures of the brain. We present an automated ap-proach using 2D deepresidual dilated networks which captures rich context information of differenttissues for the segmentation of eight brain structures. The proposed system wasevaluated in the MICCAI Brain Segmentation Challenge and ranked 9th out of 22teams. We further compared the method with traditional U-Net usingleave-one-subject-out cross-validation setting on the public dataset.Experimental results shows that the proposed method outperforms traditionalU-Net (i.e. 80.9% vs 78.3% in averaged Dice score, 4.35mm vs 11.59mm inaveraged robust Hausdorff distance) and is computationally efficient. ", "id2": "764", "id3": "None"}
{"id": "766", "content": "In this paper, we propose a new pre-training scheme for U-net based imagesegmentation. We first train the encoding arm as a localization network topredict the center of the target, before extending it into a U-net architecturefor segmentation. We apply our proposed method to the problem of segmenting theoptic disc from fundus photographs. Our work shows that the features learned byencoding arm can be transferred to the segmentation network to reduce theannotation burden. We propose that an approach could have broad utility formedical image segmentation, and alleviate the burden of delineating complexstructures by pre-training on annotations that are much easier to acquire. ", "id2": "765", "id3": "None"}
{"id": "767", "content": "Building detection from satellite multispectral imagery data is being afundamental but a challenging problem mainly because it requires correctrecovery of building footprints from high-resolution images. In this work, wepropose a deep learning approach for building detection by applying numerousenhancements throughout the process. Initial dataset is preprocessed by 2-sigmapercentile normalization. Then data preparation includes ensemble modellingwhere 3 models were created while incorporating OpenStreetMap data. BinaryDistance Transformation (BDT) is used for improving data labeling process andthe U-Net (Convolutional Networks for Biomedical Image Segmentation) ismodified by adding batch normalization wrappers. Afterwards, it is explainedhow each component of our approach is correlated with the final detectionaccuracy. Finally, we compare our results with winning solutions of SpaceNet 2competition for real satellite multispectral images of Vegas, Paris, Shanghaiand Khartoum, demonstrating the importance of our solution for achieving higherbuilding detection accuracy. ", "id2": "766", "id3": "None"}
{"id": "768", "content": "Hyperspectral satellite imaging attracts enormous research attention in theremote sensing community, hence automated approaches for precise segmentationof such imagery are being rapidly developed. In this letter, we share ourobservations on the strategy for validating hyperspectral image segmentationalgorithms currently followed in the literature, and show that it can lead toover-optimistic experimental insights. We introduce a new routine forgenerating segmentation benchmarks, and use it to elaborate ready-to-usehyperspectral training-test data partitions. They can be utilized for fairvalidation of new and existing algorithms without any training-test dataleakage. ", "id2": "767", "id3": "None"}
{"id": "769", "content": "The training of many existing end-to-end steering angle prediction modelsheavily relies on steering angles as the supervisory signal. Without learningfrom much richer contexts, these methods are susceptible to the presence ofsharp road curves, challenging traffic conditions, strong shadows, and severelighting changes. In this paper, we considerably improve the accuracy androbustness of predictions through heterogeneous auxiliary networks featuremimicking, a new and effective training method that provides us with muchricher contextual signals apart from steering direction. Specifically, we trainour steering angle predictive model by distilling multi-layer knowledge frommultiple heterogeneous auxiliary networks that perform related but differenttasks, e.g., image segmentation or optical flow estimation. As opposed tomulti-task learning, our method does not require expensive annotations ofrelated tasks on the target set. This is made possible by applying contemporaryoff-the-shelf networks on the target set and mimicking their features indifferent layers after transformation. The auxiliary networks are discardedafter training without affecting the runtime efficiency of our model. Ourapproach achieves a new state-of-the-art on Udacity and Comma.ai, outperformingthe previous best by a large margin of 12.8% and 52.1%, respectively.Encouraging results are also shown on Berkeley Deep Drive (BDD) dataset. ", "id2": "768", "id3": "None"}
{"id": "770", "content": "Automatic extraction of liver and tumor from CT volumes is a challenging taskdue to their heterogeneous and diffusive shapes. Recently, 2D and 3D deepconvolutional neural networks have become popular in medical image segmentationtasks because of the utilization of large labeled datasets to learnhierarchical features. However, 3D networks have some drawbacks due to theirhigh cost on computational resources. In this paper, we propose a 3D hybridresidual attention-aware segmentation method, named RA-UNet, to preciselyextract the liver volume of interests (VOI) and segment tumors from the liverVOI. The proposed network has a basic architecture as a 3D U-Net which extractscontextual information combining low-level feature maps with high-level ones.Attention modules are stacked so that the attention-aware features changeadaptively as the network goes very deep and this is made possible byresidual learning. This is the first work that an attention residual mechanismis used to process medical volumetric images. We evaluated our framework on thepublic MICCAI 2017 Liver Tumor Segmentation dataset and the 3DIRCADb dataset.The results show that our architecture outperforms other state-of-the-artmethods. We also extend our RA-UNet to brain tumor segmentation on theBraTS2018 and BraTS2017 datasets, and the results indicate that RA-UNetachieves good performance on a brain tumor segmentation task as well. ", "id2": "769", "id3": "None"}
{"id": "771", "content": "Locating region of interest for breast cancer masses in the mammographicimage is a challenging problem in medical image processing. In this researchwork, the keen idea is to efficiently extract suspected mass region for furtherexamination. In particular to this fact breast boundary segmentation on slicedrgb image using modified intensity based approach followed by quad tree baseddivision to spot out suspicious area are proposed in the paper. To evaluate theperformance DDSM standard dataset are experimented and achieved acceptableaccuracy. ", "id2": "770", "id3": "None"}
{"id": "772", "content": "We address the problem of segmenting 3D multi-modal medical images inscenarios where very few labeled examples are available for training.Leveraging the recent success of adversarial learning for semi-supervisedsegmentation, we propose a novel method based on Generative AdversarialNetworks (GANs) to train a segmentation model with both labeled and unlabeledimages. The proposed method prevents over-fitting by learning to discriminatebetween true and fake patches obtained by a generator network. Our work extendscurrent adversarial learning approaches, which focus on 2D single-modalityimages, to the more challenging context of 3D volumes of multiple modalities.The proposed method is evaluated on the problem of segmenting brain MRI fromthe iSEG-2017 and MRBrainS 2013 datasets. Significant performance improvementis reported, compared to state-of-art segmentation networks trained in afully-supervised manner. In addition, our work presents a comprehensiveanalysis of different GAN architectures for semi-supervised segmentation,showing recent techniques like feature matching to yield a higher performancethan conventional adversarial training approaches. Our code is publiclyavailable at https://github.com/arnab39/FewShot_GAN-Unet3D ", "id2": "771", "id3": "None"}
{"id": "773", "content": "Collecting training data from the physical world is usually time-consumingand even dangerous for fragile robots, and thus, recent advances in robotlearning advocate the use of simulators as the training platform.Unfortunately, the reality gap between synthetic and real visual data prohibitsdirect migration of the models trained in virtual worlds to the real world.This paper proposes a modular architecture for tackling the virtual-to-realproblem. The proposed architecture separates the learning model into aperception module and a control policy module, and uses semantic imagesegmentation as the meta representation for relating these two modules. Theperception module translates the perceived RGB image to semantic imagesegmentation. The control policy module is implemented as a deep reinforcementlearning agent, which performs actions based on the translated imagesegmentation. Our architecture is evaluated in an obstacle avoidance task and atarget following task. Experimental results show that our architecturesignificantly outperforms all of the baseline methods in both virtual and realenvironments, and demonstrates a faster learning curve than them. We alsopresent a detailed analysis for a variety of variant configurations, andvalidate the transferability of our modular architecture. ", "id2": "772", "id3": "None"}
{"id2": 1074, "id3": "772", "content": "Collecting training data from the physical world is usually time-consumingand even dangerous for fragile robots, and thus, recent advances in robotlearning advocate the use of simulators as the training platform.Unfortunately, the reality gap between synthetic and real visual data prohibitsdirect migration of the models trained in virtual worlds to the real world.This paper proposes a modular architecture for tackling the virtual-to-realproblem. The proposed architecture separates the learning model into aperception module and a control policy module, and uses semantic imagesegmentation as the meta representation for relating these two modules. Theperception module translates the perceived RGB image to semantic imagesegmentation. The control policy module is implemented as a deep reinforcementlearning agent, which performs actions based on the translated imagesegmentation. Our architecture is evaluated in an obstacle avoidance task and atarget following task. Experimental results show that our architecturesignificantly outperforms all of the baseline methods in both virtual and realenvironments, and demonstrates a faster learning curve than them. We alsopresent a detailed analysis for a variety of variant configurations, andvalidate the transferability of our modular architecture."}
{"id": "774", "content": "State of the art methods for semantic image segmentation are trained in asupervised fashion using a large corpus of fully labeled training images.However, gathering such a corpus is expensive, due to human annotation effort,in contrast to gathering unlabeled data. We propose an active learning-basedstrategy, called CEREALS, in which a human only has to hand-label a few,automatically selected, regions within an unlabeled image corpus. Thisminimizes human annotation effort while maximizing the performance of asemantic image segmentation method. The automatic selection procedure isachieved by: a) using a suitable information measure combined with an estimateabout human annotation effort, which is inferred from a learned cost model, andb) exploiting the spatial coherency of an image. The performance of CEREALS isdemonstrated on Cityscapes, where we are able to reduce the annotation effortto 17%, while keeping 95% of the mean Intersection over Union (mIoU) of a modelthat was trained with the fully annotated training set of Cityscapes. ", "id2": "773", "id3": "None"}
{"id": "775", "content": "Objective: Deformable image registration is a fundamental problem in medicalimage analysis, with applications such as longitudinal studies, populationmodeling, and atlas based image segmentation. Registration is often phrased asan optimization problem, i.e., finding a deformation field that is optimalaccording to a given objective function. Discrete, combinatorial, optimizationtechniques have successfully been employed to solve the resulting optimizationproblem. Specifically, optimization based on $ alpha$-expansion with minimalgraph cuts has been proposed as a powerful tool for image registration. Thehigh computational cost of the graph-cut based optimization approach, however,limits the utility of this approach for registration of large volume images.Methods: Here, we propose to accelerate graph-cut based deformable registrationby dividing the image into overlapping sub-regions and restricting the$ alpha$-expansion moves to a single sub-region at a time. Results: Wedemonstrate empirically that this approach can achieve a large reduction incomputation time -- from days to minutes -- with only a small penalty in termsof solution quality. Conclusion: The reduction in computation time provided bythe proposed method makes graph cut based deformable registration viable forlarge volume images. Significance: Graph cut based image registration haspreviously been shown to produce excellent results, but the high computationalcost has hindered the adoption of the method for registration of large medicalvolume images. Our proposed method lifts this restriction, requiring only asmall fraction of the computational cost to produce results of comparablequality. ", "id2": "774", "id3": "None"}
{"id": "776", "content": "Weakly-supervised image segmentation is an important task in computer vision.A key problem is how to obtain high quality objects location from image-levelcategory. Classification activation mapping is a common method which can beused to generate high-precise object location cues. However these location cuesare generally very sparse and small such that they can not provide effectiveinformation for image segmentation. In this paper, we propose a saliency guidedimage segmentation network to resolve this problem. We employ a self-attentionsaliency method to generate subtle saliency maps, and render the location cuesgrow as seeds by seeded region growing method to expand pixel-level labelsextent. In the process of seeds growing, we use the saliency values to weightthe similarity between pixels to control the growing. Therefore saliencyinformation could help generate discriminative object regions, and the effectsof wrong salient pixels can be suppressed efficiently. Experimental results ona common segmentation dataset PASCAL VOC2012 demonstrate the effectiveness ofour method. ", "id2": "775", "id3": "None"}
{"id": "777", "content": "We propose a generalized focal loss function based on the Tversky index toaddress the issue of data imbalance in medical image segmentation. Compared tothe commonly used Dice loss, our loss function achieves a better trade offbetween precision and recall when training on small structures such as lesions.To evaluate our loss function, we improve the attention U-Net model byincorporating an image pyramid to preserve contextual features. We experimenton the BUS 2017 dataset and ISIC 2018 dataset where lesions occupy 4.84% and21.4% of the images area and improve segmentation accuracy when compared to thestandard U-Net by 25.7% and 3.6%, respectively. ", "id2": "776", "id3": "None"}
{"id": "778", "content": "In this paper, we focus on three problems in deep learning based medicalimage segmentation. Firstly, U-net, as a popular model for medical imagesegmentation, is difficult to train when convolutional layers increase eventhough a deeper network usually has a better generalization ability because ofmore learnable parameters. Secondly, the exponential ReLU (ELU), as analternative of ReLU, is not much different from ReLU when the network ofinterest gets deep. Thirdly, the Dice loss, as one of the pervasive lossfunctions for medical image segmentation, is not effective when the predictionis close to ground truth and will cause oscillation during training. To addressthe aforementioned three problems, we propose and validate a deeper networkthat can fit medical image datasets that are usually small in the sample size.Meanwhile, we propose a new loss function to accelerate the learning processand a combination of different activation functions to improve the networkperformance. Our experimental results suggest that our network is comparable orsuperior to state-of-the-art methods. ", "id2": "777", "id3": "None"}
{"id": "779", "content": "Magnetic resonance imaging (MRI) is the non-invasive modality of choice forbody tissue composition analysis due to its excellent soft tissue contrast andlack of ionizing radiation. However, quantification of body compositionrequires an accurate segmentation of fat, muscle and other tissues from MRimages, which remains a challenging goal due to the intensity overlap betweenthem. In this study, we propose a fully automated, data-driven imagesegmentation platform that addresses multiple difficulties in segmenting MRimages such as varying inhomogeneity, non-standardness, and noise, whileproducing high-quality definition of different tissues. In contrast to mostapproaches in the literature, we perform segmentation operation by combiningthree different MRI contrasts and a novel segmentation tool which takes intoaccount variability in the data. The proposed system, based on a novel affinitydefinition within the fuzzy connectivity (FC) image segmentation family,prevents the need for user intervention and reparametrization of thesegmentation algorithms. In order to make the whole system fully automated, weadapt an affinity propagation clustering algorithm to roughly identify tissueregions and image background. We perform a thorough evaluation of the proposedalgorithms individual steps as well as comparison with several approaches fromthe literature for the main application of muscle/fat separation. Furthermore,whole-body tissue composition and brain tissue delineation were conducted toshow the generalization ability of the proposed system. This new automatedplatform outperforms other state-of-the-art segmentation approaches both inaccuracy and efficiency. ", "id2": "778", "id3": "None"}
{"id": "780", "content": "We propose a segmentation framework that uses deep neural networks andintroduce two innovations. First, we describe a biophysics-based domainadaptation method. Second, we propose an automatic method to segment white andgray matter, and cerebrospinal fluid, in addition to tumorous tissue. Regardingour first innovation, we use a domain adaptation framework that combines anovel multispecies biophysical tumor growth model with a generative adversarialmodel to create realistic looking synthetic multimodal MR images with knownsegmentation. Regarding our second innovation, we propose an automatic approachto enrich available segmentation data by computing the segmentation for healthytissues. This segmentation, which is done using diffeomorphic imageregistration between the BraTS training data and a set of prelabeled atlases,provides more information for training and reduces the class imbalance problem.Our overall approach is not specific to any particular neural network and canbe used in conjunction with existing solutions. We demonstrate the performanceimprovement using a 2D U-Net for the BraTS18 segmentation challenge. Ourbiophysics based domain adaptation achieves better results, as compared to theexisting state-of-the-art GAN model used to create synthetic data for training. ", "id2": "779", "id3": "None"}
{"id": "781", "content": "Image segmentation is the process of partitioning an image into meaningfulsegments. The meaning of the segments is subjective due to the definition ofhomogeneity is varied based on the users perspective hence the automation ofthe segmentation is challenging. Watershed is a popular segmentation techniquewhich assumes topographic map in an image, with the brightness of each pixelrepresenting its height, and finds the lines that run along the tops of ridges.The results from the algorithm typically suffer from over segmentation due tothe lack of knowledge of the objects being classified. This paper presents anapproach to reduce the over segmentation of watershed algorithm by assumingthat the different adjacent segments of an object have similar colordistribution. The approach demonstrates an improvement over conventionalwatershed algorithm. ", "id2": "780", "id3": "None"}
{"id": "782", "content": "We consider an important task of effective and efficient semantic imagesegmentation. In particular, we adapt a powerful semantic segmentationarchitecture, called RefineNet, into the more compact one, suitable even fortasks requiring real-time performance on high-resolution inputs. To this end,we identify computationally expensive blocks in the original setup, and proposetwo modifications aimed to decrease the number of parameters and floating pointoperations. By doing that, we achieve more than twofold model reduction, whilekeeping the performance levels almost intact. Our fastest model undergoes asignificant speed-up boost from 20 FPS to 55 FPS on a generic GPU card on512x512 inputs with solid 81.1% mean iou performance on the test set of PASCALVOC, while our slowest model with 32 FPS (from original 17 FPS) shows 82.7%mean iou on the same dataset. Alternatively, we showcase that our approach iseasily mixable with light-weight classification networks: we attain 79.2% meaniou on PASCAL VOC using a model that contains only 3.3M parameters and performsonly 9.3B floating point operations. ", "id2": "781", "id3": "None"}
{"id": "783", "content": "In this paper, we propose a novel deep learning framework for anatomysegmentation and automatic landmark- ing. Specifically, we focus on thechallenging problem of mandible segmentation from cone-beam computed tomography(CBCT) scans and identification of 9 anatomical landmarks of the mandible onthe geodesic space. The overall approach employs three inter-related steps. Instep 1, we propose a deep neu- ral network architecture with carefully designedregularization, and network hyper-parameters to perform image segmentationwithout the need for data augmentation and complex post- processing refinement.In step 2, we formulate the landmark localization problem directly on thegeodesic space for sparsely- spaced anatomical landmarks. In step 3, we proposeto use a long short-term memory (LSTM) network to identify closely- spacedlandmarks, which is rather difficult to obtain using other standard detectionnetworks. The proposed fully automated method showed superior efficacy comparedto the state-of-the- art mandible segmentation and landmarking approaches incraniofacial anomalies and diseased states. We used a very challenging CBCTdataset of 50 patients with a high-degree of craniomaxillofacial (CMF)variability that is realistic in clinical practice. Complementary to thequantitative analysis, the qualitative visual inspection was conducted fordistinct CBCT scans from 250 patients with high anatomical variability. We havealso shown feasibility of the proposed work in an independent dataset fromMICCAI Head-Neck Challenge (2015) achieving the state-of-the-art performance.Lastly, we present an in-depth analysis of the proposed deep networks withrespect to the choice of hyper-parameters such as pooling and activationfunctions. ", "id2": "782", "id3": "None"}
{"id": "784", "content": "This work addresses the problem of semantic image segmentation of nighttimescenes. Although considerable progress has been made in semantic imagesegmentation, it is mainly related to daytime scenarios. This paper proposes anovel method to progressive adapt the semantic models trained on daytimescenes, along with large-scale annotations therein, to nighttime scenes via thebridge of twilight time -- the time between dawn and sunrise, or between sunsetand dusk. The goal of the method is to alleviate the cost of human annotationfor nighttime images by transferring knowledge from standard daytimeconditions. In addition to the method, a new dataset of road scenes iscompiled; it consists of 35,000 images ranging from daytime to twilight timeand to nighttime. Also, a subset of the nighttime images are densely annotatedfor method evaluation. Our experiments show that our method is effective formodel adaptation from daytime scenes to nighttime scenes, without using extrahuman annotation. ", "id2": "783", "id3": "None"}
{"id": "785", "content": "In this paper, we propose a novel fully convolutional two-stream fusionnetwork (FCTSFN) for interactive image segmentation. The proposed networkincludes two sub-networks: a two-stream late fusion network (TSLFN) thatpredicts the foreground at a reduced resolution, and a multi-scale refiningnetwork (MSRN) that refines the foreground at full resolution. The TSLFNincludes two distinct deep streams followed by a fusion network. The intuitionis that, since user interactions are more direct information onforeground/background than the image itself, the two-stream structure of theTSLFN reduces the number of layers between the pure user interaction featuresand the network output, allowing the user interactions to have a more directimpact on the segmentation result. The MSRN fuses the features from differentlayers of TSLFN with different scales, in order to seek the local to globalinformation on the foreground to refine the segmentation result at fullresolution. We conduct comprehensive experiments on four benchmark datasets.The results show that the proposed network achieves competitive performancecompared to current state-of-the-art interactive image segmentation methods ", "id2": "784", "id3": "None"}
{"id": "786", "content": "We propose a novel framework for structured prediction via adversariallearning. Existing adversarial learning methods involve two separate networks,i.e., the structured prediction models and the discriminative models, in thetraining. The information captured by discriminative models complements that inthe structured prediction models, but few existing researches have studied onutilizing such information to improve structured prediction models at theinference stage. In this work, we propose to refine the predictions ofstructured prediction models by effectively integrating discriminative modelsinto the prediction. Discriminative models are treated as energy-based models.Similar to the adversarial learning, discriminative models are trained toestimate scores which measure the quality of predicted outputs, whilestructured prediction models are trained to predict contrastive outputs withmaximal energy scores. In this way, the gradient vanishing problem isameliorated, and thus we are able to perform inference by following the ascentgradient directions of discriminative models to refine structured predictionmodels. The proposed method is able to handle a range of tasks, e.g.,multi-label classification and image segmentation. Empirical results on thesetwo tasks validate the effectiveness of our learning method. ", "id2": "785", "id3": "None"}
{"id": "787", "content": "Histopathological prognostication of neoplasia including most tumor gradingsystems are based upon a number of criteria. Probably the most important is thenumber of mitotic figures which are most commonly determined as the mitoticcount (MC), i.e. number of mitotic figures within 10 consecutive high powerfields. Often the area with the highest mitotic activity is to be selected forthe MC. However, since mitotic activity is not known in advance, an arbitrarychoice of this region is considered one important cause for high variability inthe prognostication and grading.  In this work, we present an algorithmic approach that first calculates amitotic cell map based upon a deep convolutional network. This map is in asecond step used to construct a mitotic activity estimate. Lastly, we selectthe image segment representing the size of ten high power fields with theoverall highest mitotic activity as a region proposal for an expert MCdetermination. We evaluate the approach using a dataset of 32 completelyannotated whole slide images, where 22 were used for training of the networkand 10 for test. We find a correlation of r=0.936 in mitotic count estimate. ", "id2": "786", "id3": "None"}
{"id": "788", "content": "Left atrium shape has been shown to be an independent predictor of recurrenceafter atrial fibrillation (AF) ablation. Shape-based representation isimperative to such an estimation process, where correspondence-basedrepresentation offers the most flexibility and ease-of-computation forpopulation-level shape statistics. Nonetheless, population-level shaperepresentations in the form of image segmentation and correspondence modelsderived from cardiac MRI require significant human resources with sufficientanatomy-specific expertise. In this paper, we propose a machine learningapproach that uses deep networks to estimate AF recurrence by predicting shapedescriptors directly from MRI images, with NO image pre-processing involved. Wealso propose a novel data augmentation scheme to effectively train a deepnetwork in a limited training data setting. We compare this new method ofestimating shape descriptors from images with the state-of-the-artcorrespondence-based shape modeling that requires image segmentation andcorrespondence optimization. Results show that the proposed method and thecurrent state-of-the-art produce statistically similar outcomes on AFrecurrence, eliminating the need for expensive pre-processing pipelines andassociated human labor. ", "id2": "787", "id3": "None"}
{"id2": 1075, "id3": "787", "content": "Left atrium shape has been shown to be an independent predictor of recurrenceafter atrial fibrillation (AF) ablation. Shape-based representation isimperative to such an estimation process, where correspondence-basedrepresentation offers the most flexibility and ease-of-computation forpopulation-level shape statistics. Nonetheless, population-level shaperepresentations in the form of image segmentation and correspondence modelsderived from cardiac MRI require significant human resources with sufficientanatomy-specific expertise. In this paper, we propose a machine learningapproach that uses deep networks to estimate AF recurrence by predicting shapedescriptors directly from MRI images, with NO image pre-processing involved. Wealso propose a novel data augmentation scheme to effectively train a deepnetwork in a limited training data setting. We compare this new method ofestimating shape descriptors from images with the state-of-the-artcorrespondence-based shape modeling that requires image segmentation andcorrespondence optimization. Results show that the proposed method and thecurrent state-of-the-art produce statistically similar outcomes on AFrecurrence, eliminating the need for expensive pre-processing pipelines andassociated human labor."}
{"id": "789", "content": "Accurate and reliable image segmentation is an essential part of biomedicalimage analysis. In this paper, we consider the problem of biomedical imagesegmentation using deep convolutional neural networks. We propose a newend-to-end network architecture that effectively integrates local and globalcontextual patterns of histologic primitives to obtain a more reliablesegmentation result. Specifically, we introduce a deep fully convolutionresidual network with a new skip connection strategy to control the contextualinformation passed forward. Moreover, our trained model is also computationallyinexpensive due to its small number of network parameters. We evaluate ourmethod on two public datasets for epithelium segmentation and tubulesegmentation tasks. Our experimental results show that the proposed methodprovides a fast and effective way of producing a pixel-wise dense prediction ofbiomedical images. ", "id2": "788", "id3": "None"}
{"id": "790", "content": "Being able to effectively identify clouds and monitor their evolution is oneimportant step toward more accurate quantitative precipitation estimation andforecast. In this study, a new gradient-based cloud-image segmentationtechnique is developed using tools from image processing techniques. Thismethod integrates morphological image gradient magnitudes to separable cloudsystems and patches boundaries. A varying scale-kernel is implemented to reducethe sensitivity of image segmentation to noise and capture objects with variousfinenesses of the edges in remote-sensing images. The proposed method isflexible and extendable from single- to multi-spectral imagery. Case studieswere carried out to validate the algorithm by applying the proposedsegmentation algorithm to synthetic radiances for channels of the GeostationaryOperational Environmental Satellites (GOES-R) simulated by a high-resolutionweather prediction model. The proposed method compares favorably with theexisting cloud-patch-based segmentation technique implemented in thePERSIANN-CCS (Precipitation Estimation from Remotely Sensed Information usingArtificial Neural Network - Cloud Classification System) rainfall retrievalalgorithm. Evaluation of event-based images indicates that the proposedalgorithm has potential to improve rain detection and estimation skills with anaverage of more than 45% gain comparing to the segmentation technique used inPERSIANN-CCS and identifying cloud regions as objects with accuracy rates up to98%. ", "id2": "789", "id3": "None"}
{"id": "791", "content": "The U-Net was presented in 2015. With its straight-forward and successfularchitecture it quickly evolved to a commonly used benchmark in medical imagesegmentation. The adaptation of the U-Net to novel problems, however, comprisesseveral degrees of freedom regarding the exact architecture, preprocessing,training and inference. These choices are not independent of each other andsubstantially impact the overall performance. The present paper introduces thennU-Net (no-new-Net), which refers to a robust and self-adapting framework onthe basis of 2D and 3D vanilla U-Nets. We argue the strong case for taking awaysuperfluous bells and whistles of many proposed network designs and insteadfocus on the remaining aspects that make out the performance andgeneralizability of a method. We evaluate the nnU-Net in the context of theMedical Segmentation Decathlon challenge, which measures segmentationperformance in ten disciplines comprising distinct entities, image modalities,image geometries and dataset sizes, with no manual adjustments between datasetsallowed. At the time of manuscript submission, nnU-Net achieves the highestmean dice scores across all classes and seven phase 1 tasks (except class 1 inBrainTumour) in the online leaderboard of the challenge. ", "id2": "790", "id3": "None"}
{"id": "792", "content": "This paper proposes a deep learning architecture that attains statisticallysignificant improvements over traditional algorithms in Poisson image denoisingespically when the noise is strong. Poisson noise commonly occurs in low-lightand photon- limited settings, where the noise can be most accurately modeled bythe Poission distribution. Poisson noise traditionally prevails only inspecific fields such as astronomical imaging. However, with the booming marketof surveillance cameras, which commonly operate in low-light environments, ormobile phones, which produce noisy night scene pictures due to lower-gradesensors, the necessity for an advanced Poisson image denoising algorithm hasincreased. Deep learning has achieved amazing breakthroughs in other imagingproblems, such image segmentation and recognition, and this paper proposes adeep learning denoising network that outperforms traditional algorithms inPoisson denoising especially when the noise is strong. The architectureincorporates a hybrid of convolutional and deconvolutional layers along withsymmetric connections. The denoising network achieved statistically significant0.38dB, 0.68dB, and 1.04dB average PSNR gains over benchmark traditionalalgorithms in experiments with image peak values 4, 2, and 1. The denoisingnetwork can also operate with shorter computational time while stilloutperforming the benchmark algorithm by tuning the reconstruction stridesizes. ", "id2": "791", "id3": "None"}
{"id": "793", "content": "Over the past years, computer vision community has contributed to enormousprogress in semantic image segmentation, a per-pixel classification task,crucial for dense scene understanding and rapidly becoming vital in lots ofreal-world applications, including driverless cars and medical imaging. Mostrecent models are now reaching previously unthinkable numbers (e.g., 89% meaniou on PASCAL VOC, 83% on CityScapes), and, while intersection-over-union and arange of other metrics provide the general picture of model performance, inthis paper we aim to extend them into other meaningful and important forapplications characteristics, answering such questions as how accurate themodel segmentation is on small objects in the general scene?, or what are thesources of uncertainty that cause the model to make an erroneous prediction?.Besides establishing a methodology that covers the performance of a singlemodel from different perspectives, we also showcase several extensions that canbe worth pursuing in order to further improve current results in semanticsegmentation. ", "id2": "792", "id3": "None"}
{"id2": 1076, "id3": "792", "content": "Over the past years, computer vision community has contributed to enormousprogress in semantic image segmentation, a per-pixel classification task,crucial for dense scene understanding and rapidly becoming vital in lots ofreal-world applications, including driverless cars and medical imaging. Mostrecent models are now reaching previously unthinkable numbers (e.g., 89% meaniou on PASCAL VOC, 83% on CityScapes), and, while intersection-over-union and arange of other metrics provide the general picture of model performance, inthis paper we aim to extend them into other meaningful and important forapplications characteristics, answering such questions as how accurate themodel segmentation is on small objects in the general scene?, or what are thesources of uncertainty that cause the model to make an erroneous prediction?.Besides establishing a methodology that covers the performance of a singlemodel from different perspectives, we also showcase several extensions that canbe worth pursuing in order to further improve current results in semanticsegmentation."}
{"id": "794", "content": "Encoding images as a series of high-level constructs, such as brush strokesor discrete shapes, can often be key to both human and machine understanding.In many cases, however, data is only available in pixel form. We present amethod for generating images directly in a high-level domain (e.g. brushstrokes), without the need for real pairwise data. Specifically, we train acanvas network to imitate the mapping of high-level constructs to pixels,followed by a high-level drawing network which is optimized through thismapping towards solving a desired image recreation or translation task. Wesuccessfully discover sequential vector representations of symbols, largesketches, and 3D objects, utilizing only pixel data. We display applications ofour method in image segmentation, and present several ablation studiescomparing various configurations. ", "id2": "793", "id3": "None"}
{"id": "795", "content": "With the introduction of fully convolutional neural networks, deep learninghas raised the benchmark for medical image segmentation on both speed andaccuracy, and different networks have been proposed for 2D and 3D segmentationwith promising results. Nevertheless, most networks only handle relativelysmall numbers of labels (<10), and there are very limited works on handlinghighly unbalanced object sizes especially in 3D segmentation. In this paper, wepropose a network architecture and the corresponding loss function whichimprove segmentation of very small structures. By combining skip connectionsand deep supervision with respect to the computational feasibility of 3Dsegmentation, we propose a fast converging and computationally efficientnetwork architecture for accurate segmentation. Furthermore, inspired by theconcept of focal loss, we propose an exponential logarithmic loss whichbalances the labels not only by their relative sizes but also by theirsegmentation difficulties. We achieve an average Dice coefficient of 82% onbrain segmentation with 20 labels, with the ratio of the smallest to largestobject sizes as 0.14%. Less than 100 epochs are required to reach suchaccuracy, and segmenting a 128x128x128 volume only takes around 0.4 s. ", "id2": "794", "id3": "None"}
{"id": "796", "content": "Recently, diagnosis, therapy and monitoring of human diseases involve avariety of imaging modalities, such as magnetic resonance imaging(MRI),computed tomography(CT), Ultrasound(US) and Positron-emission tomography(PET)as well as a variety of modern optical techniques. Over the past two decade, ithas been recognized that advanced image processing techniques provide valuableinformation to physicians for diagnosis, image guided therapy and surgery, andmonitoring of the treated organ to the therapy. Many researchers and companieshave invested significant efforts in the developments of advanced medical imageanalysis methods; especially in the two core studies of medical imagesegmentation and registration, segmentations of organs and lesions are used toquantify volumes and shapes used in diagnosis and monitoring treatment;registration of multimodality images of organs improves detection, diagnosisand staging of diseases as well as image-guided surgery and therapy,registration of images obtained from the same modality are used to monitorprogression of therapy. These challenging clinical-motivated applicationsintroduce novel and sophisticated mathematical problems which stimulatedevelopments of advanced optimization and computing methods, especially convexoptimization attaining optimum in a global sense, hence, bring an enormousspread of research topics for recent computational medical image analysis.Particularly, distinct from the usual image processing, most medical imageshave a big volume of acquired data, often in 3D or 4D (3D + t) along with greatnoises or incomplete image information, and form the challenging large-scaleoptimization problems; how to process such poor big data of medical imagesefficiently and solve the corresponding optimization problems robustly are thekey factors of modern medical image analysis. ", "id2": "795", "id3": "None"}
{"id": "797", "content": "Semantic image segmentation, which becomes one of the key applications inimage processing and computer vision domain, has been used in multiple domainssuch as medical area and intelligent transportation. Lots of benchmark datasetsare released for researchers to verify their algorithms. Semantic segmentationhas been studied for many years. Since the emergence of Deep Neural Network(DNN), segmentation has made a tremendous progress. In this paper, we dividesemantic image segmentation methods into two categories: traditional and recentDNN method. Firstly, we briefly summarize the traditional method as well asdatasets released for segmentation, then we comprehensively investigate recentmethods based on DNN which are described in the eight aspects: fullyconvolutional network, upsample ways, FCN joint with CRF methods, dilatedconvolution approaches, progresses in backbone network, pyramid methods,Multi-level feature and multi-stage method, supervised, weakly-supervised andunsupervised methods. Finally, a conclusion in this area is drawn. ", "id2": "796", "id3": "None"}
{"id": "798", "content": "Deep convolutional neural network (DCNN) is the state-of-the-art method forimage segmentation, which is one of key challenging computer vision tasks.However, DCNN requires a lot of training images with corresponding image masksto get a good segmentation result. Image annotation software which is easy touse and allows fast image mask generation is in great demand. To the best ofour knowledge, all existing image annotation software support only drawingbounding polygons, bounding boxes, or bounding ellipses to mark target objects.These existing software are inefficient when targeting objects that haveirregular shapes (e.g., defects in fabric images or tire images). In this paperwe design an easy-to-use image annotation software called Mask Editor for imagemask generation. Mask Editor allows drawing any bounding curve to mark objectsand improves efficiency to mark objects with irregular shapes. Mask Editor alsosupports drawing bounding polygons, drawing bounding boxes, drawing boundingellipses, painting, erasing, super-pixel-marking, image cropping, multi-classmasks, mask loading, and mask modifying. ", "id2": "797", "id3": "None"}
{"id": "799", "content": "Tumor detection in biomedical imaging is a time-consuming process for medicalprofessionals and is not without errors. Thus in recent decades, researchershave developed algorithmic techniques for image processing using a wide varietyof mathematical methods, such as statistical modeling, variational techniques,and machine learning. In this paper, we propose a semi-automatic method forliver segmentation of 2D CT scans into three labels denoting healthy, vessel,or tumor tissue based on graph cuts. First, we create a feature vector for eachpixel in a novel way that consists of the 59 intensity values in the timeseries data and propose a simplified perimeter cost term in the energyfunctional. We normalize the data and perimeter terms in the functional toexpedite the graph cut without having to optimize the scaling parameter$ lambda$. In place of a training process, predetermined tissue means arecomputed based on sample regions identified by expert radiologists. Theproposed method also has the advantage of being relatively simple to implementcomputationally. It was evaluated against the ground truth on a clinical CTdataset of 10 tumors and yielded segmentations with a mean Dice similaritycoefficient (DSC) of .77 and mean volume overlap error (VOE) of 36.7%. Theaverage processing time was 1.25 minutes per slice. ", "id2": "798", "id3": "None"}
{"id": "800", "content": "The design of neural network architectures is an important component forachieving state-of-the-art performance with machine learning systems across abroad array of tasks. Much work has endeavored to design and buildarchitectures automatically through clever construction of a search spacepaired with simple learning algorithms. Recent progress has demonstrated thatsuch meta-learning methods may exceed scalable human-invented architectures onimage classification tasks. An open question is the degree to which suchmethods may generalize to new domains. In this work we explore the constructionof meta-learning techniques for dense image prediction focused on the tasks ofscene parsing, person-part segmentation, and semantic image segmentation.Constructing viable search spaces in this domain is challenging because of themulti-scale representation of visual information and the necessity to operateon high resolution imagery. Based on a survey of techniques in dense imageprediction, we construct a recursive search space and demonstrate that evenwith efficient random search, we can identify architectures that outperformhuman-invented architectures and achieve state-of-the-art performance on threedense prediction tasks including 82.7 % on Cityscapes (street scene parsing),71.3 % on PASCAL-Person-Part (person-part segmentation), and 87.9 % on PASCALVOC 2012 (semantic image segmentation). Additionally, the resultingarchitecture is more computationally efficient, requiring half the parametersand half the computational cost as previous state of the art systems. ", "id2": "799", "id3": "None"}
{"id": "801", "content": "Robot perception systems need to perform reliable image segmentation inreal-time on noisy, raw perception data. State-of-the-art segmentationapproaches use large CNN models and carefully constructed datasets; however,these models focus on accuracy at the cost of real-time inference. Furthermore,the standard semantic segmentation datasets are not large enough for trainingCNNs without augmentation and are not representative of noisy, uncurated robotperception data. We propose improving the performance of real-time segmentationframeworks on robot perception data by transferring features learned fromsynthetic segmentation data. We show that pretraining real-time segmentationarchitectures with synthetic segmentation data instead of ImageNet improvesfine-tuning performance by reducing the bias learned in pretraining and closingthe  textit transfer gap  as a result. Our experiments show that our real-timerobot perception models pretrained on synthetic data outperform thosepretrained on ImageNet for every scale of fine-tuning data examined. Moreover,the degree to which synthetic pretraining outperforms ImageNet pretrainingincreases as the availability of robot data decreases, making our approachattractive for robotics domains where dataset collection is hard and/orexpensive. ", "id2": "800", "id3": "None"}
{"id": "802", "content": "Binary image segmentation plays an important role in computer vision and hasbeen widely used in many applications such as image and video editing, objectextraction, and photo composition. In this paper, we propose a novelinteractive binary image segmentation method based on the Markov Random Field(MRF) framework and the fast bilateral solver (FBS) technique. Specifically, weemploy the geodesic distance component to build the unary term. To ensure bothcomputation efficiency and effective responsiveness for interactivesegmentation, superpixels are used in computing geodesic distances instead ofpixels. Furthermore, we take a bilateral affinity approach for the pairwiseterm in order to preserve edge information and denoise. Through the alternatingdirection strategy, the MRF energy minimization problem is divided into twosubproblems, which then can be easily solved by steepest gradient descent (SGD)and FBS respectively. Experimental results on the VGG interactive imagesegmentation dataset show that the proposed algorithm outperforms severalstate-of-the-art ones, and in particular, it can achieve satisfactoryedge-smooth segmentation results even when the foreground and background colorappearances are quite indistinctive. ", "id2": "801", "id3": "None"}
{"id": "803", "content": "Autosomal Dominant Polycystic Kidney Disease (ADPKD) characterized byprogressive growth of renal cysts is the most prevalent and potentially lethalmonogenic renal disease, affecting one in every 500-100 people. Total KidneyVolume (TKV) and its growth computed from Computed Tomography images has beenaccepted as an essential prognostic marker for renal function loss. Due tolarge variation in shape and size of kidney in ADPKD, existing methods tocompute TKV (i.e. to segment ADKP) including those based on 2D convolutionalneural networks are not accurate enough to be directly useful in clinicalpractice. In this work, we propose multi-task 3D Convolutional Neural Networksto segment ADPK and achieve a mean DICE score of 0.95 and mean absolutepercentage TKV error of 3.86. Additionally, to solve the challenge of classimbalance, we propose to simply bootstrap cross entropy loss and compareresults with recently prevalent dice loss in medical image segmentationcommunity. ", "id2": "802", "id3": "None"}
{"id": "804", "content": "Learning long-term spatial-temporal features are critical for many videoanalysis tasks. However, existing video segmentation methods predominantly relyon static image segmentation techniques, and methods capturing temporaldependency for segmentation have to depend on pretrained optical flow models,leading to suboptimal solutions for the problem. End-to-end sequential learningto explore spatialtemporal features for video segmentation is largely limitedby the scale of available video segmentation datasets, i.e., even the largestvideo segmentation dataset only contains 90 short video clips. To solve thisproblem, we build a new large-scale video object segmentation dataset calledYouTube Video Object Segmentation dataset (YouTube-VOS). Our dataset contains4,453 YouTube video clips and 94 object categories. This is by far the largestvideo object segmentation dataset to our knowledge and has been released athttp://youtube-vos.org. We further evaluate several existing state-of-the-artvideo object segmentation algorithms on this dataset which aims to establishbaselines for the development of new algorithms in the future. ", "id2": "803", "id3": "None"}
{"id": "805", "content": "Contemporary deep learning based medical image segmentation algorithmsrequire hours of annotation labor by domain experts. These data hungry deepmodels perform sub-optimally in the presence of limited amount of labeled data.In this paper, we present a data efficient learning framework using the recentconcept of Generative Adversarial Networks; this allows a deep neural networkto perform significantly better than its fully supervised counterpart in lowannotation regime. The proposed method is an extension of our previous workwith the addition of a new unsupervised adversarial loss and a structuredprediction based architecture. To the best of our knowledge, this work is thefirst demonstration of an adversarial framework based structured predictionmodel for medical image segmentation. Though generic, we apply our method forsegmentation of blood vessels in retinal fundus images. We experiment withextreme low annotation budget (0.8 - 1.6% of contemporary annotation size). OnDRIVE and STARE datasets, the proposed method outperforms our previous methodand other fully supervised benchmark models by significant margins especiallywith very low number of annotated examples. In addition, our systematicablation studies suggest some key recipes for successfully training GAN basedsemi-supervised algorithms with an encoder-decoder style network architecture. ", "id2": "804", "id3": "None"}
{"id": "806", "content": "This chapter provides insight on how iris recognition, one of the leadingbiometric identification technologies in the world, can be impacted bypathologies and illnesses present in the eye, what are the possiblerepercussions of this influence, and what are the possible means for takingsuch effects into account when matching iris samples.  To make this study possible, a special database of iris images has been used,representing more than 20 different medical conditions of the ocular region(including cataract, glaucoma, rubeosis iridis, synechiae, iris defects,corneal pathologies and other) and containing almost 3000 samples collectedfrom 230 distinct irises. Then, with the use of four different iris recognitionmethods, a series of experiments has been conducted, concluding in severalimportant observations.  One of the most popular ocular disorders worldwide - the cataract - is shownto worsen genuine comparison scores when results obtained fromcataract-affected eyes are compared to those coming from healthy irises. Ananalysis devoted to different types of impact on eye structures caused bydiseases is also carried out with significant results. The enrollment processis highly sensitive to those eye conditions that make the iris obstructed orintroduce geometrical distortions. Disorders affecting iris geometry, orproducing obstructions are exceptionally capable of degrading the genuinecomparison scores, so that the performance of the entire biometric system canbe influenced. Experiments also reveal that imperfect execution of the imagesegmentation stage is the most prominent contributor to recognition errors. ", "id2": "805", "id3": "None"}
{"id": "807", "content": "In this paper, we propose an efficient pseudo-marginal Markov chain MonteCarlo (MCMC) sampling approach to draw samples from posterior shapedistributions for image segmentation. The computation time of the proposedapproach is independent from the size of the training set used to learn theshape prior distribution nonparametrically. Therefore, it scales well for verylarge data sets. Our approach is able to characterize the posterior probabilitydensity in the space of shapes through its samples, and to return multiplesolutions, potentially from different modes of a multimodal probabilitydensity, which would be encountered, e.g., in segmenting objects from multipleshape classes. Experimental results demonstrate the potential of the proposedapproach. ", "id2": "806", "id3": "None"}
{"id": "808", "content": "Learning long-term spatial-temporal features are critical for many videoanalysis tasks. However, existing video segmentation methods predominantly relyon static image segmentation techniques, and methods capturing temporaldependency for segmentation have to depend on pretrained optical flow models,leading to suboptimal solutions for the problem. End-to-end sequential learningto explore spatial-temporal features for video segmentation is largely limitedby the scale of available video segmentation datasets, i.e., even the largestvideo segmentation dataset only contains 90 short video clips. To solve thisproblem, we build a new large-scale video object segmentation dataset calledYouTube Video Object Segmentation dataset (YouTube-VOS). Our dataset contains3,252 YouTube video clips and 78 categories including common objects and humanactivities. This is by far the largest video object segmentation dataset to ourknowledge and we have released it at https://youtube-vos.org. Based on thisdataset, we propose a novel sequence-to-sequence network to fully exploitlong-term spatial-temporal information in videos for segmentation. Wedemonstrate that our method is able to achieve the best results on ourYouTube-VOS test set and comparable results on DAVIS 2016 compared to thecurrent state-of-the-art methods. Experiments show that the large scale datasetis indeed a key factor to the success of our model. ", "id2": "807", "id3": "None"}
{"id": "809", "content": "This paper delivers a new database of iris images collected in visible lightusing a mobile phones camera and presents results of experiments involvingexisting commercial and open-source iris recognition methods, namely: IriCore,VeriEye, MIRLIN and OSIRIS. Several important observations are made.  First, we manage to show that after simple preprocessing, such images offergood visibility of iris texture even in heavily-pigmented irides. Second, forall four methods, the enrollment stage is not much affected by the fact thatdifferent type of data is used as input. This translates to zero orclose-to-zero Failure To Enroll, i.e., cases when templates could not beextracted from the samples. Third, we achieved good matching accuracy, withcorrect genuine match rate exceeding 94.5% for all four methods, whilesimultaneously being able to maintain zero false match rate in every case.Correct genuine match rate of over 99.5% was achieved using one of thecommercial methods, showing that such images can be used with the existingbiometric solutions with minimum additional effort required. Finally, theexperiments revealed that incorrect image segmentation is the most prevalentcause of recognition accuracy decrease.  To our best knowledge, this is the first database of iris images capturedusing a mobile device, in which image quality exceeds this of a near-infraredilluminated iris images, as defined in ISO/IEC 19794-6 and 29794-6 documents.This database will be publicly available to all researchers. ", "id2": "808", "id3": "None"}
{"id": "810", "content": "This paper presents the experimental study revealing weaker performance ofthe automatic iris recognition methods for cataract-affected eyes when comparedto healthy eyes. There is little research on the topic, mostly incorporatingscarce databases that are often deficient in images representing more than oneillness. We built our own database, acquiring 1288 eye images of 37 patients ofthe Medical University of Warsaw. Those images represent several common oculardiseases, such as cataract, along with less ordinary conditions, such as irispattern alterations derived from illness or eye trauma. Images were captured innear-infrared light (used in biometrics) and for selected cases also in visiblelight (used in ophthalmological diagnosis). Since cataract is a disorder thatis most populated by samples in the database, in this paper we focus solely onthis illness. To assess the extent of the performance deterioration we usethree iris recognition methodologies (commercial and academic solutions) tocalculate genuine match scores for healthy eyes and those influenced bycataract. Results show a significant degradation in iris recognitionreliability manifesting by worsening the genuine scores in all three matchersused in this study (12% of genuine score increase for an academic matcher, upto 175% of genuine score increase obtained for an example commercial matcher).This increase in genuine scores affected the final false non-match rate in twomatchers. To our best knowledge this is the only study of such kind thatemploys more than one iris matcher, and analyzes the iris image segmentation asa potential source of decreased reliability. ", "id2": "809", "id3": "None"}
{"id2": 1077, "id3": "809", "content": "This paper presents the experimental study revealing weaker performance ofthe automatic iris recognition methods for cataract-affected eyes when comparedto healthy eyes. There is little research on the topic, mostly incorporatingscarce databases that are often deficient in images representing more than oneillness. We built our own database, acquiring 1288 eye images of 37 patients ofthe Medical University of Warsaw. Those images represent several common oculardiseases, such as cataract, along with less ordinary conditions, such as irispattern alterations derived from illness or eye trauma. Images were captured innear-infrared light (used in biometrics) and for selected cases also in visiblelight (used in ophthalmological diagnosis). Since cataract is a disorder thatis most populated by samples in the database, in this paper we focus solely onthis illness. To assess the extent of the performance deterioration we usethree iris recognition methodologies (commercial and academic solutions) tocalculate genuine match scores for healthy eyes and those influenced bycataract. Results show a significant degradation in iris recognitionreliability manifesting by worsening the genuine scores in all three matchersused in this study (12% of genuine score increase for an academic matcher, upto 175% of genuine score increase obtained for an example commercial matcher).This increase in genuine scores affected the final false non-match rate in twomatchers. To our best knowledge this is the only study of such kind thatemploys more than one iris matcher, and analyzes the iris image segmentation asa potential source of decreased reliability."}
{"id": "811", "content": "Leaping into the rapidly developing world of deep learning is an exciting andsometimes confusing adventure. All of the advice and tutorials available can behard to organize and work through, especially when training specific models onspecific datasets, different from those originally used to train the network.In this short guide, we aim to walk the reader through the techniques that wehave used to successfully train two deep neural networks for pixel-wiseclassification, including some data management and augmentation approaches forworking with image data that may be insufficiently annotated or relativelyhomogenous. ", "id2": "810", "id3": "None"}
{"id": "812", "content": "The tracing of neural pathways through large volumes of image data is anincredibly tedious and time-consuming process that significantly encumbersprogress in neuroscience. We are exploring deep learnings potential toautomate segmentation of high-resolution scanning electron microscope (SEM)image data to remove that barrier. We have started with neural pathway tracingthrough 5.1GB of whole-brain serial-section slices from larval zebrafishcollected by the Center for Brain Science at Harvard University. This kind ofmanual image segmentation requires years of careful work to properly trace theneural pathways in an organism as small as a zebrafish larva (approximately 5mmin total body length). In automating this process, we would vastly improveproductivity, leading to faster data analysis and breakthroughs inunderstanding the complexity of the brain. We will build upon prior attempts toemploy deep learning for automatic image segmentation extending methods forunconventional deep learning data. ", "id2": "811", "id3": "None"}
{"id": "813", "content": "In this work, a region-based Deep Convolutional Neural Network framework isproposed for document structure learning. The contribution of this workinvolves efficient training of region based classifiers and effectiveensembling for document image classification. A primary level of inter-domaintransfer learning is used by exporting weights from a pre-trained VGG16architecture on the ImageNet dataset to train a document classifier on wholedocument images. Exploiting the nature of region based influence modelling, asecondary level of intra-domain transfer learning is used for rapid trainingof deep learning models for image segments. Finally, stacked generalizationbased ensembling is utilized for combining the predictions of the base deepneural network models. The proposed method achieves state-of-the-art accuracyof 92.2% on the popular RVL-CDIP document image dataset, exceeding benchmarksset by existing algorithms. ", "id2": "812", "id3": "None"}
{"id": "814", "content": "For the task of subdecimeter aerial imagery segmentation, fine-grainedsemantic segmentation results are usually difficult to obtain because ofcomplex remote sensing content and optical conditions. Recently, convolutionalneural networks (CNNs) have shown outstanding performance on this task.Although many deep neural network structures and techniques have been appliedto improve the accuracy, few have paid attention to better differentiating theeasily confused classes. In this paper, we propose TreeSegNet which adopts anadaptive network to increase the classification rate at the pixelwise level.Specifically, based on the infrastructure of DeepUNet, a Tree-CNN block inwhich each node represents a ResNeXt unit is constructed adaptively accordingto the confusion matrix and the proposed TreeCutting algorithm. By transportingfeature maps through concatenating connections, the Tree-CNN block fusesmultiscale features and learns best weights for the model. In experiments onthe ISPRS 2D semantic labeling Potsdam dataset, the results obtained byTreeSegNet are better than those of other published state-of-the-art methods.Detailed comparison and analysis show that the improvement brought by theadaptive Tree-CNN block is significant. ", "id2": "813", "id3": "None"}
{"id": "815", "content": "Every year millions of people die due to disease of Cancer. Due to itsinvasive nature it is very complex to cure even in primary stages. Hence, onlymethod to survive this disease completely is via forecasting by analyzing theearly mutation in cells of the patient biopsy. Cell Segmentation can be used tofind cell which have left their nuclei. This enables faster cure and high rateof survival. Cell counting is a hard, yet tedious task that would greatlybenefit from automation. To accomplish this task, segmentation of cells need tobe accurate. In this paper, we have improved the learning of training data byour network. It can annotate precise masks on test data. we examine thestrength of activation functions in medical image segmentation task byimproving learning rates by our proposed Carving Technique. Identifying thecells nuclei is the starting point for most analyses, identifying nuclei allowsresearchers to identify each individual cell in a sample, and by measuring howcells react to various treatments, the researcher can understand the underlyingbiological processes at work. Experimental results shows the efficiency of theproposed work. ", "id2": "814", "id3": "None"}
{"id": "816", "content": "Spatial pyramid pooling module or encode-decoder structure are used in deepneural networks for semantic segmentation task. The former networks are able toencode multi-scale contextual information by probing the incoming features withfilters or pooling operations at multiple rates and multiple effectivefields-of-view, while the latter networks can capture sharper object boundariesby gradually recovering the spatial information. In this work, we propose tocombine the advantages from both methods. Specifically, our proposed model,DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder moduleto refine the segmentation results especially along object boundaries. Wefurther explore the Xception model and apply the depthwise separableconvolution to both Atrous Spatial Pyramid Pooling and decoder modules,resulting in a faster and stronger encoder-decoder network. We demonstrate theeffectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets,achieving the test set performance of 89.0 % and 82.1 % without anypost-processing. Our paper is accompanied with a publicly available referenceimplementation of the proposed models in Tensorflow at url https://github.com/tensorflow/models/tree/master/research/deeplab . ", "id2": "815", "id3": "None"}
{"id2": 1078, "id3": "815", "content": "Spatial pyramid pooling module or encode-decoder structure are used in deepneural networks for semantic segmentation task. The former networks are able toencode multi-scale contextual information by probing the incoming features withfilters or pooling operations at multiple rates and multiple effectivefields-of-view, while the latter networks can capture sharper object boundariesby gradually recovering the spatial information. In this work, we propose tocombine the advantages from both methods. Specifically, our proposed model,DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder moduleto refine the segmentation results especially along object boundaries. Wefurther explore the Xception model and apply the depthwise separableconvolution to both Atrous Spatial Pyramid Pooling and decoder modules,resulting in a faster and stronger encoder-decoder network. We demonstrate theeffectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets,achieving the test set performance of 89.0 % and 82.1 % without anypost-processing. Our paper is accompanied with a publicly available referenceimplementation of the proposed models in Tensorflow at url https://github.com/tensorflow/models/tree/master/research/deeplab ."}
{"id": "817", "content": "For many biological image segmentation tasks, including topologicalknowledge, such as the nesting of classes, can greatly improve results.However, most out-of-the-box CNN models are still blind to such priorinformation. In this paper, we propose a novel approach to encode thisinformation, through a multi-level activation layer and three compatiblelosses. We benchmark all of them on nuclei segmentation in bright-fieldmicroscopy cell images from the 2018 Data Science Bowl challenge, offering anexemplary segmentation task with cells and nested subcellular structures. Ourscheme greatly speeds up learning, and outperforms standard multi-classclassification with soft-max activation and a previously proposed methodstemming from it, improving the Dice score significantly (p-values<0.007). Ourapproach is conceptually simple, easy to implement and can be integrated in anyCNN architecture. It can be generalized to a higher number of classes, with orwithout further relations of containment. ", "id2": "816", "id3": "None"}
{"id": "818", "content": "Confocal Laser Endomicroscope (CLE) is a novel handheld fluorescence imagingdevice that has shown promise for rapid intraoperative diagnosis of brain tumortissue. Currently CLE is capable of image display only and lacks an automaticsystem to aid the surgeon in analyzing the images. The goal of this project wasto develop a computer-aided diagnostic approach for CLE imaging of human gliomawith feature localization function. Despite the tremendous progress in objectdetection and image segmentation methods in recent years, most of such methodsrequire large annotated datasets for training. However, manual annotation ofthousands of histopathological images by physicians is costly and timeconsuming. To overcome this problem, we propose a Weakly-Supervised Learning(WSL)-based model for feature localization that trains on image-levelannotations, and then localizes incidences of a class-of-interest in the testimage. We developed a novel convolutional neural network for diagnosticfeatures localization from CLE images by employing a novel multiscaleactivation map that is laterally inhibited and collaterally integrated. Tovalidate our method, we compared proposed models output to the manualannotation performed by four neurosurgeons on test images. Proposed modelachieved 88% mean accuracy and 86% mean intersection over union on intermediatefeatures and 87% mean accuracy and 88% mean intersection over union onrestrictive fine features, while outperforming other state of the art methodstested. This system can improve accuracy and efficiency in characterization ofCLE images of glioma tissue during surgery, augment intraoperativedecision-making process regarding tumor margin and affect resection rates. ", "id2": "817", "id3": "None"}
{"id": "819", "content": "Existing works on semantic segmentation typically consider a small number oflabels, ranging from tens to a few hundreds. With a large number of labels,training and evaluation of such task become extremely challenging due tocorrelation between labels and lack of datasets with complete annotations. Weformulate semantic segmentation as a problem of image segmentation given asemantic concept, and propose a novel system which can potentially handle anunlimited number of concepts, including objects, parts, stuff, and attributes.We achieve this using a weakly and semi-supervised framework leveragingmultiple datasets with different levels of supervision. We first train a deepneural network on a 6M stock image dataset with only image-level labels tolearn visual-semantic embedding on 18K concepts. Then, we refine and extend theembedding network to predict an attention map, using a curated dataset withbounding box annotations on 750 concepts. Finally, we train an attention-drivenclass agnostic segmentation network using an 80-category fully annotateddataset. We perform extensive experiments to validate that the proposed systemperforms competitively to the state of the art on fully supervised concepts,and is capable of producing accurate segmentations for weakly learned andunseen concepts. ", "id2": "818", "id3": "None"}
{"id": "820", "content": "Within this thesis we propose a platform for combining Augmented Reality (AR)hardware with machine learning in a user-oriented pipeline, offering to themedical staff an intuitive 3D visualization of volumetric Computed Tomography(CT) and Magnetic Resonance Imaging (MRI) medical image segmentations insidethe AR headset, that does not need human intervention for loading, processingand segmentation of medical images. The AR visualization, based on MicrosoftHoloLens, employs a modular and thus scalable frontend-backend architecture forreal-time visualizations on multiple AR headsets. As Convolutional NeuralNetworks (CNNs) have lastly demonstrated superior performance for the machinelearning task of image semantic segmentation, the pipeline also includes afully automated CNN algorithm for the segmentation of the liver from CT scans.The model is based on the Deep Retinal Image Understanding (DRIU) model whichis a Fully Convolutional Network with side outputs from feature maps withdifferent resolution, extracted at different stages of the network. Thealgorithm is 2.5D which means that the input is a set of consecutive scanslices. The experiments have been performed on the Liver Tumor SegmentationChallenge (LiTS) dataset for liver segmentation and demonstrated good resultsand flexibility. While multiple approaches exist in the domain, only few ofthem have focused on overcoming the practical aspects which still largely holdthis technology away from the operating rooms. In line with this, we also arenext planning an evaluation from medical doctors and radiologists in areal-world environment. ", "id2": "819", "id3": "None"}
{"id": "821", "content": "Unsupervised learning poses one of the most difficult challenges in computervision today. The task has an immense practical value with many applications inartificial intelligence and emerging technologies, as large quantities ofunlabeled videos can be collected at relatively low cost. In this paper, weaddress the unsupervised learning problem in the context of detecting the mainforeground objects in single images. We train a student deep network to predictthe output of a teacher pathway that performs unsupervised object discovery invideos or large image collections. Our approach is different from publishedmethods on unsupervised object discovery. We move the unsupervised learningphase during training time, then at test time we apply the standardfeed-forward processing along the student pathway. This strategy has thebenefit of allowing increased generalization possibilities during training,while remaining fast at testing. Our unsupervised learning algorithm can runover several generations of student-teacher training. Thus, a group of studentnetworks trained in the first generation collectively create the teacher at thenext generation. In experiments our method achieves top results on threecurrent datasets for object discovery in video, unsupervised image segmentationand saliency detection. At test time the proposed system is fast, being one totwo orders of magnitude faster than published unsupervised methods. ", "id2": "820", "id3": "None"}
{"id2": 1079, "id3": "820", "content": "Unsupervised learning poses one of the most difficult challenges in computervision today. The task has an immense practical value with many applications inartificial intelligence and emerging technologies, as large quantities ofunlabeled videos can be collected at relatively low cost. In this paper, weaddress the unsupervised learning problem in the context of detecting the mainforeground objects in single images. We train a student deep network to predictthe output of a teacher pathway that performs unsupervised object discovery invideos or large image collections. Our approach is different from publishedmethods on unsupervised object discovery. We move the unsupervised learningphase during training time, then at test time we apply the standardfeed-forward processing along the student pathway. This strategy has thebenefit of allowing increased generalization possibilities during training,while remaining fast at testing. Our unsupervised learning algorithm can runover several generations of student-teacher training. Thus, a group of studentnetworks trained in the first generation collectively create the teacher at thenext generation. In experiments our method achieves top results on threecurrent datasets for object discovery in video, unsupervised image segmentationand saliency detection. At test time the proposed system is fast, being one totwo orders of magnitude faster than published unsupervised methods."}
{"id": "822", "content": "The interactive image segmentation algorithm can provide an intelligent waysto understand the intention of user input. Many interactive methods have theproblem of that ask for large number of user input. To efficient produceintuitive segmentation under limited user input is important for industrialapplication. In this paper, we reveal a positive feedback system on imagesegmentation to show the pixels of self-learning. Two approaches, iterativerandom walks and boundary random walks, are proposed for segmentationpotential, which is the key step in feedback system. Experiment results onimage segmentation indicates that proposed algorithms can obtain more efficientinput to random walks. And higher segmentation performance can be obtained byapplying the iterative boundary random walks algorithm. ", "id2": "821", "id3": "None"}
{"id": "823", "content": "Segmentation of the left atrial chamber and assessing its morphology, areessential for improving our understanding of atrial fibrillation, the mostcommon type of cardiac arrhythmia. Automation of this process in 3D gadoliniumenhanced-MRI (GE-MRI) data is desirable, as manual delineation istime-consuming, challenging and observer-dependent. Recently, deepconvolutional neural networks (CNNs) have gained tremendous traction andachieved state-of-the-art results in medical image segmentation. However, it isdifficult to incorporate local and global information without using contracting(pooling) layers, which in turn reduces segmentation accuracy for smallerstructures. In this paper, we propose a 3D CNN for volumetric segmentation ofthe left atrial chamber in LGE-MRI. Our network is based on the well knownU-Net architecture. We employ a 3D fully convolutional network, with dilatedconvolutions in the lowest level of the network, and residual connectionsbetween encoder blocks to incorporate local and global knowledge. The resultsshow that including global context through the use of dilated convolutions,helps in domain adaptation, and the overall segmentation accuracy is improvedin comparison to a 3D U-Net. ", "id2": "822", "id3": "None"}
{"id": "824", "content": "In this paper, we adopt 3D Convolutional Neural Networks to segmentvolumetric medical images. Although deep neural networks have been proven to bevery effective on many 2D vision tasks, it is still challenging to apply themto 3D tasks due to the limited amount of annotated 3D data and limitedcomputational resources. We propose a novel 3D-based coarse-to-fine frameworkto effectively and efficiently tackle these challenges. The proposed 3D-basedframework outperforms the 2D counterpart to a large margin since it canleverage the rich spatial infor- mation along all three axes. We conductexperiments on two datasets which include healthy and pathological pancreasesrespectively, and achieve the current state-of-the-art in terms ofDice-S  o rensen Coefficient (DSC). On the NIH pancreas segmentation dataset,we outperform the previous best by an average of over 2%, and the worst case isimproved by 7% to reach almost 70%, which indicates the reliability of ourframework in clinical applications. ", "id2": "823", "id3": "None"}
{"id": "825", "content": "Multi-atlas segmentation approach is one of the most widely-used imagesegmentation techniques in biomedical applications. There are two majorchallenges in this category of methods, i.e., atlas selection and label fusion.In this paper, we propose a novel multi-atlas segmentation method thatformulates multi-atlas segmentation in a deep learning framework for bettersolving these challenges. The proposed method, dubbed deep fusion net (DFN), isa deep architecture that integrates a feature extraction subnet and a non-localpatch-based label fusion (NL-PLF) subnet in a single network. The networkparameters are learned by end-to-end training for automatically learning deepfeatures that enable optimal performance in a NL-PLF framework. The learneddeep features are further utilized in defining a similarity measure for atlasselection. By evaluating on two public cardiac MR datasets of SATA-13 and LV-09for left ventricle segmentation, our approach achieved 0.833 in averaged Dicemetric (ADM) on SATA-13 dataset and 0.95 in ADM for epicardium segmentation onLV-09 dataset, comparing favorably with the other automatic left ventriclesegmentation methods. We also tested our approach on Cardiac Atlas Project(CAP) testing set of MICCAI 2013 SATA Segmentation Challenge, and our methodachieved 0.815 in ADM, ranking highest at the time of writing. ", "id2": "824", "id3": "None"}
{"id": "826", "content": "Confusing classes that are ubiquitous in real world often degrade performancefor many vision related applications like object detection, classification, andsegmentation. The confusion errors are not only caused by similar visualpatterns but also amplified by various factors during the training of ourdesigned models, such as reduced feature resolution in the encoding process orimbalanced data distributions. A large amount of deep learning based networkstructures has been proposed in recent years to deal with these individualfactors and improve network performance. However, to our knowledge, no existingwork in semantic image segmentation is designed to tackle confusion errorsexplicitly. In this paper, we present a novel and general network structurethat reduces confusion errors in more direct manner and apply the network forsemantic segmentation. There are two major contributions in our networkstructure: 1) We ensemble subnets with heterogeneous output spaces based on thediscriminative confusing groups. The training for each subnet can distinguishconfusing classes within the group without affecting unrelated classes outsidethe group. 2) We propose an improved cross-entropy loss function that maximizesthe probability assigned to the correct class and penalizes the probabilitiesassigned to the confusing classes at the same time. Our network structure is ageneral structure and can be easily adapted to any other networks to furtherreduce confusion errors. Without any changes in the feature encoder andpost-processing steps, our experiments demonstrate consistent and significantimprovements on different baseline models on Cityscapes and PASCAL VOC datasets(e.g., 3.05% over ResNet-101 and 1.30% over ResNet-38). ", "id2": "825", "id3": "None"}
{"id": "827", "content": "Semantic Segmentation using deep convolutional neural network pose morecomplex challenge for any GPU intensive task. As it has to compute million ofparameters, it results to huge memory consumption. Moreover, extracting finerfeatures and conducting supervised training tends to increase the complexity.With the introduction of Fully Convolutional Neural Network, which uses finerstrides and utilizes deconvolutional layers for upsampling, it has been a go tofor any image segmentation task. In this paper, we propose two segmentationarchitecture which not only needs one-third the parameters to compute but alsogives better accuracy than the similar architectures. The model weights weretransferred from the popular neural net like VGG19 and VGG16 which were trainedon Imagenet classification data-set. Then we transform all the fully connectedlayers to convolutional layers and use dilated convolution for decreasing theparameters. Lastly, we add finer strides and attach four skip architectureswhich are element-wise summed with the deconvolutional layers in steps. Wetrain and test on different sparse and fine data-sets like Pascal VOC2012,Pascal-Context and NYUDv2 and show how better our model performs in this tasks.On the other hand our model has a faster inference time and consumes lessmemory for training and testing on NVIDIA Pascal GPUs, making it more efficientand less memory consuming architecture for pixel-wise segmentation. ", "id2": "826", "id3": "None"}
{"id": "828", "content": "Thalamic alterations are relevant to many neurological disorders includingAlzheimers disease, Parkinsons disease and multiple sclerosis. Routineinterventions to improve symptom severity in movement disorders, for example,often consist of surgery or deep brain stimulation to diencephalic nuclei.Therefore, accurate delineation of grey matter thalamic subregions is of theupmost clinical importance. MRI is highly appropriate for structuralsegmentation as it provides different views of the anatomy from a singlescanning session. Though with several contrasts potentially available, it isalso of increasing importance to develop new image segmentation techniques thatcan operate multi-spectrally. We hereby propose a new segmentation method foruse with multi-modality data, which we evaluated for automated segmentation ofmajor thalamic subnuclear groups using T1-, T2*-weighted and quantitativesusceptibility mapping (QSM) information. The proposed method consists of foursteps: highly iterative image co-registration, manual segmentation on theaverage training-data template, supervised learning for pattern recognition,and a final convex optimisation step imposing further spatial constraints torefine the solution. This led to solutions in greater agreement with manualsegmentation than the standard Morel atlas based approach. Furthermore, we showthat the multi-contrast approach boosts segmentation performances. We theninvestigated whether prior knowledge using the training-template contours couldfurther improve convex segmentation accuracy and robustness, which led tohighly precise multi-contrast segmentations in single subjects. This approachcan be extended to most 3D imaging data types and any region of interestdiscernible in single scans or multi-subject templates. ", "id2": "827", "id3": "None"}
{"id": "829", "content": "Mesh labeling is the key problem of classifying the facets of a 3D mesh witha label among a set of possible ones. State-of-the-art methods model meshlabeling as a Markov Random Field over the facets. These algorithms map imagesegmentations to the mesh by minimizing an energy function that comprises adata term, a smoothness terms, and class-specific priors. The latter favor alabeling with respect to another depending on the orientation of the facetnormals. In this paper we propose a novel energy term that acts as a prior, butdoes not require any prior knowledge about the scene nor scene-specificrelationship among classes. It bootstraps from a coarse mapping of the 2Dsegmentations on the mesh, and it favors the facets to be labeled according tothe statistics of the mesh normals in their neighborhood. We tested ourapproach against five different datasets and, even if we do not inject priorknowledge, our method adapts to the data and overcomes the state-of-the-art. ", "id2": "828", "id3": "None"}
{"id2": 1080, "id3": "828", "content": "Mesh labeling is the key problem of classifying the facets of a 3D mesh witha label among a set of possible ones. State-of-the-art methods model meshlabeling as a Markov Random Field over the facets. These algorithms map imagesegmentations to the mesh by minimizing an energy function that comprises adata term, a smoothness terms, and class-specific priors. The latter favor alabeling with respect to another depending on the orientation of the facetnormals. In this paper we propose a novel energy term that acts as a prior, butdoes not require any prior knowledge about the scene nor scene-specificrelationship among classes. It bootstraps from a coarse mapping of the 2Dsegmentations on the mesh, and it favors the facets to be labeled according tothe statistics of the mesh normals in their neighborhood. We tested ourapproach against five different datasets and, even if we do not inject priorknowledge, our method adapts to the data and overcomes the state-of-the-art."}
{"id": "830", "content": "In recent years, Fully Convolutional Networks (FCN) has been widely used invarious semantic segmentation tasks, including multi-modal remote sensingimagery. How to fuse multi-modal data to improve the segmentation performancehas always been a research hotspot. In this paper, a novel end-toend fullyconvolutional neural network is proposed for semantic segmentation of naturalcolor, infrared imagery and Digital Surface Models (DSM). It is based on amodified DeepUNet and perform the segmentation in a multi-task way. Thechannels are clustered into groups and processed on different task pipelines.After a series of segmentation and fusion, their shared features and privatefeatures are successfully merged together. Experiment results show that thefeature fusion network is efficient. And our approach achieves good performancein ISPRS Semantic Labeling Contest (2D). ", "id2": "829", "id3": "None"}
{"id": "831", "content": "As melanoma diagnoses increase across the US, automated efforts to identifymalignant lesions become increasingly of interest to the research community.Segmentation of dermoscopic images is the first step in this process, thusaccuracy is crucial. Although techniques utilizing convolutional neuralnetworks have been used in the past for lesion segmentation, we present asolution employing the recently published DeepLab 3, an atrous convolutionmethod for image segmentation. Although the results produced by this run arenot ideal, with a mean Jaccard index of 0.498, we believe that with furtheradjustments and modifications to the compatibility with the DeepLab code andwith training on more powerful processing units, this method may achieve betterresults in future trials. ", "id2": "830", "id3": "None"}
{"id": "832", "content": "One of the problems on the way to successful implementation of neuralnetworks is the quality of annotation. For instance, different annotators canannotate images in a different way and very often their decisions do not matchexactly and in extreme cases are even mutually exclusive which results in noisyannotations and, consequently, inaccurate predictions.  To avoid that problem in the task of computed tomography (CT) imagingsegmentation we propose a clearing algorithm for annotations. It consists of 3stages:  - annotators scoring, which assigns a higher confidence level to betterannotators;  - nodules scoring, which assigns a higher confidence level to nodulesconfirmed by good annotators;  - nodules merging, which aggregates annotations according to nodulesconfidence.  In general, the algorithm can be applied to many different tasks (namely,binary and multi-class semantic segmentation, and also with trivial adjustmentsto classification and regression) where there are several annotators labelingeach image. ", "id2": "831", "id3": "None"}
{"id": "833", "content": "White matter hyperintensity (WMH) is commonly found in elder individuals andappears to be associated with brain diseases. U-net is a convolutional networkthat has been widely used for biomedical image segmentation. Recently, U-nethas been successfully applied to WMH segmentation. Random initialization isusally used to initialize the model weights in the U-net. However, the modelmay coverage to different local optima with different randomly initializedweights. We find a combination of thresholding and averaging the outputs ofU-nets with different random initializations can largely improve the WMHsegmentation accuracy. Based on this observation, we propose a post-processingtechnique concerning the way how averaging and thresholding are conducted.Specifically, we first transfer the score maps from three U-nets to binarymasks via thresholding and then average those binary masks to obtain the finalWMH segmentation. Both quantitative analysis (via the Dice similaritycoefficient) and qualitative analysis (via visual examinations) reveal thesuperior performance of the proposed method. This post-processing technique isindependent of the model used. As such, it can also be applied to situationswhere other deep learning models are employed, especially when randominitialization is adopted and pre-training is unavailable. ", "id2": "832", "id3": "None"}
{"id": "834", "content": "Convolutional neural networks are powerful tools for image segmentation andclassification. Here, we use this method to identify and mark the heart regionof Drosophila at different developmental stages in the cross-sectional imagesacquired by a custom optical coherence microscopy (OCM) system. With ourwell-trained convolutional neural network model, the heart regions throughmultiple heartbeat cycles can be marked with an intersection over union (IOU)of ~86%. Various morphological and dynamical cardiac parameters can bequantified accurately with automatically segmented heart regions. This studydemonstrates an efficient heart segmentation method to analyze OCM images ofthe beating heart in Drosophila. ", "id2": "833", "id3": "None"}
{"id": "835", "content": "Deep neural network architectures have traditionally been designed andexplored with human expertise in a long-lasting trial-and-error process. Thisprocess requires huge amount of time, expertise, and resources. To address thistedious problem, we propose a novel algorithm to optimally find hyperparametersof a deep network architecture automatically. We specifically focus ondesigning neural architectures for medical image segmentation task. Ourproposed method is based on a policy gradient reinforcement learning for whichthe reward function is assigned a segmentation evaluation utility (i.e., diceindex). We show the efficacy of the proposed method with its low computationalcost in comparison with the state-of-the-art medical image segmentationnetworks. We also present a new architecture design, a densely connectedencoder-decoder CNN, as a strong baseline architecture to apply the proposedhyperparameter search algorithm. We apply the proposed algorithm to each layerof the baseline architectures. As an application, we train the proposed systemon cine cardiac MR images from Automated Cardiac Diagnosis Challenge (ACDC)MICCAI 2017. Starting from a baseline segmentation architecture, the resultingnetwork architecture obtains the state-of-the-art results in accuracy withoutperforming any trial-and-error based architecture design approaches or closesupervision of the hyperparameters changes. ", "id2": "834", "id3": "None"}
{"id": "836", "content": "The Conditional Random Field as a Recurrent Neural Network layer is arecently proposed algorithm meant to be placed on top of an existingFully-Convolutional Neural Network to improve the quality of semanticsegmentation. In this paper, we test whether this algorithm, which was shown toimprove semantic segmentation for 2D RGB images, is able to improvesegmentation quality for 3D multi-modal medical images. We developed animplementation of the algorithm which works for any number of spatialdimensions, input/output image channels, and reference image channels. As faras we know this is the first publicly available implementation of this sort. Wetested the algorithm with two distinct 3D medical imaging datasets, weconcluded that the performance differences observed were not statisticallysignificant. Finally, in the discussion section of the paper, we go into thereasons as to why this technique transfers poorly from natural images tomedical images. ", "id2": "835", "id3": "None"}
{"id": "837", "content": "In this paper, we present UNet++, a new, more powerful architecture formedical image segmentation. Our architecture is essentially a deeply-supervisedencoder-decoder network where the encoder and decoder sub-networks areconnected through a series of nested, dense skip pathways. The re-designed skippathways aim at reducing the semantic gap between the feature maps of theencoder and decoder sub-networks. We argue that the optimizer would deal withan easier learning task when the feature maps from the decoder and encodernetworks are semantically similar. We have evaluated UNet++ in comparison withU-Net and wide U-Net architectures across multiple medical image segmentationtasks: nodule segmentation in the low-dose CT scans of chest, nucleisegmentation in the microscopy images, liver segmentation in abdominal CTscans, and polyp segmentation in colonoscopy videos. Our experimentsdemonstrate that UNet++ with deep supervision achieves an average IoU gain of3.9 and 3.4 points over U-Net and wide U-Net, respectively. ", "id2": "836", "id3": "None"}
{"id": "838", "content": "We propose a novel method for learning convolutional neural imagerepresentations without manual supervision. We use motion cues in the form ofoptical flow, to supervise representations of static images. The obviousapproach of training a network to predict flow from a single image can beneedlessly difficult due to intrinsic ambiguities in this prediction task. Weinstead propose a much simpler learning goal: embed pixels such that thesimilarity between their embeddings matches that between their optical flowvectors. At test time, the learned deep network can be used without access tovideo or flow information and transferred to tasks such as imageclassification, detection, and segmentation. Our method, which significantlysimplifies previous attempts at using motion for self-supervision, achievesstate-of-the-art results in self-supervision using motion cues, competitiveresults for self-supervision in general, and is overall state of the art inself-supervised pretraining for semantic image segmentation, as demonstrated onstandard benchmarks. ", "id2": "837", "id3": "None"}
{"id": "839", "content": "Over the past decades, state-of-the-art medical image segmentation hasheavily rested on signal processing paradigms, most notably registration-basedlabel propagation and pair-wise patch comparison, which are generally slowdespite a high segmentation accuracy. In recent years, deep learning hasrevolutionalized computer vision with many practices outperforming prior art,in particular the convolutional neural network (CNN) studies on imageclassification. Deep CNN has also started being applied to medical imagesegmentation lately, but generally involves long training and demanding memoryrequirements, achieving limited success. We propose a patch-based deep learningframework based on a revisit to the classic neural network model withsubstantial modernization, including the use of Rectified Linear Unit (ReLU)activation, dropout layers, 2.5D tri-planar patch multi-pathway settings. In atest application to hippocampus segmentation using 100 brain MR images from theADNI database, our approach significantly outperformed prior art in terms ofboth segmentation accuracy and speed: scoring a median Dice score up to 90.98%on a near real-time performance (<1s). ", "id2": "838", "id3": "None"}
{"id": "840", "content": "Semantic segmentation of medical images is a crucial step for thequantification of healthy anatomy and diseases alike. The majority of thecurrent state-of-the-art segmentation algorithms are based on deep neuralnetworks and rely on large datasets with full pixel-wise annotations. Producingsuch annotations can often only be done by medical professionals and requireslarge amounts of valuable time. Training a medical image segmentation networkwith weak annotations remains a relatively unexplored topic. In this work weinvestigate training strategies to learn the parameters of a pixel-wisesegmentation network from scribble annotations alone. We evaluate thetechniques on public cardiac (ACDC) and prostate (NCI-ISBI) segmentationdatasets. We find that the networks trained on scribbles suffer from aremarkably small degradation in Dice of only 2.9% (cardiac) and 4.5% (prostate)with respect to a network trained on full annotations. ", "id2": "839", "id3": "None"}
{"id2": 1081, "id3": "839", "content": "Semantic segmentation of medical images is a crucial step for thequantification of healthy anatomy and diseases alike. The majority of thecurrent state-of-the-art segmentation algorithms are based on deep neuralnetworks and rely on large datasets with full pixel-wise annotations. Producingsuch annotations can often only be done by medical professionals and requireslarge amounts of valuable time. Training a medical image segmentation networkwith weak annotations remains a relatively unexplored topic. In this work weinvestigate training strategies to learn the parameters of a pixel-wisesegmentation network from scribble annotations alone. We evaluate thetechniques on public cardiac (ACDC) and prostate (NCI-ISBI) segmentationdatasets. We find that the networks trained on scribbles suffer from aremarkably small degradation in Dice of only 2.9% (cardiac) and 4.5% (prostate)with respect to a network trained on full annotations."}
{"id": "841", "content": "Unpaired image-to-image translation is the problem of mapping an image in thesource domain to one in the target domain, without requiring correspondingimage pairs. To ensure the translated images are realistically plausible,recent works, such as Cycle-GAN, demands this mapping to be invertible. While,this requirement demonstrates promising results when the domains are unimodal,its performance is unpredictable in a multi-modal scenario such as in an imagesegmentation task. This is because, invertibility does not necessarily enforcesemantic correctness. To this end, we present a semantically-consistent GANframework, dubbed Sem-GAN, in which the semantics are defined by the classidentities of image segments in the source domain as produced by a semanticsegmentation algorithm. Our proposed framework includes consistency constraintson the translation task that, together with the GAN loss and thecycle-constraints, enforces that the images when translated will inherit theappearances of the target domain, while (approximately) maintaining theiridentities from the source domain. We present experiments on severalimage-to-image translation tasks and demonstrate that Sem-GAN improves thequality of the translated images significantly, sometimes by more than 20% onthe FCN score. Further, we show that semantic segmentation models, trained withsynthetic images translated via Sem-GAN, leads to significantly bettersegmentation results than other variants. ", "id2": "840", "id3": "None"}
{"id2": 1082, "id3": "840", "content": "Unpaired image-to-image translation is the problem of mapping an image in thesource domain to one in the target domain, without requiring correspondingimage pairs. To ensure the translated images are realistically plausible,recent works, such as Cycle-GAN, demands this mapping to be invertible. While,this requirement demonstrates promising results when the domains are unimodal,its performance is unpredictable in a multi-modal scenario such as in an imagesegmentation task. This is because, invertibility does not necessarily enforcesemantic correctness. To this end, we present a semantically-consistent GANframework, dubbed Sem-GAN, in which the semantics are defined by the classidentities of image segments in the source domain as produced by a semanticsegmentation algorithm. Our proposed framework includes consistency constraintson the translation task that, together with the GAN loss and thecycle-constraints, enforces that the images when translated will inherit theappearances of the target domain, while (approximately) maintaining theiridentities from the source domain. We present experiments on severalimage-to-image translation tasks and demonstrate that Sem-GAN improves thequality of the translated images significantly, sometimes by more than 20% onthe FCN score. Further, we show that semantic segmentation models, trained withsynthetic images translated via Sem-GAN, leads to significantly bettersegmentation results than other variants."}
{"id": "842", "content": "Herein, we present a system for hyperspectral image segmentation thatutilizes multiple class--based denoising autoencoders which are efficientlytrained. Moreover, we present a novel hyperspectral data augmentation methodfor labelled HSI data using linear mixtures of pixels from each class, whichhelps the system with edge pixels which are almost always mixed pixels.Finally, we utilize a deep neural network and morphological hole-filling toprovide robust image classification. Results run on the Salinas dataset verifythe high performance of the proposed algorithm. ", "id2": "841", "id3": "None"}
{"id": "843", "content": "This paper presents a method for segmenting iris images obtained from thedeceased subjects, by training a deep convolutional neural network (DCNN)designed for the purpose of semantic segmentation. Post-mortem iris recognitionhas recently emerged as an alternative, or additional, method useful inforensic analysis. At the same time it poses many new challenges from thetechnological standpoint, one of them being the image segmentation stage, whichhas proven difficult to be reliably executed by conventional iris recognitionmethods. Our approach is based on the SegNet architecture, fine-tuned with1,300 manually segmented post-mortem iris images taken from theWarsaw-BioBase-Post-Mortem-Iris v1.0 database. The experiments presented inthis paper show that this data-driven solution is able to learn specificdeformations present in post-mortem samples, which are missing from aliveirises, and offers a considerable improvement over the state-of-the-art,conventional segmentation algorithm (OSIRIS): the Intersection over Union (IoU)metric was improved from 73.6% (for OSIRIS) to 83% (for DCNN-based presented inthis paper) averaged over subject-disjoint, multiple splits of the data intotrain and test subsets. This paper offers the first known to us method ofautomatic processing of post-mortem iris images. We offer source codes with thetrained DCNN that perform end-to-end segmentation of post-mortem iris images,as described in this paper. Also, we offer binary masks corresponding to manualsegmentation of samples from Warsaw-BioBase-Post-Mortem-Iris v1.0 database tofacilitate development of alternative methods for post-mortem irissegmentation. ", "id2": "842", "id3": "None"}
{"id": "844", "content": "Yield and its prediction is one of the most important tasks in grapevinebreeding purposes and vineyard management. Commonly, this trait is estimatedmanually right before harvest by extrapolation, which mostly islabor-intensive, destructive and inaccurate. In the present study an automatedimage-based workflow was developed quantifying inflorescences and singleflowers in unprepared field images of grapevines, i.e. no artificial backgroundor light was applied. It is a novel approach for non-invasive, inexpensive andobjective phenotyping with high-throughput.  First, image regions depicting inflorescences were identified and localized.This was done by segmenting the images into the classes inflorescence andnon-inflorescence using a Fully Convolutional Network (FCN). Efficient imagesegmentation hereby is the most challenging step regarding the small geometryand dense distribution of flowers (several hundred flowers per inflorescence),similar color of all plant organs in the fore- and background as well as thecircumstance that only approximately 5% of an image show inflorescences. Thetrained FCN achieved a mean Intersection Over Union (IOU) of 87.6% on the testdata set. Finally, individual flowers were extracted from theinflorescence-areas using Circular Hough Transform. The flower extractionachieved a recall of 80.3% and a precision of 70.7% using the segmentationderived by the trained FCN model.  Summarized, the presented approach is a promising strategy in order topredict yield potential automatically in the earliest stage of grapevinedevelopment which is applicable for objective monitoring and evaluations ofbreeding material, genetic repositories or commercial vineyards. ", "id2": "843", "id3": "None"}
{"id": "845", "content": "Training deep neural networks on large and sparse datasets is stillchallenging and can require large amounts of computation and memory. In thiswork, we address the task of performing semantic segmentation on largevolumetric data sets, such as CT scans. Our contribution is threefold: 1) Wepropose a boosted sampling scheme that uses a-posterior error maps, generatedthroughout training, to focus sampling on difficult regions, resulting in amore informative loss. This results in a significant training speed up andimproves learning performance for image segmentation. 2) We propose a novelalgorithm for boosting the SGD learning rate schedule by adaptively increasingand lowering the learning rate, avoiding the need for extensive hyperparametertuning. 3) We show that our method is able to attain new state-of-the-artresults on the VISCERAL Anatomy benchmark. ", "id2": "844", "id3": "None"}
{"id": "846", "content": "We develop three efficient approaches for generating visual explanations from3D convolutional neural networks (3D-CNNs) for Alzheimers diseaseclassification. One approach conducts sensitivity analysis on hierarchical 3Dimage segmentation, and the other two visualize network activations on aspatial map. Visual checks and a quantitative localization benchmark indicatethat all approaches identify important brain parts for Alzheimers diseasediagnosis. Comparative analysis show that the sensitivity analysis basedapproach has difficulty handling loosely distributed cerebral cortex, andapproaches based on visualization of activations are constrained by theresolution of the convolutional layer. The complementarity of these methodsimproves the understanding of 3D-CNNs in Alzheimers disease classificationfrom different perspectives. ", "id2": "845", "id3": "None"}
{"id": "847", "content": "Liver cancer is one of the leading causes of cancer death. To assist doctorsin hepatocellular carcinoma diagnosis and treatment planning, an accurate andautomatic liver and tumor segmentation method is highly demanded in theclinical practice. Recently, fully convolutional neural networks (FCNs),including 2D and 3D FCNs, serve as the back-bone in many volumetric imagesegmentation. However, 2D convolutions can not fully leverage the spatialinformation along the third dimension while 3D convolutions suffer from highcomputational cost and GPU memory consumption. To address these issues, wepropose a novel hybrid densely connected UNet (H-DenseUNet), which consists ofa 2D DenseUNet for efficiently extracting intra-slice features and a 3Dcounterpart for hierarchically aggregating volumetric contexts under the spiritof the auto-context algorithm for liver and tumor segmentation. We formulatethe learning process of H-DenseUNet in an end-to-end manner, where theintra-slice representations and inter-slice features can be jointly optimizedthrough a hybrid feature fusion (HFF) layer. We extensively evaluated ourmethod on the dataset of MICCAI 2017 Liver Tumor Segmentation (LiTS) Challengeand 3DIRCADb Dataset. Our method outperformed other state-of-the-arts on thesegmentation results of tumors and achieved very competitive performance forliver segmentation even with a single model. ", "id2": "846", "id3": "None"}
{"id": "848", "content": "Cross modal image syntheses is gaining significant interests for its abilityto estimate target images of a different modality from a given set of sourceimages,like estimating MR to MR, MR to CT, CT to PET etc, without the need foran actual acquisition.Though they show potential for applications in radiationtherapy planning,image super resolution, atlas construction, image segmentationetc.The synthesis results are not as accurate as the actual acquisition.In thispaper,we address the problem of multi modal image synthesis by proposing afully convolutional deep learning architecture called the SynNet.We extend theproposed architecture for various input output configurations. And finally, wepropose a structure preserving custom loss function for cross-modal imagesynthesis.We validate the proposed SynNet and its extended framework on BRATSdataset with comparisons against three state-of-the art methods.And the resultsof the proposed custom loss function is validated against the traditional lossfunction used by the state-of-the-art methods for cross modal image synthesis. ", "id2": "847", "id3": "None"}
{"id": "849", "content": "Deep neural networks (DNNs) have become increasingly important due to theirexcellent empirical performance on a wide range of problems. However,regularization is generally achieved by indirect means, largely due to thecomplex set of functions defined by a network and the difficulty in measuringfunction complexity. There exists no method in the literature for additiveregularization based on a norm of the function, as is classically considered instatistical learning theory. In this work, we propose sampling-basedapproximations to weighted function norms as regularizers for deep neuralnetworks. We provide, to the best of our knowledge, the first proof in theliterature of the NP-hardness of computing function norms of DNNs, motivatingthe necessity of an approximate approach. We then derive a generalization boundfor functions trained with weighted norms and prove that a natural stochasticoptimization strategy minimizes the bound. Finally, we empirically validate theimproved performance of the proposed regularization strategies for both convexfunction sets as well as DNNs on real-world classification and imagesegmentation tasks demonstrating improved performance over weight decay,dropout, and batch normalization. Source code will be released at the time ofpublication. ", "id2": "848", "id3": "None"}
{"id": "850", "content": "One of the time-consuming routine work for a radiologist is to discernanatomical structures from tomographic images. For assisting radiologists, thispaper develops an automatic segmentation method for pelvic magnetic resonance(MR) images. The task has three major challenges 1) A pelvic organ can havevarious sizes and shapes depending on the axial image, which requires localcontexts to segment correctly. 2) Different organs often have quite similarappearance in MR images, which requires global context to segment. 3) Thenumber of available annotated images are very small to use the latestsegmentation algorithms. To address the challenges, we propose a novelconvolutional neural network called Attention-Pyramid network (APNet) thateffectively exploits both local and global contexts, in addition to adata-augmentation technique that is particularly effective for MR images. Inorder to evaluate our method, we construct fine-grained (50 pelvic organs) MRimage segmentation dataset, and experimentally confirm the superior performanceof our techniques over the state-of-the-art image segmentation methods. ", "id2": "849", "id3": "None"}
{"id": "851", "content": "We present a simple solution for segmenting grayscale images using existingConnected Component Labeling (CCL) algorithms (which are generally applied tobinary images), which was efficient enough to be implemented in a constrained(embedded automotive) architecture. Our solution customizes the region growingand merging approach, and is primarily targeted for stereoscopic disparityimages where nearer objects carry more relevance. We provide results from astandard OpenCV implementation for some basic cases and an image from theTsukuba stereo-pair dataset. ", "id2": "850", "id3": "None"}
{"id": "852", "content": "We introduce an approach for image segmentation based on sparsecorrespondences between keypoints in testing and training images. Keypointsrepresent automatically identified distinctive image locations, where eachkeypoint correspondence suggests a transformation between images. We use thesecorrespondences to transfer label maps of entire organs from the trainingimages to the test image. The keypoint transfer algorithm includes three steps:(i) keypoint matching, (ii) voting-based keypoint labeling, and (iii)keypoint-based probabilistic transfer of organ segmentations. We reportsegmentation results for abdominal organs in whole-body CT and MRI, as well asin contrast-enhanced CT and MRI. Our method offers a speed-up of about threeorders of magnitude in comparison to common multi-atlas segmentation, whileachieving an accuracy that compares favorably. Moreover, keypoint transfer doesnot require the registration to an atlas or a training phase. Finally, themethod allows for the segmentation of scans with highly variable field-of-view. ", "id2": "851", "id3": "None"}
{"id": "853", "content": "Automated medical image segmentation, specifically using deep learning, hasshown outstanding performance in semantic segmentation tasks. However, thesemethods rarely quantify their uncertainty, which may lead to errors indownstream analysis. In this work we propose to use Bayesian neural networks toquantify uncertainty within the domain of semantic segmentation. We alsopropose a method to convert voxel-wise segmentation uncertainty into volumetricuncertainty, and calibrate the accuracy and reliability of confidence intervalsof derived measurements. When applied to a tumour volume estimationapplication, we demonstrate that by using such modelling of uncertainty, deeplearning systems can be made to report volume estimates with well-calibratederror-bars, making them safer for clinical use. We also show that theuncertainty estimates extrapolate to unseen data, and that the confidenceintervals are robust in the presence of artificial noise. This could be used toprovide a form of quality control and quality assurance, and may permit furtheradoption of deep learning tools in the clinic. ", "id2": "852", "id3": "None"}
{"id": "854", "content": "Mucous glands lesions analysis and assessing of malignant potential of colonpolyps are very important tasks of surgical pathology. However, differentialdiagnosis of colon polyps often seems impossible by classical methods and it isnecessary to involve computer methods capable of assessing minimal differencesto extend the capabilities of the classical pathology examination. Accuratesegmentation of mucous glands from histology images is a crucial step to obtainreliable morphometric criteria for quantitative diagnostic methods. We reviewmajor trends in histological images segmentation and design a new convolutionalneural network for mucous gland segmentation. ", "id2": "853", "id3": "None"}
{"id2": 1083, "id3": "853", "content": "Mucous glands lesions analysis and assessing of malignant potential of colonpolyps are very important tasks of surgical pathology. However, differentialdiagnosis of colon polyps often seems impossible by classical methods and it isnecessary to involve computer methods capable of assessing minimal differencesto extend the capabilities of the classical pathology examination. Accuratesegmentation of mucous glands from histology images is a crucial step to obtainreliable morphometric criteria for quantitative diagnostic methods. We reviewmajor trends in histological images segmentation and design a new convolutionalneural network for mucous gland segmentation."}
{"id": "855", "content": "Myocardial contrast echocardiography (MCE) is an imaging technique thatassesses left ventricle function and myocardial perfusion for the detection ofcoronary artery diseases. Automatic MCE perfusion quantification is challengingand requires accurate segmentation of the myocardium from noisy andtime-varying images. Random forests (RF) have been successfully applied to manymedical image segmentation tasks. However, the pixel-wise RF classifier ignorescontextual relationships between label outputs of individual pixels. RF whichonly utilizes local appearance features is also susceptible to data sufferingfrom large intensity variations. In this paper, we demonstrate how to overcomethe above limitations of classic RF by presenting a fully automaticsegmentation pipeline for myocardial segmentation in full-cycle 2D MCE data.Specifically, a statistical shape model is used to provide shape priorinformation that guide the RF segmentation in two ways. First, a novel shapemodel (SM) feature is incorporated into the RF framework to generate a moreaccurate RF probability map. Second, the shape model is fitted to the RFprobability map to refine and constrain the final segmentation to plausiblemyocardial shapes. We further improve the performance by introducing a boundingbox detection algorithm as a preprocessing step in the segmentation pipeline.Our approach on 2D image is further extended to 2D+t sequence which ensurestemporal consistency in the resultant sequence segmentations. When evaluated onclinical MCE data, our proposed method achieves notable improvement insegmentation accuracy and outperforms other state-of-the-art methods includingthe classic RF and its variants, active shape model and image registration. ", "id2": "854", "id3": "None"}
{"id": "856", "content": "Convolutional networks (ConvNets) have achieved great successes in variouschallenging vision tasks. However, the performance of ConvNets would degradewhen encountering the domain shift. The domain adaptation is more significantwhile challenging in the field of biomedical image analysis, wherecross-modality data have largely different distributions. Given that annotatingthe medical data is especially expensive, the supervised transfer learningapproaches are not quite optimal. In this paper, we propose an unsuperviseddomain adaptation framework with adversarial learning for cross-modalitybiomedical image segmentations. Specifically, our model is based on a dilatedfully convolutional network for pixel-wise prediction. Moreover, we build aplug-and-play domain adaptation module (DAM) to map the target input tofeatures which are aligned with source domain feature space. A domain criticmodule (DCM) is set up for discriminating the feature space of both domains. Weoptimize the DAM and DCM via an adversarial loss without using any targetdomain label. Our proposed method is validated by adapting a ConvNet trainedwith MRI images to unpaired CT data for cardiac structures segmentations, andachieved very promising results. ", "id2": "855", "id3": "None"}
{"id": "857", "content": "Learning inter-domain mappings from unpaired data can improve performance instructured prediction tasks, such as image segmentation, by reducing the needfor paired data. CycleGAN was recently proposed for this problem, butcritically assumes the underlying inter-domain mapping is approximatelydeterministic and one-to-one. This assumption renders the model ineffective fortasks requiring flexible, many-to-many mappings. We propose a new model, calledAugmented CycleGAN, which learns many-to-many mappings between domains. Weexamine Augmented CycleGAN qualitatively and quantitatively on several imagedatasets. ", "id2": "856", "id3": "None"}
{"id": "858", "content": "Semantic image segmentation plays an important role in modelingpatient-specific anatomy. We propose a convolution neural network, calledKid-Net, along with a training schema to segment kidney vessels: artery, veinand collecting system. Such segmentation is vital during the surgical planningphase in which medical decisions are made before surgical incision. Our maincontribution is developing a training schema that handles unbalanced data,reduces false positives and enables high-resolution segmentation with a limitedmemory budget. These objectives are attained using dynamic weighting, randomsampling and 3D patch segmentation. Manual medical image annotation is bothtime-consuming and expensive. Kid-Net reduces kidney vessels segmentation timefrom matter of hours to minutes. It is trained end-to-end using 3D patches fromvolumetric CT-images. A complete segmentation for a 512x512x512 CT-volume isobtained within a few minutes (1-2 mins) by stitching the output 3D patchestogether. Feature down-sampling and up-sampling are utilized to achieve higherclassification and localization accuracies. Quantitative and qualitativeevaluation results on a challenging testing dataset show Kid-Net competence. ", "id2": "857", "id3": "None"}
{"id2": 1084, "id3": "857", "content": "Semantic image segmentation plays an important role in modelingpatient-specific anatomy. We propose a convolution neural network, calledKid-Net, along with a training schema to segment kidney vessels: artery, veinand collecting system. Such segmentation is vital during the surgical planningphase in which medical decisions are made before surgical incision. Our maincontribution is developing a training schema that handles unbalanced data,reduces false positives and enables high-resolution segmentation with a limitedmemory budget. These objectives are attained using dynamic weighting, randomsampling and 3D patch segmentation. Manual medical image annotation is bothtime-consuming and expensive. Kid-Net reduces kidney vessels segmentation timefrom matter of hours to minutes. It is trained end-to-end using 3D patches fromvolumetric CT-images. A complete segmentation for a 512x512x512 CT-volume isobtained within a few minutes (1-2 mins) by stitching the output 3D patchestogether. Feature down-sampling and up-sampling are utilized to achieve higherclassification and localization accuracies. Quantitative and qualitativeevaluation results on a challenging testing dataset show Kid-Net competence."}
{"id": "859", "content": "Brain extraction is a fundamental step for most brain imaging studies. Inthis paper, we investigate the problem of skull stripping and proposecomplementary segmentation networks (CompNets) to accurately extract the brainfrom T1-weighted MRI scans, for both normal and pathological brain images. Theproposed networks are designed in the framework of encoder-decoder networks andhave two pathways to learn features from both the brain tissue and itscomplementary part located outside of the brain. The complementary pathwayextracts the features in the non-brain region and leads to a robust solution tobrain extraction from MRIs with pathologies, which do not exist in our trainingdataset. We demonstrate the effectiveness of our networks by evaluating them onthe OASIS dataset, resulting in the state of the art performance under thetwo-fold cross-validation setting. Moreover, the robustness of our networks isverified by testing on images with introduced pathologies and by showing itsinvariance to unseen brain pathologies. In addition, our complementary networkdesign is general and can be extended to address other image segmentationproblems with better generalization. ", "id2": "858", "id3": "None"}
{"id2": 1085, "id3": "858", "content": "Brain extraction is a fundamental step for most brain imaging studies. Inthis paper, we investigate the problem of skull stripping and proposecomplementary segmentation networks (CompNets) to accurately extract the brainfrom T1-weighted MRI scans, for both normal and pathological brain images. Theproposed networks are designed in the framework of encoder-decoder networks andhave two pathways to learn features from both the brain tissue and itscomplementary part located outside of the brain. The complementary pathwayextracts the features in the non-brain region and leads to a robust solution tobrain extraction from MRIs with pathologies, which do not exist in our trainingdataset. We demonstrate the effectiveness of our networks by evaluating them onthe OASIS dataset, resulting in the state of the art performance under thetwo-fold cross-validation setting. Moreover, the robustness of our networks isverified by testing on images with introduced pathologies and by showing itsinvariance to unseen brain pathologies. In addition, our complementary networkdesign is general and can be extended to address other image segmentationproblems with better generalization."}
{"id": "860", "content": "Recent advances in deep learning based image segmentation methods haveenabled real-time performance with human-level accuracy. However, occasionallyeven the best method fails due to low image quality, artifacts or unexpectedbehaviour of black box algorithms. Being able to predict segmentation qualityin the absence of ground truth is of paramount importance in clinical practice,but also in large-scale studies to avoid the inclusion of invalid data insubsequent analysis.  In this work, we propose two approaches of real-time automated qualitycontrol for cardiovascular MR segmentations using deep learning. First, wetrain a neural network on 12,880 samples to predict Dice SimilarityCoefficients (DSC) on a per-case basis. We report a mean average error (MAE) of0.03 on 1,610 test samples and 97% binary classification accuracy forseparating low and high quality segmentations. Secondly, in the scenario whereno manually annotated data is available, we train a network to predict DSCscores from estimated quality obtained via a reverse testing strategy. Wereport an MAE=0.14 and 91% binary classification accuracy for this case.Predictions are obtained in real-time which, when combined with real-timesegmentation methods, enables instant feedback on whether an acquired scan isanalysable while the patient is still in the scanner. This further enables newapplications of optimising image acquisition towards best possible analysisresults. ", "id2": "859", "id3": "None"}
{"id": "861", "content": "Automatic parsing of anatomical objects in X-ray images is critical to manyclinical applications in particular towards image-guided invention and workflowautomation. Existing deep network models require a large amount of labeleddata. However, obtaining accurate pixel-wise labeling in X-ray images reliesheavily on skilled clinicians due to the large overlaps of anatomy and thecomplex texture patterns. On the other hand, organs in 3D CT scans preserveclearer structures as well as sharper boundaries and thus can be easilydelineated. In this paper, we propose a novel model framework for learningautomatic X-ray image parsing from labeled CT scans. Specifically, a DenseImage-to-Image network (DI2I) for multi-organ segmentation is first trained onX-ray like Digitally Reconstructed Radiographs (DRRs) rendered from 3D CTvolumes. Then we introduce a Task Driven Generative Adversarial Network(TD-GAN) architecture to achieve simultaneous style transfer and parsing forunseen real X-ray images. TD-GAN consists of a modified cycle-GAN substructurefor pixel-to-pixel translation between DRRs and X-ray images and an addedmodule leveraging the pre-trained DI2I to enforce segmentation consistency. TheTD-GAN framework is general and can be easily adapted to other learning tasks.In the numerical experiments, we validate the proposed model on 815 DRRs and153 topograms. While the vanilla DI2I without any adaptation fails completelyon segmenting the topograms, the proposed model does not require any topogramlabels and is able to provide a promising average dice of 85% which achievesthe same level accuracy of supervised training (88%). ", "id2": "860", "id3": "None"}
{"id": "862", "content": "NeuroNet is a deep convolutional neural network mimicking multiple popularand state-of-the-art brain segmentation tools including FSL, SPM, and MALPEM.The network is trained on 5,000 T1-weighted brain MRI scans from the UK BiobankImaging Study that have been automatically segmented into brain tissue andcortical and sub-cortical structures using the standard neuroimaging pipelines.Training a single model from these complementary and partially overlappinglabel maps yields a new powerful all-in-one, multi-output segmentation tool.The processing time for a single subject is reduced by an order of magnitudecompared to running each individual software package. We demonstrate very goodreproducibility of the original outputs while increasing robustness tovariations in the input data. We believe NeuroNet could be an important tool inlarge-scale population imaging studies and serve as a new standard inneuroscience by reducing the risk of introducing bias when choosing a specificsoftware package. ", "id2": "861", "id3": "None"}
{"id": "863", "content": "This paper proposed a retinal image segmentation method based on conditionalGenerative Adversarial Network (cGAN) to segment optic disc. The proposed modelconsists of two successive networks: generator and discriminator. The generatorlearns to map information from the observing input (i.e., retinal fundus colorimage), to the output (i.e., binary mask). Then, the discriminator learns as aloss function to train this mapping by comparing the ground-truth and thepredicted output with observing the input image as a condition.Experiments wereperformed on two publicly available dataset; DRISHTI GS1 and RIM-ONE. Theproposed model outperformed state-of-the-art-methods by achieving around 0.96%and 0.98% of Jaccard and Dice coefficients, respectively. Moreover, an imagesegmentation is performed in less than a second on recent GPU. ", "id2": "862", "id3": "None"}
{"id": "864", "content": "We investigate the problem of Language-Based Image Editing (LBIE). Given asource image and a natural language description, we want to generate a targetimage by editing the source image based on the description. We propose ageneric modeling framework for two sub-tasks of LBIE: language-based imagesegmentation and image colorization. The framework uses recurrent attentivemodels to fuse image and language features. Instead of using a fixed step size,we introduce for each region of the image a termination gate to dynamicallydetermine after each inference step whether to continue extrapolatingadditional information from the textual description. The effectiveness of theframework is validated on three datasets. First, we introduce a syntheticdataset, called CoSaL, to evaluate the end-to-end performance of our LBIEsystem. Second, we show that the framework leads to state-of-the-artperformance on image segmentation on the ReferIt dataset. Third, we present thefirst language-based colorization result on the Oxford-102 Flowers dataset. ", "id2": "863", "id3": "None"}
{"id": "865", "content": "Fully convolutional neural networks (F-CNNs) have set the state-of-the-art inimage segmentation for a plethora of applications. Architectural innovationswithin F-CNNs have mainly focused on improving spatial encoding or networkconnectivity to aid gradient flow. In this paper, we explore an alternatedirection of recalibrating the feature maps adaptively, to boost meaningfulfeatures, while suppressing weak ones. We draw inspiration from the recentlyproposed squeeze & excitation (SE) module for channel recalibration of featuremaps for image classification. Towards this end, we introduce three variants ofSE modules for image segmentation, (i) squeezing spatially and excitingchannel-wise (cSE), (ii) squeezing channel-wise and exciting spatially (sSE)and (iii) concurrent spatial and channel squeeze & excitation (scSE). Weeffectively incorporate these SE modules within three differentstate-of-the-art F-CNNs (DenseNet, SD-Net, U-Net) and observe consistentimprovement of performance across all architectures, while minimally effectingmodel complexity. Evaluations are performed on two challenging applications:whole brain segmentation on MRI scans (Multi-Atlas Labelling Challenge Dataset)and organ segmentation on whole body contrast enhanced CT scans (VisceralDataset). ", "id2": "864", "id3": "None"}
{"id": "866", "content": "Hourglass networks such as the U-Net and V-Net are popular neuralarchitectures for medical image segmentation and counting problems. Typicalinstances of hourglass networks contain shortcut connections between mirroringlayers. These shortcut connections improve the performance and it ishypothesized that this is due to mitigating effects on the vanishing gradientproblem and the ability of the model to combine feature maps from earlier andlater layers. We propose a method for not only combining feature maps ofmirroring layers but also feature maps of layers with different spatialdimensions. For instance, the method enables the integration of the bottleneckfeature map with those of the reconstruction layers. The proposed approach isapplicable to any hourglass architecture. We evaluated the contextual hourglassnetworks on image segmentation and object counting problems in the medicaldomain. We achieve competitive results outperforming popular hourglass networksby up to 17 percentage points. ", "id2": "865", "id3": "None"}
{"id": "867", "content": "In this work, we evaluate the use of superpixel pooling layers in deepnetwork architectures for semantic segmentation. Superpixel pooling is aflexible and efficient replacement for other pooling strategies thatincorporates spatial prior information. We propose a simple and efficientGPU-implementation of the layer and explore several designs for the integrationof the layer into existing network architectures. We provide experimentalresults on the IBSR and Cityscapes dataset, demonstrating that superpixelpooling can be leveraged to consistently increase network accuracy with minimalcomputational overhead. Source code is available athttps://github.com/bermanmaxim/superpixPool ", "id2": "866", "id3": "None"}
{"id": "868", "content": "Uncertainty estimation methods are expected to improve the understanding andquality of computer-assisted methods used in medical applications (e.g.,neurosurgical interventions, radiotherapy planning), where automated medicalimage segmentation is crucial. In supervised machine learning, a commonpractice to generate ground truth label data is to merge observer annotations.However, as many medical image tasks show a high inter-observer variabilityresulting from factors such as image quality, different levels of userexpertise and domain knowledge, little is known as to how inter-observervariability and commonly used fusion methods affect the estimation ofuncertainty of automated image segmentation. In this paper we analyze theeffect of common image label fusion techniques on uncertainty estimation, andpropose to learn the uncertainty among observers. The results highlight thenegative effect of fusion methods applied in deep learning, to obtain reliableestimates of segmentation uncertainty. Additionally, we show that the learnedobservers uncertainty can be combined with current standard Monte Carlodropout Bayesian neural networks to characterize uncertainty of modelsparameters. ", "id2": "867", "id3": "None"}
{"id2": 1086, "id3": "867", "content": "Uncertainty estimation methods are expected to improve the understanding andquality of computer-assisted methods used in medical applications (e.g.,neurosurgical interventions, radiotherapy planning), where automated medicalimage segmentation is crucial. In supervised machine learning, a commonpractice to generate ground truth label data is to merge observer annotations.However, as many medical image tasks show a high inter-observer variabilityresulting from factors such as image quality, different levels of userexpertise and domain knowledge, little is known as to how inter-observervariability and commonly used fusion methods affect the estimation ofuncertainty of automated image segmentation. In this paper we analyze theeffect of common image label fusion techniques on uncertainty estimation, andpropose to learn the uncertainty among observers. The results highlight thenegative effect of fusion methods applied in deep learning, to obtain reliableestimates of segmentation uncertainty. Additionally, we show that the learnedobservers uncertainty can be combined with current standard Monte Carlodropout Bayesian neural networks to characterize uncertainty of modelsparameters."}
{"id": "869", "content": "Semantic image segmentation is one the most demanding task, especially foranalysis of traffic conditions for self-driving cars. Here the results ofapplication of several deep learning architectures (PSPNet and ICNet) forsemantic image segmentation of traffic stereo-pair images are presented. Theimages from Cityscapes dataset and custom urban images were analyzed as to thesegmentation accuracy and image inference time. For the models pre-trained onCityscapes dataset, the inference time was equal in the limits of standarddeviation, but the segmentation accuracy was different for various cities andstereo channels even. The distributions of accuracy (mean intersection overunion - mIoU) values for each city and channel are asymmetric, long-tailed, andhave many extreme outliers, especially for PSPNet network in comparison toICNet network. Some statistical properties of these distributions (skewness,kurtosis) allow us to distinguish these two networks and open the questionabout relations between architecture of deep learning networks and statisticaldistribution of the predicted results (mIoU here). The results obtaineddemonstrated the different sensitivity of these networks to: (1) the localstreet view peculiarities in different cities that should be taken into accountduring the targeted fine tuning the models before their practical applications,(2) the right and left data channels in stereo-pairs. For both networks, thedifference in the predicted results (mIoU here) for the right and left datachannels in stereo-pairs is out of the limits of statistical error in relationto mIoU values. It means that the traffic stereo pairs can be effectively usednot only for depth calculations (as it is usually used), but also as anadditional data channel that can provide much more information about sceneobjects than simple duplication of the same street view images. ", "id2": "868", "id3": "None"}
{"id": "870", "content": "Recent neural-network-based architectures for image segmentation makeextensive usage of feature forwarding mechanisms to integrate information frommultiple scales. Although yielding good results, even deeper architectures andalternative methods for feature fusion at different resolutions have beenscarcely investigated for medical applications. In this work we propose toimplement segmentation via an encoder-decoder architecture which differs fromany other previously published method since (i) it employs a very deeparchitecture based on residual learning and (ii) combines features via aconvolutional Long Short Term Memory (LSTM), instead of concatenation orsummation. The intuition is that the memory mechanism implemented by LSTMs canbetter integrate features from different scales through a coarse-to-finestrategy; hence the name Coarse-to-Fine Context Memory (CFCM). We demonstratethe remarkable advantages of this approach on two datasets: the Montgomerycounty lung segmentation dataset, and the EndoVis 2015 challenge dataset forsurgical instrument segmentation. ", "id2": "869", "id3": "None"}
{"id": "871", "content": "In recent years, deep learning (DL) methods have become powerful tools forbiomedical image segmentation. However, high annotation efforts and costs arecommonly needed to acquire sufficient biomedical training data for DL models.To alleviate the burden of manual annotation, in this paper, we propose a newweakly supervised DL approach for biomedical image segmentation using boxesonly annotation. First, we develop a method to combine graph search (GS) and DLto generate fine object masks from box annotation, in which DL uses boxannotation to compute a rough segmentation for GS and then GS is applied tolocate the optimal object boundaries. During the mask generation process, wecarefully utilize information from box annotation to filter out potentialerrors, and then use the generated masks to train an accurate DL segmentationnetwork. Extensive experiments on gland segmentation in histology images, lymphnode segmentation in ultrasound images, and fungus segmentation in electronmicroscopy images show that our approach attains superior performance over thebest known state-of-the-art weakly supervised DL method and is able to achieve(1) nearly the same accuracy compared to fully supervised DL methods with farless annotation effort, (2) significantly better results with similarannotation time, and (3) robust performance in various applications. ", "id2": "870", "id3": "None"}
{"id": "872", "content": "Deep learning has been widely accepted as a promising solution for medicalimage segmentation, given a sufficiently large representative dataset of imageswith corresponding annotations. With ever increasing amounts of annotatedmedical datasets, it is infeasible to train a learning method always with alldata from scratch. This is also doomed to hit computational limits, e.g.,memory or runtime feasible for training. Incremental learning can be apotential solution, where new information (images or anatomy) is introducediteratively. Nevertheless, for the preservation of the collective information,it is essential to keep some important (i.e. representative) images andannotations from the past, while adding new information. In this paper, weintroduce a framework for applying incremental learning for segmentation andpropose novel methods for selecting representative data therein. Wecomparatively evaluate our methods in different scenarios using MR images andvalidate the increased learning capacity with using our methods. ", "id2": "871", "id3": "None"}
{"id": "873", "content": "Deep learning (DL) based semantic segmentation methods have been providingstate-of-the-art performance in the last few years. More specifically, thesetechniques have been successfully applied to medical image classification,segmentation, and detection tasks. One deep learning technique, U-Net, hasbecome one of the most popular for these applications. In this paper, wepropose a Recurrent Convolutional Neural Network (RCNN) based on U-Net as wellas a Recurrent Residual Convolutional Neural Network (RRCNN) based on U-Netmodels, which are named RU-Net and R2U-Net respectively. The proposed modelsutilize the power of U-Net, Residual Network, as well as RCNN. There areseveral advantages of these proposed architectures for segmentation tasks.First, a residual unit helps when training deep architecture. Second, featureaccumulation with recurrent residual convolutional layers ensures betterfeature representation for segmentation tasks. Third, it allows us to designbetter U-Net architecture with same number of network parameters with betterperformance for medical image segmentation. The proposed models are tested onthree benchmark datasets such as blood vessel segmentation in retina images,skin cancer segmentation, and lung lesion segmentation. The experimentalresults show superior performance on segmentation tasks compared to equivalentmodels including U-Net and residual U-Net (ResU-Net). ", "id2": "872", "id3": "None"}
{"id2": 1087, "id3": "872", "content": "Deep learning (DL) based semantic segmentation methods have been providingstate-of-the-art performance in the last few years. More specifically, thesetechniques have been successfully applied to medical image classification,segmentation, and detection tasks. One deep learning technique, U-Net, hasbecome one of the most popular for these applications. In this paper, wepropose a Recurrent Convolutional Neural Network (RCNN) based on U-Net as wellas a Recurrent Residual Convolutional Neural Network (RRCNN) based on U-Netmodels, which are named RU-Net and R2U-Net respectively. The proposed modelsutilize the power of U-Net, Residual Network, as well as RCNN. There areseveral advantages of these proposed architectures for segmentation tasks.First, a residual unit helps when training deep architecture. Second, featureaccumulation with recurrent residual convolutional layers ensures betterfeature representation for segmentation tasks. Third, it allows us to designbetter U-Net architecture with same number of network parameters with betterperformance for medical image segmentation. The proposed models are tested onthree benchmark datasets such as blood vessel segmentation in retina images,skin cancer segmentation, and lung lesion segmentation. The experimentalresults show superior performance on segmentation tasks compared to equivalentmodels including U-Net and residual U-Net (ResU-Net)."}
{"id": "874", "content": "This paper addresses the search for a fast and meaningful image segmentationin the context of $k$-means clustering. The proposed method builds on awidely-used local version of Lloyds algorithm, called Simple Linear IterativeClustering (SLIC). We propose an algorithm which extends SLIC to dynamicallyadjust the local search, adopting superpixel resolution dynamically tostructure existent in the image, and thus provides for more meaningfulsuperpixels in the same linear runtime as standard SLIC. The proposed method isevaluated against state-of-the-art techniques and improved boundary adherenceand undersegmentation error are observed, whilst still remaining among thefastest algorithms which are tested. ", "id2": "873", "id3": "None"}
{"id": "875", "content": "We desgin a novel fully convolutional network architecture for shapes,denoted by Shape Fully Convolutional Networks (SFCN). 3D shapes are representedas graph structures in the SFCN architecture, based on novel graph convolutionand pooling operations, which are similar to convolution and pooling operationsused on images. Meanwhile, to build our SFCN architecture in the original imagesegmentation fully convolutional network (FCN) architecture, we also design andimplement a generating operation  with bridging function. This ensures that theconvolution and pooling operation we have designed can be successfully appliedin the original FCN architecture. In this paper, we also present a new shapesegmentation approach based on SFCN. Furthermore, we allow more general andchallenging input, such as mixed datasets of different categories of shapes which can prove the ability of our generalisation. In our approach, SFCNs aretrained triangles-to-triangles by using three low-level geometric features asinput. Finally, the feature voting-based multi-label graph cuts is adopted tooptimise the segmentation results obtained by SFCN prediction. The experimentresults show that our method can effectively learn and predict mixed shapedatasets of either similar or different characteristics, and achieve excellentsegmentation results. ", "id2": "874", "id3": "None"}
{"id": "876", "content": "In this paper, we propose an efficient architecture for semantic imagesegmentation using the depth-to-space (D2S) operation. Our D2S model iscomprised of a standard CNN encoder followed by a depth-to-space reordering ofthe final convolutional feature maps. Our approach eliminates the decoderportion of traditional encoder-decoder segmentation models and reduces theamount of computation almost by half. As a participant of the DeepGlobe RoadExtraction competition, we evaluate our models on the corresponding roadsegmentation dataset. Our highly efficient D2S models exhibit comparableperformance to standard segmentation models with much lower computational cost. ", "id2": "875", "id3": "None"}
{"id": "877", "content": "Many deep learning architectures for semantic segmentation involve a FullyConvolutional Neural Network (FCN) followed by a Conditional Random Field (CRF)to carry out inference over an image. These models typically involve unarypotentials based on local appearance features computed by FCNs, and binarypotentials based on the displacement between pixels. We show that while currentmethods succeed in segmenting whole objects, they perform poorly in situationsinvolving a large number of object parts. We therefore suggest incorporatinginto the inference algorithm additional higher-order potentials inspired by theway humans identify and localize parts. We incorporate two relations that wereshown to be useful to human object identification - containment and attachment- into the energy term of the CRF and evaluate their performance on the PascalVOC Parts dataset. Our experimental results show that the segmentation of fineparts is positively affected by the addition of these two relations, and thatthe segmentation of fine parts can be further influenced by complex structuralfeatures. ", "id2": "876", "id3": "None"}
{"id": "878", "content": "We propose a geometric convexity shape prior preservation method forvariational level set based image segmentation methods. Our method is builtupon the fact that the level set of a convex signed distanced function must beconvex. This property enables us to transfer a complicated geometricalconvexity prior into a simple inequality constraint on the function. An activeset based Gauss-Seidel iteration is used to handle this constrainedminimization problem to get an efficient algorithm. We apply our method toregion and edge based level set segmentation models including Chan-Vese (CV)model with guarantee that the segmented region will be convex. Experimentalresults show the effectiveness and quality of the proposed model and algorithm. ", "id2": "877", "id3": "None"}
{"id": "879", "content": "A variety of deep neural networks have been applied in medical imagesegmentation and achieve good performance. Unlike natural images, medicalimages of the same imaging modality are characterized by the same pattern,which indicates that same normal organs or tissues locate at similar positionsin the images. Thus, in this paper we try to incorporate the prior knowledge ofmedical images into the structure of neural networks such that the priorknowledge can be utilized for accurate segmentation. Based on this idea, wepropose a novel deep network called knowledge-based fully convolutional network(KFCN) for medical image segmentation. The segmentation function andcorresponding error is analyzed. We show the existence of an asymptoticallystable region for KFCN which traditional FCN doesnt possess. Experimentsvalidate our knowledge assumption about the incorporation of prior knowledgeinto the convolution kernels of KFCN and show that KFCN can achieve areasonable segmentation and a satisfactory accuracy. ", "id2": "878", "id3": "None"}
{"id": "880", "content": "We provide initial seedings to the Quick Shift clustering algorithm, whichapproximate the locally high-density regions of the data. Such seedings act asmore stable and expressive cluster-cores than the singleton modes found byQuick Shift. We establish statistical consistency guarantees for thismodification. We then show strong clustering performance on real datasets aswell as promising applications to image segmentation. ", "id2": "879", "id3": "None"}
{"id": "881", "content": "Medical image segmentation requires consensus ground truth segmentations tobe derived from multiple expert annotations. A novel approach is proposed thatobtains consensus segmentations from experts using graph cuts (GC) and semisupervised learning (SSL). Popular approaches use iterative ExpectationMaximization (EM) to estimate the final annotation and quantify annotatorsperformance. Such techniques pose the risk of getting trapped in local minima.We propose a self consistency (SC) score to quantify annotator consistencyusing low level image features. SSL is used to predict missing annotations byconsidering global features and local image consistency. The SC score alsoserves as the penalty cost in a second order Markov random field (MRF) costfunction optimized using graph cuts to derive the final consensus label. Graphcut obtains a global maximum without an iterative procedure. Experimentalresults on synthetic images, real data of Crohns disease patients and retinalimages show our final segmentation to be accurate and more consistent thancompeting methods. ", "id2": "880", "id3": "None"}
{"id": "882", "content": "We propose a novel attention gate (AG) model for medical imaging thatautomatically learns to focus on target structures of varying shapes and sizes.Models trained with AGs implicitly learn to suppress irrelevant regions in aninput image while highlighting salient features useful for a specific task.This enables us to eliminate the necessity of using explicit externaltissue/organ localisation modules of cascaded convolutional neural networks(CNNs). AGs can be easily integrated into standard CNN architectures such asthe U-Net model with minimal computational overhead while increasing the modelsensitivity and prediction accuracy. The proposed Attention U-Net architectureis evaluated on two large CT abdominal datasets for multi-class imagesegmentation. Experimental results show that AGs consistently improve theprediction performance of U-Net across different datasets and training sizeswhile preserving computational efficiency. The code for the proposedarchitecture is publicly available. ", "id2": "881", "id3": "None"}
{"id": "883", "content": "We introduce a new multi-dimensional nonlinear embedding -- Piecewise FlatEmbedding (PFE) -- for image segmentation. Based on the theory of sparse signalrecovery, piecewise flat embedding with diverse channels attempts to recover apiecewise constant image representation with sparse region boundaries andsparse cluster value scattering. The resultant piecewise flat embeddingexhibits interesting properties such as suppressing slowly varying signals, andoffers an image representation with higher region identifiability which isdesirable for image segmentation or high-level semantic analysis tasks. Weformulate our embedding as a variant of the Laplacian Eigenmap embedding withan $L_ 1,p  (0<p leq1)$ regularization term to promote sparse solutions. First,we devise a two-stage numerical algorithm based on Bregman iterations tocompute $L_ 1,1 $-regularized piecewise flat embeddings. We further generalizethis algorithm through iterative reweighting to solve the general$L_ 1,p $-regularized problem. To demonstrate its efficacy, we integrate PFEinto two existing image segmentation frameworks, segmentation based onclustering and hierarchical segmentation based on contour detection.Experiments on four major benchmark datasets, BSDS500, MSRC, StanfordBackground Dataset, and PASCAL Context, show that segmentation algorithmsincorporating our embedding achieve significantly improved results. ", "id2": "882", "id3": "None"}
{"id": "884", "content": "For the challenging semantic image segmentation task the most efficientmodels have traditionally combined the structured modelling capabilities ofConditional Random Fields (CRFs) with the feature extraction power of CNNs. Inmore recent works however, CRF post-processing has fallen out of favour. Weargue that this is mainly due to the slow training and inference speeds ofCRFs, as well as the difficulty of learning the internal CRF parameters. Toovercome both issues we propose to add the assumption of conditionalindependence to the framework of fully-connected CRFs. This allows us toreformulate the inference in terms of convolutions, which can be implementedhighly efficiently on GPUs. Doing so speeds up inference and training by afactor of more then 100. All parameters of the convolutional CRFs can easily beoptimized using backpropagation. To facilitating further CRF research we makeour implementation publicly available. Please visit:https://github.com/MarvinTeichmann/ConvCRF ", "id2": "883", "id3": "None"}
{"id": "885", "content": "Urban facade segmentation from automatically acquired imagery, in contrast totraditional image segmentation, poses several unique challenges. 360-degreephotospheres captured from vehicles are an effective way to capture a largenumber of images, but this data presents difficult-to-model warping andstitching artifacts. In addition, each pixel can belong to multiple facadeelements, and different facade elements (e.g., window, balcony, sill, etc.) arecorrelated and vary wildly in their characteristics. In this paper, we proposethree network architectures of varying complexity to achieve multilabelsemantic segmentation of facade images while exploiting their uniquecharacteristics. Specifically, we propose a MULTIFACSEGNET architecture toassign multiple labels to each pixel, a SEPARABLE architecture as a low-rankformulation that encourages extraction of rectangular elements, and aCOMPATIBILITY network that simultaneously seeks segmentation across facadeelement types allowing the network to see intermediate output probabilitiesof the various facade element classes. Our results on benchmark datasets showsignificant improvements over existing facade segmentation approaches for thetypical facade elements. For example, on one commonly used dataset, theaccuracy scores for window(the most important architectural element) increasesfrom 0.91 to 0.97 percent compared to the best competing method, and comparableimprovements on other element types. ", "id2": "884", "id3": "None"}
{"id": "886", "content": "Due to low tissue contrast, irregular object appearance, and unpredictablelocation variation, segmenting the objects from different medical imagingmodalities (e.g., CT, MR) is considered as an important yet challenging task.In this paper, we present a novel method for interactive medical imagesegmentation with the following merits. (1) Our design is fundamentallydifferent from previous pure patch-based and image-based segmentation methods.We observe that during delineation, the physician repeatedly check theinside-outside intensity changing to determine the boundary, which indicatesthat comparison in an inside-outside manner is extremely important. Thus, weinnovatively model our segmentation task as learning the representation of thebi-directional sequential patches, starting from (or ending in) the givencentral point of the object. This can be realized by our proposed ConvRNNnetwork embedded with a gated memory propagation unit. (2) Unlike previousinteractive methods (requiring bounding box or seed points), we only ask thephysician to merely click on the rough central point of the object beforesegmentation, which could simultaneously enhance the performance and reduce thesegmentation time. (3) We utilize our method in a multi-level framework forbetter performance. We systematically evaluate our method in three differentsegmentation tasks including CT kidney tumor, MR prostate, and PROMISE12challenge, showing promising results compared with state-of-the-art methods.The code is available here: href https://github.com/sunalbert/Sequential-patch-based-segmentation  Sequential-patch-based-segmentation . ", "id2": "885", "id3": "None"}
{"id": "887", "content": "State-of-the-art semantic segmentation approaches increase the receptivefield of their models by using either a downsampling path composed ofpoolings/strided convolutions or successive dilated convolutions. However, itis not clear which operation leads to best results. In this paper, wesystematically study the differences introduced by distinct receptive fieldenlargement methods and their impact on the performance of a novelarchitecture, called Fully Convolutional DenseResNet (FC-DRN). FC-DRN has adensely connected backbone composed of residual networks. Following standardimage segmentation architectures, receptive field enlargement operations thatchange the representation level are interleaved among residual networks. Thisallows the model to exploit the benefits of both residual and denseconnectivity patterns, namely: gradient flow, iterative refinement ofrepresentations, multi-scale feature combination and deep supervision. In orderto highlight the potential of our model, we test it on the challenging CamVidurban scene understanding benchmark and make the following observations: 1)downsampling operations outperform dilations when the model is trained fromscratch, 2) dilations are useful during the finetuning step of the model, 3)coarser representations require less refinement steps, and 4) ResNets (by modelconstruction) are good regularizers, since they can reduce the model capacitywhen needed. Finally, we compare our architecture to alternative methods andreport state-of-the-art result on the Camvid dataset, with at least twice fewerparameters. ", "id2": "886", "id3": "None"}
{"id": "888", "content": "Machine learning models produce state-of-the-art results in many MRI imagessegmentation. However, most of these models are trained on very large datasetswhich come from experts manual labeling. This labeling process is very timeconsuming and costs experts work. Therefore finding a way to reduce this costis on high demand. In this paper, we propose a segmentation method whichexploits MRI images sequential structure to nearly drop out this labeling task.Only the first slice needs to be manually labeled to train the model which theninfers the next slices segmentation. Inference result is another datum used totrain the model again. The updated model then infers the third slice and thesame process is carried out until the last slice. The proposed model is ancombination of two Random Forest algorithms: the classical one and a recent onenamely Mondrian Forests. We applied our method on human left ventriclesegmentation and results are very promising. This method can also be used togenerate labels. ", "id2": "887", "id3": "None"}
{"id": "889", "content": "We present a semi-parametric approach to photographic image synthesis fromsemantic layouts. The approach combines the complementary strengths ofparametric and nonparametric techniques. The nonparametric component is amemory bank of image segments constructed from a training set of images. Givena novel semantic layout at test time, the memory bank is used to retrievephotographic references that are provided as source material to a deep network.The synthesis is performed by a deep network that draws on the providedphotographic material. Experiments on multiple semantic segmentation datasetsshow that the presented approach yields considerably more realistic images thanrecent purely parametric techniques. The results are shown in the supplementaryvideo at https://youtu.be/U4Q98lenGLQ ", "id2": "888", "id3": "None"}
{"id": "890", "content": "This paper introduces a novel algorithm for transductive inference inhigher-order MRFs, where the unary energies are parameterized by a variableclassifier. The considered task is posed as a joint optimization problem in thecontinuous classifier parameters and the discrete label variables. In contrastto prior approaches such as convex relaxations, we propose an advantageousdecoupling of the objective function into discrete and continuous subproblemsand a novel, efficient optimization method related to ADMM. This approachpreserves integrality of the discrete label variables and guarantees globalconvergence to a critical point. We demonstrate the advantages of our approachin several experiments including video object segmentation on the DAVIS dataset and interactive image segmentation. ", "id2": "889", "id3": "None"}
{"id": "891", "content": "Many imaging tasks require global information about all pixels in an image.Conventional bottom-up classification networks globalize information bydecreasing resolution; features are pooled and downsampled into a singleoutput. But for semantic segmentation and object detection tasks, a networkmust provide higher-resolution pixel-level outputs. To globalize informationwhile preserving resolution, many researchers propose the inclusion ofsophisticated auxiliary blocks, but these come at the cost of a considerableincrease in network size and computational cost. This paper proposes stackedu-nets (SUNets), which iteratively combine features from different resolutionscales while maintaining resolution. SUNets leverage the informationglobalization power of u-nets in a deeper network architectures that is capableof handling the complexity of natural images. SUNets perform extremely well onsemantic segmentation tasks using a small number of parameters. ", "id2": "890", "id3": "None"}
{"id": "892", "content": "In the fields of nanoscience and nanotechnology, it is important to be ableto functionalize surfaces chemically for a wide variety of applications.Scanning tunneling microscopes (STMs) are important instruments in this areaused to measure the surface structure and chemistry with better than molecularresolution. Self-assembly is frequently used to create monolayers that redefinethe surface chemistry in just a single-molecule-thick layer. Indeed, STM imagesreveal rich information about the structure of self-assembled monolayers sincethey convey chemical and physical properties of the studied material.  In order to assist in and to enhance the analysis of STM and other images, wepropose and demonstrate an image-processing framework that produces two imagesegmentations: one is based on intensities (apparent heights in STM images) andthe other is based on textural patterns. The proposed framework begins with acartoon+texture decomposition, which separates an image into its cartoon andtexture components. Afterward, the cartoon image is segmented by a modifiedmultiphase version of the local Chan-Vese model, while the texture image issegmented by a combination of 2D empirical wavelet transform and a clusteringalgorithm. Overall, our proposed framework contains several new features,specifically in presenting a new application of cartoon+texture decompositionand of the empirical wavelet transforms and in developing a specializedframework to segment STM images and other data. To demonstrate the potential ofour approach, we apply it to actual STM images of cyanide monolayers onAu  111   and present their corresponding segmentation results. ", "id2": "891", "id3": "None"}
{"id": "893", "content": "America has a massive railway system. As of 2006, U.S. freight railroads have140,490 route- miles of standard gauge, but maintaining such a huge system andeliminating any dangers, like reduced track stability and poor drainage, causedby railway ballast degradation require huge amount of labor. The traditionalway to quantify the degradation of ballast is to use an index called FoulingIndex (FI) through ballast sampling and sieve analysis. However, determiningthe FI values in lab is very time-consuming and laborious, but with the help ofrecent development in the field of computer vision, a novel method for apotential machine-vison based ballast inspection system can be employed thatcan hopefully replace the traditional mechanical method. The new machine-visionapproach analyses the images of the in-service ballasts, and then utilizesimage segmentation algorithm to get ballast segments. By comparing the segmentresults and their corresponding FI values, this novel method produces amachine-vision-based index that has the best-fit relation with FI. Theimplementation details of how this algorithm works are discussed in thisreport. ", "id2": "892", "id3": "None"}
{"id": "894", "content": "Image segmentation needs both local boundary position information and globalobject context information. The performance of the recent state-of-the-artmethod, fully convolutional networks, reaches a bottleneck due to the neuralnetwork limit after balancing between the two types of informationsimultaneously in an end-to-end training style. To overcome this problem, wedivide the semantic image segmentation into temporal subtasks. First, we find apossible pixel position of some object boundary; then trace the boundary atsteps within a limited length until the whole object is outlined. We presentthe first deep reinforcement learning approach to semantic image segmentation,called DeepOutline, which outperforms other algorithms in Coco detectionleaderboard in the middle and large size person category in Coco val2017dataset. Meanwhile, it provides an insight into a divide and conquer way byreinforcement learning on computer vision problems. ", "id2": "893", "id3": "None"}
{"id": "895", "content": "We introduce the concept of derivate-based component-trees for images with anarbitrary number of channels. The approach is a natural extension of theclassical component-tree devoted to gray-scale images. The similar structureenables the translation of many gray-level image processing techniques based onthe component-tree to hyperspectral and color images. As an exampleapplication, we present an image segmentation approach that extracts MaximallyStable Homogeneous Regions (MSHR). The approach very similar to MSER but can beapplied to images with an arbitrary number of channels. As opposed to MSER, ourapproach implicitly segments regions with are both lighter and darker thantheir background for gray-scale images and can be used in OCR applicationswhere MSER will fail. We introduce a local flooding-based immersion for thederivate-based component-tree construction which is linear in the number ofpixels. In the experiments, we show that the runtime scales favorably with anincreasing number of channels and may improve algorithms which build on MSER. ", "id2": "894", "id3": "None"}
{"id": "896", "content": "We present a multilinear statistical model of the human tongue that capturesanatomical and tongue pose related shape variations separately. The model isderived from 3D magnetic resonance imaging data of 11 speakers sustainingspeech related vocal tract configurations. The extraction is performed by usinga minimally supervised method that uses as basis an image segmentation approachand a template fitting technique. Furthermore, it uses image denoising to dealwith possibly corrupt data, palate surface information reconstruction to handlepalatal tongue contacts, and a bootstrap strategy to refine the obtainedshapes. Our evaluation concludes that limiting the degrees of freedom for theanatomical and speech related variations to 5 and 4, respectively, produces amodel that can reliably register unknown data while avoiding overfittingeffects. Furthermore, we show that it can be used to generate a plausibletongue animation by tracking sparse motion capture data. ", "id2": "895", "id3": "None"}
{"id2": 1088, "id3": "895", "content": "We present a multilinear statistical model of the human tongue that capturesanatomical and tongue pose related shape variations separately. The model isderived from 3D magnetic resonance imaging data of 11 speakers sustainingspeech related vocal tract configurations. The extraction is performed by usinga minimally supervised method that uses as basis an image segmentation approachand a template fitting technique. Furthermore, it uses image denoising to dealwith possibly corrupt data, palate surface information reconstruction to handlepalatal tongue contacts, and a bootstrap strategy to refine the obtainedshapes. Our evaluation concludes that limiting the degrees of freedom for theanatomical and speech related variations to 5 and 4, respectively, produces amodel that can reliably register unknown data while avoiding overfittingeffects. Furthermore, we show that it can be used to generate a plausibletongue animation by tracking sparse motion capture data."}
{"id": "897", "content": "We propose a novel locally adaptive learning estimator for enhancing theinter- and intra- discriminative capabilities of Deep Neural Networks, whichcan be used as improved loss layer for semantic image segmentation tasks. Mostloss layers compute pixel-wise cost between feature maps and ground truths,ignoring spatial layouts and interactions between neighboring pixels with sameobject category, and thus networks cannot be effectively sensitive tointra-class connections. Stride by stride, our method firstly conducts adaptivepooling filter operating over predicted feature maps, aiming to merge predicteddistributions over a small group of neighboring pixels with same category, andthen it computes cost between the merged distribution vector and their categorylabel. Such design can make groups of neighboring predictions from samecategory involved into estimations on predicting correctness with respect totheir category, and hence train networks to be more sensitive to regionalconnections between adjacent pixels based on their categories. In theexperiments on Pascal VOC 2012 segmentation datasets, the consistently improvedresults show that our proposed approach achieves better segmentation masksagainst previous counterparts. ", "id2": "896", "id3": "None"}
{"id": "898", "content": "Semantic segmentation is an established while rapidly evolving field inmedical imaging. In this paper we focus on the segmentation of brain MagneticResonance Images (MRI) into cerebral structures using convolutional neuralnetworks (CNN). CNNs achieve good performance by finding effective highdimensional image features describing the patch content only. In this work, wepropose different ways to introduce spatial constraints into the network tofurther reduce prediction inconsistencies.  A patch based CNN architecture was trained, making use of multiple scales togather contextual information. Spatial constraints were introduced within theCNN through a distance to landmarks feature or through the integration of aprobability atlas. We demonstrate experimentally that using spatial informationhelps to reduce segmentation inconsistencies. ", "id2": "897", "id3": "None"}
{"id": "899", "content": "This paper presents a novel unsupervised segmentation method for 3D medicalimages. Convolutional neural networks (CNNs) have brought significant advancesin image segmentation. However, most of the recent methods rely on supervisedlearning, which requires large amounts of manually annotated data. Thus, it ischallenging for these methods to cope with the growing amount of medicalimages. This paper proposes a unified approach to unsupervised deeprepresentation learning and clustering for segmentation. Our proposed methodconsists of two phases. In the first phase, we learn deep featurerepresentations of training patches from a target image using jointunsupervised learning (JULE) that alternately clusters representationsgenerated by a CNN and updates the CNN parameters using cluster labels assupervisory signals. We extend JULE to 3D medical images by utilizing 3Dconvolutions throughout the CNN architecture. In the second phase, we applyk-means to the deep representations from the trained CNN and then projectcluster labels to the target image in order to obtain the fully segmentedimage. We evaluated our methods on three images of lung cancer specimensscanned with micro-computed tomography (micro-CT). The automatic segmentationof pathological regions in micro-CT could further contribute to thepathological examination process. Hence, we aim to automatically divide eachimage into the regions of invasive carcinoma, noninvasive carcinoma, and normaltissue. Our experiments show the potential abilities of unsupervised deeprepresentation learning for medical image segmentation. ", "id2": "898", "id3": "None"}
{"id": "900", "content": "This paper presents a novel method for unsupervised segmentation of pathologyimages. Staging of lung cancer is a major factor of prognosis. Measuring themaximum dimensions of the invasive component in a pathology images is anessential task. Therefore, image segmentation methods for visualizing theextent of invasive and noninvasive components on pathology images could supportpathological examination. However, it is challenging for most of the recentsegmentation methods that rely on supervised learning to cope with unlabeledpathology images. In this paper, we propose a unified approach to unsupervisedrepresentation learning and clustering for pathology image segmentation. Ourmethod consists of two phases. In the first phase, we learn featurerepresentations of training patches from a target image using the sphericalk-means. The purpose of this phase is to obtain cluster centroids which couldbe used as filters for feature extraction. In the second phase, we applyconventional k-means to the representations extracted by the centroids and thenproject cluster labels to the target images. We evaluated our methods onpathology images of lung cancer specimen. Our experiments showed that theproposed method outperforms traditional k-means segmentation and themultithreshold Otsu method both quantitatively and qualitatively with animproved normalized mutual information (NMI) score of 0.626 compared to 0.168and 0.167, respectively. Furthermore, we found that the centroids can beapplied to the segmentation of other slices from the same sample. ", "id2": "899", "id3": "None"}
{"id2": 1089, "id3": "899", "content": "This paper presents a novel method for unsupervised segmentation of pathologyimages. Staging of lung cancer is a major factor of prognosis. Measuring themaximum dimensions of the invasive component in a pathology images is anessential task. Therefore, image segmentation methods for visualizing theextent of invasive and noninvasive components on pathology images could supportpathological examination. However, it is challenging for most of the recentsegmentation methods that rely on supervised learning to cope with unlabeledpathology images. In this paper, we propose a unified approach to unsupervisedrepresentation learning and clustering for pathology image segmentation. Ourmethod consists of two phases. In the first phase, we learn featurerepresentations of training patches from a target image using the sphericalk-means. The purpose of this phase is to obtain cluster centroids which couldbe used as filters for feature extraction. In the second phase, we applyconventional k-means to the representations extracted by the centroids and thenproject cluster labels to the target images. We evaluated our methods onpathology images of lung cancer specimen. Our experiments showed that theproposed method outperforms traditional k-means segmentation and themultithreshold Otsu method both quantitatively and qualitatively with animproved normalized mutual information (NMI) score of 0.626 compared to 0.168and 0.167, respectively. Furthermore, we found that the centroids can beapplied to the segmentation of other slices from the same sample."}
{"id": "901", "content": "Breast cancer is the most frequently diagnosed cancer and leading cause ofcancer-related death among females worldwide. In this article, we investigatethe applicability of densely connected convolutional neural networks to theproblems of histology image classification and whole slide image segmentationin the area of computer-aided diagnoses for breast cancer. To this end, westudy various approaches for transfer learning and apply them to the data setfrom the 2018 grand challenge on breast cancer histology images (BACH). ", "id2": "900", "id3": "None"}
{"id": "902", "content": "Compared to the general semantic segmentation problem, portrait segmentationhas higher precision requirement on boundary area. However, this problem hasnot been well studied in previous works. In this paper, we propose aboundary-sensitive deep neural network (BSN) for portrait segmentation. BSNintroduces three novel techniques. First, an individual boundary-sensitivekernel is proposed by dilating the contour line and assigning the boundarypixels with multi-class labels. Second, a global boundary-sensitive kernel isemployed as a position sensitive prior to further constrain the overall shapeof the segmentation map. Third, we train a boundary-sensitive attributeclassifier jointly with the segmentation network to reinforce the network withsemantic boundary shape information. We have evaluated BSN on the currentlargest public portrait segmentation dataset, i.e, the PFCN dataset, as well asthe portrait images collected from other three popular image segmentationdatasets: COCO, COCO-Stuff, and PASCAL VOC. Our method achieves the superiorquantitative and qualitative performance over state-of-the-arts on all thedatasets, especially on the boundary area. ", "id2": "901", "id3": "None"}
{"id": "903", "content": "The Jaccard index, also referred to as the intersection-over-union score, iscommonly employed in the evaluation of image segmentation results given itsperceptual qualities, scale invariance - which lends appropriate relevance tosmall objects, and appropriate counting of false negatives, in comparison toper-pixel losses. We present a method for direct optimization of the meanintersection-over-union loss in neural networks, in the context of semanticimage segmentation, based on the convex Lov asz extension of submodularlosses. The loss is shown to perform better with respect to the Jaccard indexmeasure than the traditionally used cross-entropy loss. We show quantitativeand qualitative differences between optimizing the Jaccard index per imageversus optimizing the Jaccard index taken over an entire dataset. We evaluatethe impact of our method in a semantic segmentation pipeline and showsubstantially improved intersection-over-union segmentation scores on thePascal VOC and Cityscapes datasets using state-of-the-art deep learningsegmentation architectures. ", "id2": "902", "id3": "None"}
{"id": "904", "content": "In this paper, we present a new image segmentation method based on theconcept of sparse subset selection. Starting with an over-segmentation, weadopt local spectral histogram features to encode the visual information of thesmall segments into high-dimensional vectors, called superpixel features. Then,the superpixel features are fed into a novel convex model which efficientlyleverages the features to group the superpixels into a proper number ofcoherent regions. Our model automatically determines the optimal number ofcoherent regions and superpixels assignment to shape final segments. To solveour model, we propose a numerical algorithm based on the alternating directionmethod of multipliers (ADMM), whose iterations consist of two highlyparallelizable sub-problems. We show each sub-problem enjoys closed-formsolution which makes the ADMM iterations computationally very efficient.Extensive experiments on benchmark image segmentation datasets demonstrate thatour proposed method in combination with an over-segmentation can provide highquality and competitive results compared to the existing state-of-the-artmethods. ", "id2": "903", "id3": "None"}
{"id": "905", "content": "Deep convolutional neural networks (CNNs), especially fully convolutionalnetworks, have been widely applied to automatic medical image segmentationproblems, e.g., multi-organ segmentation. Existing CNN-based segmentationmethods mainly focus on looking for increasingly powerful networkarchitectures, but pay less attention to data sampling strategies for trainingnetworks more effectively. In this paper, we present a simple but effectivesample selection method for training multi-organ segmentation networks. Sampleselection exhibits an exploitation-exploration strategy, i.e., exploiting hardsamples and exploring less frequently visited samples. Based on the fact thatvery hard samples might have annotation errors, we propose a new sampleselection policy, named Relaxed Upper Confident Bound (RUCB). Compared withother sample selection policies, e.g., Upper Confident Bound (UCB), it exploitsa range of hard samples rather than being stuck with a small set of very hardones, which mitigates the influence of annotation errors during training. Weapply this new sample selection policy to training a multi-organ segmentationnetwork on a dataset containing 120 abdominal CT scans and show that it boostssegmentation performance significantly. ", "id2": "904", "id3": "None"}
{"id": "906", "content": "Examining locomotion has improved our basic understanding of motor controland aided in treating motor impairment. Mice and rats are the model system ofchoice for basic neuroscience studies of human disease. High frame rates areneeded to quantify the kinematics of running rodents, due to their high stridefrequency. Manual tracking, especially for multiple body landmarks, becomesextremely time-consuming. To overcome these limitations, we proposed the use ofsuperpixels based image segmentation as superpixels utilized both spatial andcolor information for segmentation. We segmented some parts of body and testedthe success of segmentation as a function of color space and SLIC segment size.We used a simple merging function to connect the segmented regions consideredas neighbor and having the same intensity value range. In addition, 28 featureswere extracted, and t-SNE was used to demonstrate how much the methods arecapable to differentiate the regions. Finally, we compared the segmentedregions to a manually outlined region. The results showed for segmentation,using the RGB image was slightly better compared to the hue channel. For merg-ing and classification, however, the hue representation was better as itcaptures the relevant color information in a single channel. ", "id2": "905", "id3": "None"}
{"id": "907", "content": "Image foreground extraction is a classical problem in image processing andvision, with a large range of applications. In this dissertation, we focus onthe extraction of text and graphics in mixed-content images, and design novelapproaches for various aspects of this problem.  We first propose a sparse decomposition framework, which models thebackground by a subspace containing smooth basis vectors, and foreground as asparse and connected component. We then formulate an optimization framework tosolve this problem, by adding suitable regularizations to the cost function topromote the desired characteristics of each component. We present twotechniques to solve the proposed optimization problem, one based on alternatingdirection method of multipliers (ADMM), and the other one based on robustregression. Promising results are obtained for screen content imagesegmentation using the proposed algorithm.  We then propose a robust subspace learning algorithm for the representationof the background component using training images that could contain bothbackground and foreground components, as well as noise. With the learntsubspace for the background, we can further improve the segmentation results,compared to using a fixed subspace. Lastly, we investigate a different class ofsignal/image decomposition problem, where only one signal component is activeat each signal element. In this case, besides estimating each component, weneed to find their supports, which can be specified by a binary mask. Wepropose a mixed-integer programming problem, that jointly estimates the twocomponents and their supports through an alternating optimization scheme. Weshow the application of this algorithm on various problems, including imagesegmentation, video motion segmentation, and also separation of text fromtextured images. ", "id2": "906", "id3": "None"}
{"id": "908", "content": "Superpixel-based Higher-order Conditional random fields (SP-HO-CRFs) areknown for their effectiveness in enforcing both short and long spatialcontiguity for pixelwise labelling in computer vision. However, theirhigher-order potentials are usually too complex to learn and often incur a highcomputational cost in performing inference. We propose an new approximationapproach to SP-HO-CRFs that resolves these problems. Our approach is amulti-layer CRF framework that inherits the simplicity from pairwise CRFs byformulating both the higher-order and pairwise cues into the same pairwisepotentials in the first layer. Essentially, this approach provides accuracyenhancement on the basis of pairwise CRFs without training by reusing theirpre-trained parameters and/or weights. The proposed multi-layer approachperforms especially well in delineating the boundary details (boarders) ofobject categories such as trees and bushes. Multiple sets of experimentsconducted on dataset MSRC-21 and PASCAL VOC 2012 validate the effectiveness andefficiency of the proposed methods. ", "id2": "907", "id3": "None"}
{"id": "909", "content": "The Normalized Cut (NCut) objective function, widely used in data clusteringand image segmentation, quantifies the cost of graph partitioning in a way thatbiases clusters or segments that are balanced towards having lower values thanunbalanced partitionings. However, this bias is so strong that it avoids anysingleton partitions, even when vertices are very weakly connected to the restof the graph. Motivated by the B uhler-Hein family of balanced cut costs, wepropose the family of Compassionately Conservative Balanced (CCB) Cut costs,which are indexed by a parameter that can be used to strike a compromisebetween the desire to avoid too many singleton partitions and the notion thatall partitions should be balanced. We show that CCB-Cut minimization can berelaxed into an orthogonally constrained $ ell_  tau $-minimization problemthat coincides with the problem of computing Piecewise Flat Embeddings (PFE)for one particular index value, and we present an algorithm for solving therelaxed problem by iteratively minimizing a sequence of reweighted Rayleighquotients (IRRQ). Using images from the BSDS500 database, we show that imagesegmentation based on CCB-Cut minimization provides better accuracy withrespect to ground truth and greater variability in region size than NCut-basedimage segmentation. ", "id2": "908", "id3": "None"}
{"id": "910", "content": "One of the most common tasks in medical imaging is semantic segmentation.Achieving this segmentation automatically has been an active area of research,but the task has been proven very challenging due to the large variation ofanatomy across different patients. However, recent advances in deep learninghave made it possible to significantly improve the performance of imagerecognition and semantic segmentation methods in the field of computer vision.Due to the data driven approaches of hierarchical feature learning in deeplearning frameworks, these advances can be translated to medical images withoutmuch difficulty. Several variations of deep convolutional neural networks havebeen successfully applied to medical images. Especially fully convolutionalarchitectures have been proven efficient for segmentation of 3D medical images.In this article, we describe how to build a 3D fully convolutional network(FCN) that can process 3D images in order to produce automatic semanticsegmentations. The model is trained and evaluated on a clinical computedtomography (CT) dataset and shows state-of-the-art performance in multi-organsegmentation. ", "id2": "909", "id3": "None"}
{"id": "911", "content": "Pixelwise segmentation of the left ventricular (LV) myocardium and the fourcardiac chambers in 2-D steady state free precession (SSFP) cine sequences isan essential preprocessing step for a wide range of analyses. Variability incontrast, appearance, orientation, and placement of the heart between patients,clinical views, scanners, and protocols makes fully automatic semanticsegmentation a notoriously difficult problem. Here, we present $  Omega $-Net(Omega-Net): a novel convolutional neural network (CNN) architecture forsimultaneous localization, transformation into a canonical orientation, andsemantic segmentation. First, an initial segmentation is performed on the inputimage, second, the features learned during this initial segmentation are usedto predict the parameters needed to transform the input image into a canonicalorientation, and third, a final segmentation is performed on the transformedimage. In this work, $  Omega $-Nets of varying depths were trained to detectfive foreground classes in any of three clinical views (short axis, SA,four-chamber, 4C, two-chamber, 2C), without prior knowledge of the view beingsegmented. The architecture was trained on a cohort of patients withhypertrophic cardiomyopathy and healthy control subjects. Network performanceas measured by weighted foreground intersection-over-union (IoU) wassubstantially improved in the best-performing $  Omega $- Net compared withU-Net segmentation without localization or orientation. In addition,  Omega -Net was retrained from scratch on the 2017 MICCAI ACDC dataset, andachieves state-of-the-art results on the LV and RV bloodpools, and performedslightly worse in segmentation of the LV myocardium. We conclude thisarchitecture represents a substantive advancement over prior approaches, withimplications for biomedical image segmentation more generally. ", "id2": "910", "id3": "None"}
{"id": "912", "content": "Recent advances in 3D fully convolutional networks (FCN) have made itfeasible to produce dense voxel-wise predictions of volumetric images. In thiswork, we show that a multi-class 3D FCN trained on manually labeled CT scans ofseveral anatomical structures (ranging from the large organs to thin vessels)can achieve competitive segmentation results, while avoiding the need forhandcrafting features or training class-specific models.  To this end, we propose a two-stage, coarse-to-fine approach that will firstuse a 3D FCN to roughly define a candidate region, which will then be used asinput to a second 3D FCN. This reduces the number of voxels the second FCN hasto classify to ~10% and allows it to focus on more detailed segmentation of theorgans and vessels.  We utilize training and validation sets consisting of 331 clinical CT imagesand test our models on a completely unseen data collection acquired at adifferent hospital that includes 150 CT scans, targeting three anatomicalorgans (liver, spleen, and pancreas). In challenging organs such as thepancreas, our cascaded approach improves the mean Dice score from 68.5 to82.2%, achieving the highest reported average score on this dataset. We comparewith a 2D FCN method on a separate dataset of 240 CT scans with 18 classes andachieve a significantly higher performance in small organs and vessels.Furthermore, we explore fine-tuning our models to different datasets.  Our experiments illustrate the promise and robustness of current 3D FCN basedsemantic segmentation of medical images, achieving state-of-the-art results.Our code and trained models are available for download:https://github.com/holgerroth/3Dunet_abdomen_cascade. ", "id2": "911", "id3": "None"}
{"id2": 1090, "id3": "911", "content": "Recent advances in 3D fully convolutional networks (FCN) have made itfeasible to produce dense voxel-wise predictions of volumetric images. In thiswork, we show that a multi-class 3D FCN trained on manually labeled CT scans ofseveral anatomical structures (ranging from the large organs to thin vessels)can achieve competitive segmentation results, while avoiding the need forhandcrafting features or training class-specific models. To this end, we propose a two-stage, coarse-to-fine approach that will firstuse a 3D FCN to roughly define a candidate region, which will then be used asinput to a second 3D FCN. This reduces the number of voxels the second FCN hasto classify to ~10% and allows it to focus on more detailed segmentation of theorgans and vessels. We utilize training and validation sets consisting of 331 clinical CT imagesand test our models on a completely unseen data collection acquired at adifferent hospital that includes 150 CT scans, targeting three anatomicalorgans (liver, spleen, and pancreas). In challenging organs such as thepancreas, our cascaded approach improves the mean Dice score from 68.5 to82.2%, achieving the highest reported average score on this dataset. We comparewith a 2D FCN method on a separate dataset of 240 CT scans with 18 classes andachieve a significantly higher performance in small organs and vessels.Furthermore, we explore fine-tuning our models to different datasets. Our experiments illustrate the promise and robustness of current 3D FCN basedsemantic segmentation of medical images, achieving state-of-the-art results.Our code and trained models are available for download:https://github.com/holgerroth/3Dunet_abdomen_cascade."}
{"id": "913", "content": "This work presents a region-growing image segmentation approach based onsuperpixel decomposition. From an initial contour-constrained over-segmentationof the input image, the image segmentation is achieved by iteratively mergingsimilar superpixels into regions. This approach raises two key issues: (1) howto compute the similarity between superpixels in order to perform accuratemerging and (2) in which order those superpixels must be merged together. Inthis perspective, we firstly introduce a robust adaptive multi-scale superpixelsimilarity in which region comparisons are made both at content and commonborder level. Secondly, we propose a global merging strategy to efficientlyguide the region merging process. Such strategy uses an adpative mergingcriterion to ensure that best region aggregations are given highest priorities.This allows to reach a final segmentation into consistent regions with strongboundary adherence. We perform experiments on the BSDS500 image dataset tohighlight to which extent our method compares favorably against otherwell-known image segmentation algorithms. The obtained results demonstrate thepromising potential of the proposed approach. ", "id2": "912", "id3": "None"}
{"id": "914", "content": "State-of-the-art systems for semantic image segmentation use feed-forwardpipelines with fixed computational costs. Building an image segmentation systemthat works across a range of computational budgets is challenging andtime-intensive as new architectures must be designed and trained for everycomputational setting. To address this problem we develop a recurrent neuralnetwork that successively improves prediction quality with each iteration.Importantly, the RNN may be deployed across a range of computational budgets bymerely running the model for a variable number of iterations. We find that thisarchitecture is uniquely suited for efficiently segmenting videos. Byexploiting the segmentation of past frames, the RNN can perform videosegmentation at similar quality but reduced computational cost compared tostate-of-the-art image segmentation methods. When applied to static images inthe PASCAL VOC 2012 and Cityscapes segmentation datasets, the RNN traces out aspeed-accuracy curve that saturates near the performance of state-of-the-artsegmentation methods. ", "id2": "913", "id3": "None"}
{"id": "915", "content": "Modern deep learning algorithms have triggered various image segmentationapproaches. However most of them deal with pixel based segmentation. However,superpixels provide a certain degree of contextual information while reducingcomputation cost. In our approach, we have performed superpixel level semanticsegmentation considering 3 various levels as neighbours for semantic contexts.Furthermore, we have enlisted a number of ensemble approaches like max-votingand weighted-average. We have also used the Dempster-Shafer theory ofuncertainty to analyze confusion among various classes. Our method has provedto be superior to a number of different modern approaches on the same dataset. ", "id2": "914", "id3": "None"}
{"id2": 1091, "id3": "914", "content": "Modern deep learning algorithms have triggered various image segmentationapproaches. However most of them deal with pixel based segmentation. However,superpixels provide a certain degree of contextual information while reducingcomputation cost. In our approach, we have performed superpixel level semanticsegmentation considering 3 various levels as neighbours for semantic contexts.Furthermore, we have enlisted a number of ensemble approaches like max-votingand weighted-average. We have also used the Dempster-Shafer theory ofuncertainty to analyze confusion among various classes. Our method has provedto be superior to a number of different modern approaches on the same dataset."}
{"id": "916", "content": "With pervasive applications of medical imaging in health-care, biomedicalimage segmentation plays a central role in quantitative analysis, clinicaldiagno- sis, and medical intervention. Since manual anno- tation su ers limitedreproducibility, arduous e orts, and excessive time, automatic segmentation isdesired to process increasingly larger scale histopathological data. Recently,deep neural networks (DNNs), par- ticularly fully convolutional networks(FCNs), have been widely applied to biomedical image segmenta- tion, attainingmuch improved performance. At the same time, quantization of DNNs has become anac- tive research topic, which aims to represent weights with less memory(precision) to considerably reduce memory and computation requirements of DNNswhile maintaining acceptable accuracy. In this paper, we apply quantizationtechniques to FCNs for accurate biomedical image segmentation. Unlike existinglitera- ture on quantization which primarily targets memory and computationcomplexity reduction, we apply quan- tization as a method to reduce over ttingin FCNs for better accuracy. Speci cally, we focus on a state-of- the-artsegmentation framework, suggestive annotation [22], which judiciously extractsrepresentative annota- tion samples from the original training dataset, obtain-ing an e ective small-sized balanced training dataset. We develop two newquantization processes for this framework: (1) suggestive annotation withquantiza- tion for highly representative training samples, and (2) networktraining with quantization for high accuracy. Extensive experiments on theMICCAI Gland dataset show that both quantization processes can improve thesegmentation performance, and our proposed method exceeds the currentstate-of-the-art performance by up to 1%. In addition, our method has areduction of up to 6.4x on memory usage. ", "id2": "915", "id3": "None"}
{"id": "917", "content": "Image segmentation is the process of partitioning the image into significantregions easier to analyze. Nowadays, segmentation has become a necessity inmany practical medical imaging methods as locating tumors and diseases. HiddenMarkov Random Field model is one of several techniques used in imagesegmentation. It provides an elegant way to model the segmentation process.This modeling leads to the minimization of an objective function. ConjugateGradient algorithm (CG) is one of the best known optimization techniques. Thispaper proposes the use of the Conjugate Gradient algorithm (CG) for imagesegmentation, based on the Hidden Markov Random Field. Since derivatives arenot available for this expression, finite differences are used in the CGalgorithm to approximate the first derivative. The approach is evaluated usinga number of publicly available images, where ground truth is known. The DiceCoefficient is used as an objective criterion to measure the quality ofsegmentation. The results show that the proposed CG approach compares favorablywith other variants of Hidden Markov Random Field segmentation algorithms. ", "id2": "916", "id3": "None"}
{"id": "918", "content": "As the demand for enabling high-level autonomous driving has increased inrecent years and visual perception is one of the critical features to enablefully autonomous driving, in this paper, we introduce an efficient approach forsimultaneous object detection, depth estimation and pixel-level semanticsegmentation using a shared convolutional architecture. The proposed networkmodel, which we named Driving Scene Perception Network (DSPNet), usesmulti-level feature maps and multi-task learning to improve the accuracy andefficiency of object detection, depth estimation and image segmentation tasksfrom a single input image. Hence, the resulting network model uses less than850 MiB of GPU memory and achieves 14.0 fps on NVIDIA GeForce GTX 1080 with a1024x512 input image, and both precision and efficiency have been improved overcombination of single tasks. ", "id2": "917", "id3": "None"}
{"id": "919", "content": "Image segmentation is still an open problem especially when intensities ofthe interested objects are overlapped due to the presence of intensityinhomogeneity (also known as bias field). To segment images with intensityinhomogeneities, a bias correction embedded level set model is proposed whereInhomogeneities are Estimated by Orthogonal Primary Functions (IEOPF). In theproposed model, the smoothly varying bias is estimated by a linear combinationof a given set of orthogonal primary functions. An inhomogeneous intensityclustering energy is then defined and membership functions of the clustersdescribed by the level set function are introduced to rewrite the energy as adata term of the proposed model. Similar to popular level set methods, aregularization term and an arc length term are also included to regularize andsmooth the level set function, respectively. The proposed model is thenextended to multichannel and multiphase patterns to segment colourful imagesand images with multiple objects, respectively. It has been extensively testedon both synthetic and real images that are widely used in the literature andpublic BrainWeb and IBSR datasets. Experimental results and comparison withstate-of-the-art methods demonstrate that advantages of the proposed model interms of bias correction and segmentation accuracy. ", "id2": "918", "id3": "None"}
{"id": "920", "content": "Robust cross-seasonal localization is one of the major challenges inlong-term visual navigation of autonomous vehicles. In this paper, we exploitrecent advances in semantic segmentation of images, i.e., where each pixel isassigned a label related to the type of object it represents, to attack theproblem of long-term visual localization. We show that semantically labeled 3-Dpoint maps of the environment, together with semantically segmented images, canbe efficiently used for vehicle localization without the need for detailedfeature descriptors (SIFT, SURF, etc.). Thus, instead of depending onhand-crafted feature descriptors, we rely on the training of an imagesegmenter. The resulting map takes up much less storage space compared to atraditional descriptor based map. A particle filter based semantic localizationsolution is compared to one based on SIFT-features, and even with largeseasonal variations over the year we perform on par with the larger and moredescriptive SIFT-features, and are able to localize with an error below 1 mmost of the time. ", "id2": "919", "id3": "None"}
{"id": "921", "content": "We propose a novel traffic sign detection system that simultaneouslyestimates the location and precise boundary of traffic signs usingconvolutional neural network (CNN). Estimating the precise boundary of trafficsigns is important in navigation systems for intelligent vehicles where trafficsigns can be used as 3D landmarks for road environment. Previous traffic signdetection systems, including recent methods based on CNN, only provide boundingboxes of traffic signs as output, and thus requires additional processes suchas contour estimation or image segmentation to obtain the precise signboundary. In this work, the boundary estimation of traffic signs is formulatedas a 2D pose and shape class prediction problem, and this is effectively solvedby a single CNN. With the predicted 2D pose and the shape class of a targettraffic sign in an input image, we estimate the actual boundary of the targetsign by projecting the boundary of a corresponding template sign image into theinput image plane. By formulating the boundary estimation problem as aCNN-based pose and shape prediction task, our method is end-to-end trainable,and more robust to occlusion and small targets than other boundary estimationmethods that rely on contour estimation or image segmentation. The proposedmethod with architectural optimization provides an accurate traffic signboundary estimation which is also efficient in compute, showing a detectionframe rate higher than 7 frames per second on low-power mobile platforms. ", "id2": "920", "id3": "None"}
{"id": "922", "content": "Medical image analysis, especially segmenting a specific organ, has animportant role in developing clinical decision support systems. In cardiacmagnetic resonance (MR) imaging, segmenting the left and right ventricles helpsphysicians diagnose different heart abnormalities. There are challenges forthis task, including the intensity and shape similarity between left ventricleand other organs, inaccurate boundaries and presence of noise in most of theimages. In this paper we propose an automated method for segmenting the leftventricle in cardiac MR images. We first automatically extract the region ofinterest, and then employ it as an input of a fully convolutional network. Wetrain the network accurately despite the small number of left ventricle pixelsin comparison with the whole image. Thresholding on the output map of the fullyconvolutional network and selection of regions based on their roundness areperformed in our proposed post-processing phase. The Dice score of our methodreaches 87.24% by applying this algorithm on the York dataset of heart images. ", "id2": "921", "id3": "None"}
{"id": "923", "content": "Image segmentation is the process of partitioning an image into a set ofmeaningful regions according to some criteria. Hierarchical segmentation hasemerged as a major trend in this regard as it favors the emergence of importantregions at different scales. On the other hand, many methods allow us to haveprior information on the position of structures of interest in the images. Inthis paper, we present a versatile hierarchical segmentation method that takesinto account any prior spatial information and outputs a hierarchicalsegmentation that emphasizes the contours or regions of interest whilepreserving the important structures in the image. An application of this methodto the weakly-supervised segmentation problem is presented. ", "id2": "922", "id3": "None"}
{"id2": 1092, "id3": "922", "content": "Image segmentation is the process of partitioning an image into a set ofmeaningful regions according to some criteria. Hierarchical segmentation hasemerged as a major trend in this regard as it favors the emergence of importantregions at different scales. On the other hand, many methods allow us to haveprior information on the position of structures of interest in the images. Inthis paper, we present a versatile hierarchical segmentation method that takesinto account any prior spatial information and outputs a hierarchicalsegmentation that emphasizes the contours or regions of interest whilepreserving the important structures in the image. An application of this methodto the weakly-supervised segmentation problem is presented."}
{"id": "924", "content": "Osteoarthritis (OA) is one of the major health issues among the elderlypopulation. MRI is the most popular technology to observe and evaluate theprogress of OA course. However, the extreme labor cost of MRI analysis makesthe process inefficient and expensive. Also, due to human error and subjectivenature, the inter- and intra-observer variability is rather high.Computer-aided knee MRI segmentation is currently an active research fieldbecause it can alleviate doctors and radiologists from the time consuming andtedious job, and improve the diagnosis performance which has immense potentialfor both clinic and scientific research. In the past decades, researchers haveinvestigated automatic/semi-automatic knee MRI segmentation methodsextensively. However, to the best of our knowledge, there is no comprehensivesurvey paper in this field yet. In this survey paper, we classify the existingmethods by their principles and discuss the current research status and pointout the future research trend in-depth. ", "id2": "923", "id3": "None"}
{"id": "925", "content": "This paper proposes a generative ScatterNet hybrid deep learning (G-SHDL)network for semantic image segmentation. The proposed generative architectureis able to train rapidly from relatively small labeled datasets using theintroduced structural priors. In addition, the number of filters in each layerof the architecture is optimized resulting in a computationally efficientarchitecture. The G-SHDL network produces state-of-the-art classificationperformance against unsupervised and semi-supervised learning on two imagedatasets. Advantages of the G-SHDL network over supervised methods aredemonstrated with experiments performed on training datasets of reduced size. ", "id2": "924", "id3": "None"}
{"id": "926", "content": "Deep neural networks have been successfully applied to problems such as imagesegmentation, image super-resolution, coloration and image inpainting. In thiswork we propose the use of convolutional neural networks (CNN) for imageinpainting of large regions in high-resolution textures. Due to limitedcomputational resources processing high-resolution images with neural networksis still an open problem. Existing methods separate inpainting of globalstructure and the transfer of details, which leads to blurry results and lossof global coherence in the detail transfer step. Based on advances in texturesynthesis using CNNs we propose patch-based image inpainting by a CNN that isable to optimize for global as well as detail texture statistics. Our method iscapable of filling large inpainting regions, oftentimes exceeding the qualityof comparable methods for high-resolution images. For reference patch look-upwe propose to use the same summary statistics that are used in the inpaintingprocess. ", "id2": "925", "id3": "None"}
{"id": "927", "content": "This review presents an in-depth study of the literature on segmentationmethods applied in dental imaging. Ten segmentation methods were studied andcategorized according to the type of the segmentation method (region-based,threshold-based, cluster-based, boundary-based or watershed-based), type ofX-ray images used (intra-oral or extra-oral) and characteristics of the datasetused to evaluate the methods in the state-of-the-art works. We found that theliterature has primarily focused on threshold-based segmentation methods (54%).80% of the reviewed papers have used intra-oral X-ray images in theirexperiments, demonstrating preference to perform segmentation on images ofalready isolated parts of the teeth, rather than using extra-oral X-rays, whichshow tooth structure of the mouth and bones of the face. To fill a scientificgap in the field, a novel data set based on extra-oral X-ray images areproposed here. A statistical comparison of the results found with the 10 imagesegmentation methods over our proposed data set comprised of 1,500 images isalso carried out, providing a more comprehensive source of performanceassessment. Discussion on limitations of the methods conceived over the pastyear as well as future perspectives on exploiting learning-based segmentationmethods to improve performance are also provided. ", "id2": "926", "id3": "None"}
{"id": "928", "content": "Image segmentation is an important component of many image understandingsystems. It aims to group pixels in a spatially and perceptually coherentmanner. Typically, these algorithms have a collection of parameters thatcontrol the degree of over-segmentation produced. It still remains a challengeto properly select such parameters for human-like perceptual grouping. In thiswork, we exploit the diversity of segments produced by different choices ofparameters. We scan the segmentation parameter space and generate a collectionof image segmentation hypotheses (from highly over-segmented tounder-segmented). These are fed into a cost minimization framework thatproduces the final segmentation by selecting segments that: (1) better describethe natural contours of the image, and (2) are more stable and persistent amongall the segmentation hypotheses. We compare our algorithms performance withstate-of-the-art algorithms, showing that we can achieve improved results. Wealso show that our framework is robust to the choice of segmentation kernelthat produces the initial set of hypotheses. ", "id2": "927", "id3": "None"}
{"id2": 1093, "id3": "927", "content": "Image segmentation is an important component of many image understandingsystems. It aims to group pixels in a spatially and perceptually coherentmanner. Typically, these algorithms have a collection of parameters thatcontrol the degree of over-segmentation produced. It still remains a challengeto properly select such parameters for human-like perceptual grouping. In thiswork, we exploit the diversity of segments produced by different choices ofparameters. We scan the segmentation parameter space and generate a collectionof image segmentation hypotheses (from highly over-segmented tounder-segmented). These are fed into a cost minimization framework thatproduces the final segmentation by selecting segments that: (1) better describethe natural contours of the image, and (2) are more stable and persistent amongall the segmentation hypotheses. We compare our algorithms performance withstate-of-the-art algorithms, showing that we can achieve improved results. Wealso show that our framework is robust to the choice of segmentation kernelthat produces the initial set of hypotheses."}
{"id": "929", "content": "Superpixel segmentation has become an important research problem in imageprocessing. In this paper, we propose an Iterative Spanning Forest (ISF)framework, based on sequences of Image Foresting Transforms, where one canchoose i) a seed sampling strategy, ii) a connectivity function, iii) anadjacency relation, and iv) a seed pixel recomputation procedure to generateimproved sets of connected superpixels (supervoxels in 3D) per iteration. Thesuperpixels in ISF structurally correspond to spanning trees rooted at thoseseeds. We present five ISF methods to illustrate different choices of itscomponents. These methods are compared with approaches from thestate-of-the-art in effectiveness and efficiency. The experiments involve 2Dand 3D datasets with distinct characteristics, and a high level application,named sky image segmentation. The theoretical properties of ISF aredemonstrated in the supplementary material and the results show that some ofits methods are competitive with or superior to the best baselines ineffectiveness and efficiency. ", "id2": "928", "id3": "None"}
{"id2": 1094, "id3": "928", "content": "Superpixel segmentation has become an important research problem in imageprocessing. In this paper, we propose an Iterative Spanning Forest (ISF)framework, based on sequences of Image Foresting Transforms, where one canchoose i) a seed sampling strategy, ii) a connectivity function, iii) anadjacency relation, and iv) a seed pixel recomputation procedure to generateimproved sets of connected superpixels (supervoxels in 3D) per iteration. Thesuperpixels in ISF structurally correspond to spanning trees rooted at thoseseeds. We present five ISF methods to illustrate different choices of itscomponents. These methods are compared with approaches from thestate-of-the-art in effectiveness and efficiency. The experiments involve 2Dand 3D datasets with distinct characteristics, and a high level application,named sky image segmentation. The theoretical properties of ISF aredemonstrated in the supplementary material and the results show that some ofits methods are competitive with or superior to the best baselines ineffectiveness and efficiency."}
{"id": "930", "content": "Deep convolutional networks for semantic image segmentation typically requirelarge-scale labeled data, e.g. ImageNet and MS COCO, for network pre-training.To reduce annotation efforts, self-supervised semantic segmentation is recentlyproposed to pre-train a network without any human-provided labels. The key ofthis new form of learning is to design a proxy task (e.g. image colorization),from which a discriminative loss can be formulated on unlabeled data. Manyproxy tasks, however, lack the critical supervision signals that could inducediscriminative representation for the target image segmentation task. Thusself-supervisions performance is still far from that of supervisedpre-training. In this study, we overcome this limitation by incorporating amix-and-match (M&M) tuning stage in the self-supervision pipeline. Theproposed approach is readily pluggable to many self-supervision methods anddoes not use more annotated samples than the original process. Yet, it iscapable of boosting the performance of target image segmentation task tosurpass fully-supervised pre-trained counterpart. The improvement is madepossible by better harnessing the limited pixel-wise annotations in the targetdataset. Specifically, we first introduce the mix stage, which sparselysamples and mixes patches from the target set to reflect rich and diverse localpatch statistics of target images. A match stage then forms a class-wiseconnected graph, which can be used to derive a strong triplet-baseddiscriminative loss for fine-tuning the network. Our paradigm follows thestandard practice in existing self-supervised studies and no extra data orlabel is required. With the proposed M&M approach, for the first time, aself-supervision method can achieve comparable or even better performancecompared to its ImageNet pre-trained counterpart on both PASCAL VOC2012 datasetand CityScapes dataset. ", "id2": "929", "id3": "None"}
{"id": "931", "content": "This paper reports Deep LOGISMOS approach to 3D tumor segmentation byincorporating boundary information derived from deep contextual learning toLOGISMOS - layered optimal graph image segmentation of multiple objects andsurfaces. Accurate and reliable tumor segmentation is essential to tumor growthanalysis and treatment selection. A fully convolutional network (FCN), UNet, isfirst trained using three adjacent 2D patches centered at the tumor, providingcontextual UNet segmentation and probability map for each 2D patch. The UNetsegmentation is then refined by Gaussian Mixture Model (GMM) and morphologicaloperations. The refined UNet segmentation is used to provide the initial shapeboundary to build a segmentation graph. The cost for each node of the graph isdetermined by the UNet probability maps. Finally, a max-flow algorithm isemployed to find the globally optimal solution thus obtaining the finalsegmentation. For evaluation, we applied the method to pancreatic tumorsegmentation on a dataset of 51 CT scans, among which 30 scans were used fortraining and 21 for testing. With Deep LOGISMOS, DICE Similarity Coefficient(DSC) and Relative Volume Difference (RVD) reached 83.2+-7.8% and 18.6+-17.4%respectively, both are significantly improved (p<0.05) compared with contextualUNet and/or LOGISMOS alone. ", "id2": "930", "id3": "None"}
{"id": "932", "content": "Image segmentation is a fundamental problem in medical image analysis. Inrecent years, deep neural networks achieve impressive performances on manymedical image segmentation tasks by supervised learning on large manuallyannotated data. However, expert annotations on big medical datasets aretedious, expensive or sometimes unavailable. Weakly supervised learning couldreduce the effort for annotation but still required certain amounts ofexpertise. Recently, deep learning shows a potential to produce more accuratepredictions than the original erroneous labels. Inspired by this, we introducea very weakly supervised learning method, for cystic lesion detection andsegmentation in lung CT images, without any manual annotation. Our method worksin a self-learning manner, where segmentation generated in previous steps(first by unsupervised segmentation then by neural networks) is used as groundtruth for the next level of network learning. Experiments on a cystic lunglesion dataset show that the deep learning could perform better than theinitial unsupervised annotation, and progressively improve itself afterself-learning. ", "id2": "931", "id3": "None"}
{"id": "933", "content": "Pixel-wise image segmentation is demanding task in computer vision. ClassicalU-Net architectures composed of encoders and decoders are very popular forsegmentation of medical images, satellite images etc. Typically, neural networkinitialized with weights from a network pre-trained on a large data set likeImageNet shows better performance than those trained from scratch on a smalldataset. In some practical applications, particularly in medicine and trafficsafety, the accuracy of the models is of utmost importance. In this paper, wedemonstrate how the U-Net type architecture can be improved by the use of thepre-trained encoder. Our code and corresponding pre-trained weights arepublicly available at https://github.com/ternaus/TernausNet. We compare threeweight initialization schemes: LeCun uniform, the encoder with weights fromVGG11 and full network trained on the Carvana dataset. This networkarchitecture was a part of the winning solution (1st out of 735) in the Kaggle:Carvana Image Masking Challenge. ", "id2": "932", "id3": "None"}
{"id": "934", "content": "Deep fully convolutional neural network (FCN) based architectures have showngreat potential in medical image segmentation. However, such architecturesusually have millions of parameters and inadequate number of training samplesleading to over-fitting and poor generalization. In this paper, we present anovel highly parameter and memory efficient FCN based architecture for medicalimage analysis. We propose a novel up-sampling path which incorporates longskip and short-cut connections to overcome the feature map explosion in FCNlike architectures. In order to processes the input images at multiple scalesand view points simultaneously, we propose to incorporate Inception modulesparallel structures. We also propose a novel dual loss function whose weightingscheme allows to combine advantages of cross-entropy and dice loss. We havevalidated our proposed network architecture on two publicly available datasets,namely: (i) Automated Cardiac Disease Diagnosis Challenge (ACDC-2017), (ii)Left Ventricular Segmentation Challenge (LV-2011). Our approach in ACDC-2017challenge stands second place for segmentation and first place in automatedcardiac disease diagnosis tasks with an accuracy of 100%. In the LV-2011challenge our approach attained 0.74 Jaccard index, which is so far the highestpublished result in fully automated algorithms. From the segmentation weextracted clinically relevant cardiac parameters and hand-crafted featureswhich reflected the clinical diagnostic analysis to train an ensemble systemfor cardiac disease classification. Our approach combined both cardiacsegmentation and disease diagnosis into a fully automated framework which iscomputational efficient and hence has the potential to be incorporated incomputer-aided diagnosis (CAD) tools for clinical application. ", "id2": "933", "id3": "None"}
{"id2": 1095, "id3": "933", "content": "Deep fully convolutional neural network (FCN) based architectures have showngreat potential in medical image segmentation. However, such architecturesusually have millions of parameters and inadequate number of training samplesleading to over-fitting and poor generalization. In this paper, we present anovel highly parameter and memory efficient FCN based architecture for medicalimage analysis. We propose a novel up-sampling path which incorporates longskip and short-cut connections to overcome the feature map explosion in FCNlike architectures. In order to processes the input images at multiple scalesand view points simultaneously, we propose to incorporate Inception modulesparallel structures. We also propose a novel dual loss function whose weightingscheme allows to combine advantages of cross-entropy and dice loss. We havevalidated our proposed network architecture on two publicly available datasets,namely: (i) Automated Cardiac Disease Diagnosis Challenge (ACDC-2017), (ii)Left Ventricular Segmentation Challenge (LV-2011). Our approach in ACDC-2017challenge stands second place for segmentation and first place in automatedcardiac disease diagnosis tasks with an accuracy of 100%. In the LV-2011challenge our approach attained 0.74 Jaccard index, which is so far the highestpublished result in fully automated algorithms. From the segmentation weextracted clinically relevant cardiac parameters and hand-crafted featureswhich reflected the clinical diagnostic analysis to train an ensemble systemfor cardiac disease classification. Our approach combined both cardiacsegmentation and disease diagnosis into a fully automated framework which iscomputational efficient and hence has the potential to be incorporated incomputer-aided diagnosis (CAD) tools for clinical application."}
{"id": "935", "content": "Automated cardiac image interpretation has the potential to transformclinical practice in multiple ways including enabling low-cost serialassessment of cardiac function in the primary care and rural setting. Wehypothesized that advances in computer vision could enable building a fullyautomated, scalable analysis pipeline for echocardiogram (echo) interpretation.Our approach entailed: 1) preprocessing; 2) convolutional neural networks (CNN)for view identification, image segmentation, and phasing of the cardiac cycle;3) quantification of chamber volumes and left ventricular mass; 4) particletracking to compute longitudinal strain; and 5) targeted disease detection.CNNs accurately identified views (e.g. 99% for apical 4-chamber) and segmentedindividual cardiac chambers. Cardiac structure measurements agreed with studyreport values (e.g. mean absolute deviations (MAD) of 7.7 mL/kg/m2 for leftventricular diastolic volume index, 2918 studies). We computed automatedejection fraction and longitudinal strain measurements (within 2 cohorts),which agreed with commercial software-derived values [for ejection fraction,MAD=5.3%, N=3101 studies; for strain, MAD=1.5% (n=197) and 1.6% (n=110)], anddemonstrated applicability to serial monitoring of breast cancer patients fortrastuzumab cardiotoxicity. Overall, we found that, compared to manualmeasurements, automated measurements had superior performance across seveninternal consistency metrics with an average increase in the Spearmancorrelation coefficient of 0.05 (p=0.02). Finally, we developed diseasedetection algorithms for hypertrophic cardiomyopathy and cardiac amyloidosis,with C-statistics of 0.93 and 0.84, respectively. Our pipeline lays thegroundwork for using automated interpretation to support point-of-care handheldcardiac ultrasound and large-scale analysis of the millions of echos archivedwithin healthcare systems. ", "id2": "934", "id3": "None"}
{"id": "936", "content": "Unsupervised image segmentation and denoising are two fundamental tasks inimage processing. Usually, graph based models such as multicut are used forsegmentation and variational models are employed for denoising. Our approachaddresses both problems at the same time. We propose a novel ILP formulation ofthe first derivative Potts model with the $ ell_1$ data term, where binaryvariables are introduced to deal with the $ ell_0$ norm of the regularizationterm. The ILP is then solved by a standard off-the-shelf MIP solver. Numericalexperiments are compared with the multicut problem. ", "id2": "935", "id3": "None"}
{"id": "937", "content": "Breast cancer is one of the leading causes of cancer death among womenworldwide. In clinical routine, automatic breast ultrasound (BUS) imagesegmentation is very challenging and essential for cancer diagnosis andtreatment planning. Many BUS segmentation approaches have been studied in thelast two decades, and have been proved to be effective on private datasets.Currently, the advancement of BUS image segmentation seems to meet itsbottleneck. The improvement of the performance is increasingly challenging, andonly few new approaches were published in the last several years. It is thetime to look at the field by reviewing previous approaches comprehensively andto investigate the future directions. In this paper, we study the basic ideas,theories, pros and cons of the approaches, group them into categories, andextensively review each category in depth by discussing the principles,application issues, and advantages/disadvantages. ", "id2": "936", "id3": "None"}
{"id": "938", "content": "Breast ultrasound (BUS) image segmentation is challenging and critical forBUS Computer-Aided Diagnosis (CAD) systems. Many BUS segmentation approacheshave been proposed in the last two decades, but the performances of mostapproaches have been assessed using relatively small private datasets withdiffer-ent quantitative metrics, which result in discrepancy in performancecomparison. Therefore, there is a pressing need for building a benchmark tocompare existing methods using a public dataset objectively, and to determinethe performance of the best breast tumor segmentation algorithm available todayand to investigate what segmentation strategies are valuable in clinicalpractice and theoretical study. In this work, we will publish a B-mode BUSimage segmentation benchmark (BUSIS) with 562 images and compare theperformance of five state-of-the-art BUS segmentation methods quantitatively. ", "id2": "937", "id3": "None"}
{"id": "939", "content": "Early and correct diagnosis is a very important aspect of cancer treatment.Detection of tumour in Computed Tomography scan is a tedious and tricky taskwhich requires expert knowledge and a lot of human working hours. As smallhuman error is present in any work he does, it is possible that a CT scan couldbe misdiagnosed causing the patient to become terminal. This paper introduces anovel fully automated framework which helps to detect and segment tumour, ifpresent in a lung CT scan series. It also provides useful analysis of thedetected tumour such as its approximate volume, centre location and more. Theframework provides a single click solution which analyses all CT images of asingle patient series in one go. It helps to reduce the work of manually goingthrough each CT slice and provides quicker and more accurate tumour diagnosis.It makes use of customized image processing and image segmentation methods, todetect and segment the prospective tumour region from the CT scan. It then usesa trained ensemble classifier to correctly classify the segmented region asbeing tumour or not. Tumour analysis further computed can then be used todetermine malignity of the tumour. With an accuracy of 98.14%, the implementedframework can be used in various practical scenarios, capable of eliminatingneed of any expert pathologist intervention. ", "id2": "938", "id3": "None"}
{"id": "940", "content": "Semantic image segmentation is one of the most challenged tasks in computervision. In this paper, we propose a highly fused convolutional network, whichconsists of three parts: feature downsampling, combined feature upsampling andmultiple predictions. We adopt a strategy of multiple steps of upsampling andcombined feature maps in pooling layers with its corresponding unpoolinglayers. Then we bring out multiple pre-outputs, each pre-output is generatedfrom an unpooling layer by one-step upsampling. Finally, we concatenate thesepre-outputs to get the final output. As a result, our proposed network makeshighly use of the feature information by fusing and reusing feature maps. Inaddition, when training our model, we add multiple soft cost functions onpre-outputs and final outputs. In this way, we can reduce the loss reductionwhen the loss is back propagated. We evaluate our model on three majorsegmentation datasets: CamVid, PASCAL VOC and ADE20K. We achieve astate-of-the-art performance on CamVid dataset, as well as considerableimprovements on PASCAL VOC dataset and ADE20K dataset ", "id2": "939", "id3": "None"}
{"id": "941", "content": "The automated segmentation of cells in microscopic images is an open researchproblem that has important implications for studies of the developmental andcancer processes based on in vitro models. In this paper, we present theapproach for segmentation of the DIC images of cultured cells using G-neighborsmoothing followed by Kauwahara filtering and local standard deviation approachfor boundary detection. NIH FIJI/ImageJ tools are used to create the groundtruth dataset. The results of this work indicate that detection of cellboundaries using segmentation approach even in the case of realisticmeasurement conditions is a challenging problem. ", "id2": "940", "id3": "None"}
{"id": "942", "content": "In this paper we present our system for human-in-the-loop video objectsegmentation. The backbone of our system is a method for one-shot video objectsegmentation. While fast, this method requires an accurate pixel-levelsegmentation of one (or several) frames as input. As manually annotating such asegmentation is impractical, we propose a deep interactive image segmentationmethod, that can accurately segment objects with only a handful of clicks. Onthe GrabCut dataset, our method obtains 90% IOU with just 3.8 clicks onaverage, setting the new state of the art. Furthermore, as our methoditeratively refines an initial segmentation, it can effectively correct frameswhere the video object segmentation fails, thus allowing users to quicklyobtain high quality results even on challenging sequences. Finally, weinvestigate usage patterns and give insights in how many steps users take toannotate frames, what kind of corrections they provide, etc., thus givingimportant insights for further improving interactive video segmentation. ", "id2": "941", "id3": "None"}
{"id2": 1096, "id3": "941", "content": "In this paper we present our system for human-in-the-loop video objectsegmentation. The backbone of our system is a method for one-shot video objectsegmentation. While fast, this method requires an accurate pixel-levelsegmentation of one (or several) frames as input. As manually annotating such asegmentation is impractical, we propose a deep interactive image segmentationmethod, that can accurately segment objects with only a handful of clicks. Onthe GrabCut dataset, our method obtains 90% IOU with just 3.8 clicks onaverage, setting the new state of the art. Furthermore, as our methoditeratively refines an initial segmentation, it can effectively correct frameswhere the video object segmentation fails, thus allowing users to quicklyobtain high quality results even on challenging sequences. Finally, weinvestigate usage patterns and give insights in how many steps users take toannotate frames, what kind of corrections they provide, etc., thus givingimportant insights for further improving interactive video segmentation."}
{"id": "943", "content": "A novel multi-atlas based image segmentation method is proposed byintegrating a semi-supervised label propagation method and a supervised randomforests method in a pattern recognition based label fusion framework. Thesemi-supervised label propagation method takes into consideration local andglobal image appearance of images to be segmented and segments the images bypropagating reliable segmentation results obtained by the supervised randomforests method. Particularly, the random forests method is used to train aregression model based on image patches of atlas images for each voxel of theimages to be segmented. The regression model is used to obtain reliablesegmentation results to guide the label propagation for the segmentation. Theproposed method has been compared with state-of-the-art multi-atlas based imagesegmentation methods for segmenting the hippocampus in MR images. Theexperiment results have demonstrated that our method obtained superiorsegmentation performance. ", "id2": "942", "id3": "None"}
{"id": "944", "content": "The segmentation of animals from camera-trap images is a difficult task. Toillustrate, there are various challenges due to environmental conditions andhardware limitation in these images. We proposed a multi-layer robust principalcomponent analysis (multi-layer RPCA) approach for background subtraction. Ourmethod computes sparse and low-rank images from a weighted sum of descriptors,using color and texture features as case of study for camera-trap imagessegmentation. The segmentation algorithm is composed of histogram equalizationor Gaussian filtering as pre-processing, and morphological filters with activecontour as post-processing. The parameters of our multi-layer RPCA wereoptimized with an exhaustive search. The database consists of camera-trapimages from the Colombian forest taken by the Instituto de Investigaci on deRecursos Biol ogicos Alexander von Humboldt. We analyzed the performance ofour method in inherent and therefore challenging situations of camera-trapimages. Furthermore, we compared our method with some state-of-the-artalgorithms of background subtraction, where our multi-layer RPCA outperformedthese other methods. Our multi-layer RPCA reached 76.17 and 69.97% of averagefine-grained F-measure for color and infrared sequences, respectively. To ourbest knowledge, this paper is the first work proposing multi-layer RPCA andusing it for camera-trap images segmentation. ", "id2": "943", "id3": "None"}
{"id": "945", "content": "Image segmentation is considered to be one of the critical tasks inhyperspectral remote sensing image processing. Recently, convolutional neuralnetwork (CNN) has established itself as a powerful model in segmentation andclassification by demonstrating excellent performances. The use of a graphicalmodel such as a conditional random field (CRF) contributes further in capturingcontextual information and thus improving the segmentation performance. In thispaper, we propose a method to segment hyperspectral images by considering bothspectral and spatial information via a combined framework consisting of CNN andCRF. We use multiple spectral cubes to learn deep features using CNN, and thenformulate deep CRF with CNN-based unary and pairwise potential functions toeffectively extract the semantic correlations between patches consisting ofthree-dimensional data cubes. Effective piecewise training is applied in orderto avoid the computationally expensive iterative CRF inference. Furthermore, weintroduce a deep deconvolution network that improves the segmentation masks. Wealso introduce a new dataset and experimented our proposed method on it alongwith several widely adopted benchmark datasets to evaluate the effectiveness ofour method. By comparing our results with those from several state-of-the-artmodels, we show the promising potential of our method. ", "id2": "944", "id3": "None"}
{"id": "946", "content": "We present an end-to-end trainable deep convolutional neural network (DCNN)for semantic segmentation with built-in awareness of semantically meaningfulboundaries. Semantic segmentation is a fundamental remote sensing task, andmost state-of-the-art methods rely on DCNNs as their workhorse. A major reasonfor their success is that deep networks learn to accumulate contextualinformation over very large windows (receptive fields). However, this successcomes at a cost, since the associated loss of effecive spatial resolutionwashes out high-frequency details and leads to blurry object boundaries. Here,we propose to counter this effect by combining semantic segmentation withsemantically informed edge detection, thus making class-boundaries explicit inthe model, First, we construct a comparatively simple, memory-efficient modelby adding boundary detection to the Segnet encoder-decoder architecture.Second, we also include boundary detection in FCN-type models and set up ahigh-end classifier ensemble. We show that boundary detection significantlyimproves semantic segmentation with CNNs. Our high-end ensemble achieves > 90%overall accuracy on the ISPRS Vaihingen benchmark. ", "id2": "945", "id3": "None"}
{"id": "947", "content": "The most common paradigm for vision-based multi-object tracking istracking-by-detection, due to the availability of reliable detectors forseveral important object categories such as cars and pedestrians. However,future mobile systems will need a capability to cope with rich human-madeenvironments, in which obtaining detectors for every possible object categorywould be infeasible. In this paper, we propose a model-free multi-objecttracking approach that uses a category-agnostic image segmentation method totrack objects. We present an efficient segmentation mask-based tracker whichassociates pixel-precise masks reported by the segmentation. Our approach canutilize semantic information whenever it is available for classifying objectsat the track level, while retaining the capability to track generic unknownobjects in the absence of such information. We demonstrate experimentally thatour approach achieves performance comparable to state-of-the-arttracking-by-detection methods for popular object categories such as cars andpedestrians. Additionally, we show that the proposed method can discover androbustly track a large variety of other objects. ", "id2": "946", "id3": "None"}
{"id": "948", "content": "In medicine, visualizing chromosomes is important for medical diagnostics,drug development, and biomedical research. Unfortunately, chromosomes oftenoverlap and it is necessary to identify and distinguish between the overlappingchromosomes. A segmentation solution that is fast and automated will enablescaling of cost effective medicine and biomedical research. We apply neuralnetwork-based image segmentation to the problem of distinguishing betweenpartially overlapping DNA chromosomes. A convolutional neural network iscustomized for this problem. The results achieved intersection over union (IOU)scores of 94.7% for the overlapping region and 88-94% on the non-overlappingchromosome regions. ", "id2": "947", "id3": "None"}
{"id": "949", "content": "Gastric cancer is the second leading cause of cancer-related deathsworldwide, and the major hurdle in biomedical image analysis is thedetermination of the cancer extent. This assignment has high clinical relevanceand would generally require vast microscopic assessment by pathologists. Recentadvances in deep learning have produced inspiring results on biomedical imagesegmentation, while its outcome is reliant on comprehensive annotation. Thisrequires plenty of labor costs, for the ground truth must be annotatedmeticulously by pathologists. In this paper, a reiterative learning frameworkwas presented to train our network on partial annotated biomedical images, andsuperior performance was achieved without any pre-trained or further manualannotation. We eliminate the boundary error of patch-based model through ouroverlapped region forecast algorithm. Through these advisable methods, a meanintersection over union coefficient (IOU) of 0.883 and mean accuracy of 91.09%on the partial labeled dataset was achieved, which made us win the 2017 ChinaBig Data & Artificial Intelligence Innovation and EntrepreneurshipCompetitions. ", "id2": "948", "id3": "None"}
{"id": "950", "content": "Precise 3D segmentation of infant brain tissues is an essential step towardscomprehensive volumetric studies and quantitative analysis of early braindevelopement. However, computing such segmentations is very challenging,especially for 6-month infant brain, due to the poor image quality, among otherdifficulties inherent to infant brain MRI, e.g., the isointense contrastbetween white and gray matter and the severe partial volume effect due to smallbrain sizes. This study investigates the problem with an ensemble of semi-densefully convolutional neural networks (CNNs), which employs T1-weighted andT2-weighted MR images as input. We demonstrate that the ensemble agreement ishighly correlated with the segmentation errors. Therefore, our method providesmeasures that can guide local user corrections. To the best of our knowledge,this work is the first ensemble of 3D CNNs for suggesting annotations withinimages. Furthermore, inspired by the very recent success of dense networks, wepropose a novel architecture, SemiDenseNet, which connects all convolutionallayers directly to the end of the network. Our architecture allows theefficient propagation of gradients during training, while limiting the numberof parameters, requiring one order of magnitude less parameters than popularmedical image segmentation networks such as 3D U-Net. Another contribution ofour work is the study of the impact that early or late fusions of multipleimage modalities might have on the performances of deep architectures. Wereport evaluations of our method on the public data of the MICCAI iSEG-2017Challenge on 6-month infant brain MRI segmentation, and show very competitiveresults among 21 teams, ranking first or second in most metrics. ", "id2": "949", "id3": "None"}
{"id": "951", "content": "We have developed a deep learning network for classification of differentflowers. For this, we have used Visual Geometry Groups 102 category flowerdataset having 8189 images of 102 different flowers from University of Oxford.The method is basically divided into two parts; Image segmentation andclassification. We have compared the performance of two different ConvolutionalNeural Network architectures GoogLeNet and AlexNet for classification purpose.By keeping the hyper parameters same for both architectures, we have found thatthe top 1 and top 5 accuracies of GoogLeNet are 47.15% and 69.17% respectivelywhereas the top 1 and top 5 accuracies of AlexNet are 43.39% and 68.68%respectively. These results are extremely good when compared to randomclassification accuracy of 0.98%. This method for classification of flowers canbe implemented in real time applications and can be used to help botanists fortheir research as well as camping enthusiasts. ", "id2": "950", "id3": "None"}
{"id": "952", "content": "The goal of this paper is to present a new efficient image segmentationmethod based on evolutionary computation which is a model inspired from humanbehavior. Based on this model, a four layer process for image segmentation isproposed using the split/merge approach. In the first layer, an image is splitinto numerous regions using the watershed algorithm. In the second layer, aco-evolutionary process is applied to form centers of finals segments bymerging similar primary regions. In the third layer, a meta-heuristic processuses two operators to connect the residual regions to their correspondingdetermined centers. In the final layer, an evolutionary algorithm is used tocombine the resulted similar and neighbor regions. Different layers of thealgorithm are totally independent, therefore for certain applications aspecific layer can be changed without constraint of changing other layers. Someproperties of this algorithm like the flexibility of its method, the ability touse different feature vectors for segmentation (grayscale, color, texture,etc), the ability to control uniformity and the number of final segments usingfree parameters and also maintaining small regions, makes it possible to applythe algorithm to different applications. Moreover, the independence of eachregion from other regions in the second layer, and the independence of centersin the third layer, makes parallel implementation possible. As a result thealgorithm speed will increase. The presented algorithm was tested on a standarddataset (BSDS 300) of images, and the region boundaries were compared withdifferent people segmentation contours. Results show the efficiency of thealgorithm and its improvement to similar methods. As an instance, in 70% oftested images, results are better than ACT algorithm, besides in 100% of testedimages, we had better results in comparison with VSP algorithm. ", "id2": "951", "id3": "None"}
{"id": "953", "content": "In this work, we revisit atrous convolution, a powerful tool to explicitlyadjust filters field-of-view as well as control the resolution of featureresponses computed by Deep Convolutional Neural Networks, in the application ofsemantic image segmentation. To handle the problem of segmenting objects atmultiple scales, we design modules which employ atrous convolution in cascadeor in parallel to capture multi-scale context by adopting multiple atrousrates. Furthermore, we propose to augment our previously proposed AtrousSpatial Pyramid Pooling module, which probes convolutional features at multiplescales, with image-level features encoding global context and further boostperformance. We also elaborate on implementation details and share ourexperience on training our system. The proposed DeepLabv3 systemsignificantly improves over our previous DeepLab versions without DenseCRFpost-processing and attains comparable performance with other state-of-artmodels on the PASCAL VOC 2012 semantic image segmentation benchmark. ", "id2": "952", "id3": "None"}
{"id": "954", "content": "We define and study error detection and correction tasks that are useful for3D reconstruction of neurons from electron microscopic imagery, and for imagesegmentation more generally. Both tasks take as input the raw image and abinary mask representing a candidate object. For the error detection task, thedesired output is a map of split and merge errors in the object. For the errorcorrection task, the desired output is the true object. We call this objectmask pruning, because the candidate object mask is assumed to be a superset ofthe true object. We train multiscale 3D convolutional networks to perform bothtasks. We find that the error-detecting net can achieve high accuracy. Theaccuracy of the error-correcting net is enhanced if its input object mask isadvice (union of erroneous objects) from the error-detecting net. ", "id2": "953", "id3": "None"}
{"id": "955", "content": "According to the World Health Organization, breast cancer is the most commonform of cancer in women. It is the second leading cause of death among womenround the world, becoming the most fatal form of cancer. Mammographic imagesegmentation is a fundamental task to support image analysis and diagnosis,taking into account shape analysis of mammary lesions and their borders.However, mammogram segmentation is a very hard process, once it is highlydependent on the types of mammary tissues. In this work we present a newsemi-supervised segmentation algorithm based on the modification of the GrowCutalgorithm to perform automatic mammographic image segmentation once a region ofinterest is selected by a specialist. In our proposal, we used fuzzy Gaussianmembership functions to modify the evolution rule of the original GrowCutalgorithm, in order to estimate the uncertainty of a pixel being object orbackground. The main impact of the proposed method is the significant reductionof expert effort in the initialization of seed points of GrowCut to performaccurate segmentation, once it removes the need of selection of backgroundseeds. We also constructed an automatic point selection process based on thesimulated annealing optimization method, avoiding the need of humanintervention. The proposed approach was qualitatively compared with otherstate-of-the-art segmentation techniques, considering the shape of segmentedregions. In order to validate our proposal, we built an image classifier usinga classical multilayer perceptron. We used Zernike moments to extract segmentedimage features. This analysis employed 685 mammograms from IRMA breast cancerdatabase, using fat and fibroid tissues. Results show that the proposedtechnique could achieve a classification rate of 91.28 % for fat tissues,evidencing the feasibility of our approach. ", "id2": "954", "id3": "None"}
{"id": "956", "content": "In this paper, we propose a simple but effective method for fast imagesegmentation. We re-examine the locality-preserving character of spectralclustering by constructing a graph over image regions with both global andlocal connections. Our novel approach to build graph connections relies on twokey observations: 1) local region pairs that co-occur frequently will have ahigh probability to reside on a common object; 2) spatially distant regions ina common object often exhibit similar visual saliency, which implies theirneighborship in a manifold. We present a novel energy function to efficientlyconduct graph partitioning. Based on multiple high quality partitions, we showthat the generated eigenvector histogram based representation can automaticallydrive effective unary potentials for a hierarchical random field model toproduce multi-class segmentation. Sufficient experiments, on the BSDS500benchmark, large-scale PASCAL VOC and COCO datasets, demonstrate thecompetitive segmentation accuracy and significantly improved efficiency of ourproposed method compared with other state of the arts. ", "id2": "955", "id3": "None"}
{"id": "957", "content": "Spleen volume estimation using automated image segmentation technique may beused to detect splenomegaly (abnormally enlarged spleen) on Magnetic ResonanceImaging (MRI) scans. In recent years, Deep Convolutional Neural Networks (DCNN)segmentation methods have demonstrated advantages for abdominal organsegmentation. However, variations in both size and shape of the spleen on MRIimages may result in large false positive and false negative labeling whendeploying DCNN based methods. In this paper, we propose the SplenomegalySegmentation Network (SSNet) to address spatial variations when segmentingextraordinarily large spleens. SSNet was designed based on the framework ofimage-to-image conditional generative adversarial networks (cGAN).Specifically, the Global Convolutional Network (GCN) was used as the generatorto reduce false negatives, while the Markovian discriminator (PatchGAN) wasused to alleviate false positives. A cohort of clinically acquired 3D MRI scans(both T1 weighted and T2 weighted) from patients with splenomegaly were used totrain and test the networks. The experimental results demonstrated that a meanDice coefficient of 0.9260 and a median Dice coefficient of 0.9262 using SSNeton independently tested MRI volumes of patients with splenomegaly. ", "id2": "956", "id3": "None"}
{"id": "958", "content": "We propose an approach to semantic (image) segmentation that reduces thecomputational costs by a factor of 25 with limited impact on the quality ofresults. Semantic segmentation has a number of practical applications, and formost such applications the computational costs are critical. The method followsa typical two-column network structure, where one column accepts an inputimage, while the other accepts a half-resolution version of that image. Byidentifying specific regions in the full-resolution image that can be safelyignored, as well as carefully tailoring the network structure, we can processapproximately 15 highresolution Cityscapes images (1024x2048) per second usinga single GTX 980 video card, while achieving a mean intersection-over-unionscore of 72.9% on the Cityscapes test set. ", "id2": "957", "id3": "None"}
{"id": "959", "content": "There has been a significant increase from 2010 to 2016 in the number ofpeople suffering from spine problems. The automatic image segmentation of thespine obtained from a computed tomography (CT) image is important fordiagnosing spine conditions and for performing surgery with computer-assistedsurgery systems. The spine has a complex anatomy that consists of 33 vertebrae,23 intervertebral disks, the spinal cord, and connecting ribs. As a result, thespinal surgeon is faced with the challenge of needing a robust algorithm tosegment and create a model of the spine. In this study, we developed anautomatic segmentation method to segment the spine, and we compared oursegmentation results with reference segmentations obtained by experts. Wedeveloped a fully automatic approach for spine segmentation from CT based on ahybrid method. This method combines the convolutional neural network (CNN) andfully convolutional network (FCN), and utilizes class redundancy as a softconstraint to greatly improve the segmentation results. The proposed method wasfound to significantly enhance the accuracy of the segmentation results and thesystem processing time. Our comparison was based on 12 measurements: the Dicecoefficient (94%), Jaccard index (93%), volumetric similarity (96%),sensitivity (97%), specificity (99%), precision (over segmentation; 8.3 andunder segmentation 2.6), accuracy (99%), Matthews correlation coefficient(0.93), mean surface distance (0.16 mm), Hausdorff distance (7.4 mm), andglobal consistency error (0.02). We experimented with CT images from 32patients, and the experimental results demonstrated the efficiency of theproposed method. ", "id2": "958", "id3": "None"}
{"id": "960", "content": "With the rapidly increasing interest in machine learning based solutions forautomatic image annotation, the availability of reference annotations foralgorithm training is one of the major bottlenecks in the field. Crowdsourcinghas evolved as a valuable option for low-cost and large-scale data annotation;however, quality control remains a major issue which needs to be addressed. Toour knowledge, we are the first to analyze the annotation process to improvecrowd-sourced image segmentation. Our method involves training a regressor toestimate the quality of a segmentation from the annotators clickstream data.The quality estimation can be used to identify spam and weight individualannotations by their (estimated) quality when merging multiple segmentations ofone image. Using a total of 29,000 crowd annotations performed on publiclyavailable data of different object classes, we show that (1) our method ishighly accurate in estimating the segmentation quality based on clickstreamdata, (2) outperforms state-of-the-art methods for merging multipleannotations. As the regressor does not need to be trained on the object classthat it is applied to it can be regarded as a low-cost option for qualitycontrol and confidence analysis in the context of crowd-based image annotation. ", "id2": "959", "id3": "None"}
{"id": "961", "content": "We present the 2017 Visual Domain Adaptation (VisDA) dataset and challenge, alarge-scale testbed for unsupervised domain adaptation across visual domains.Unsupervised domain adaptation aims to solve the real-world problem of domainshift, where machine learning models trained on one domain must be transferredand adapted to a novel visual domain without additional supervision. TheVisDA2017 challenge is focused on the simulation-to-reality shift and has twoassociated tasks: image classification and image segmentation. The goal in bothtracks is to first train a model on simulated, synthetic data in the sourcedomain and then adapt it to perform well on real image data in the unlabeledtest domain. Our dataset is the largest one to date for cross-domain objectclassification, with over 280K images across 12 categories in the combinedtraining, validation and testing domains. The image segmentation dataset isalso large-scale with over 30K images across 18 categories in the threedomains. We compare VisDA to existing cross-domain adaptation datasets andprovide a baseline performance analysis using various domain adaptation modelsthat are currently popular in the field. ", "id2": "960", "id3": "None"}
{"id": "962", "content": "We propose a novel Active Learning framework capable to train effectively aconvolutional neural network for semantic segmentation of medical imaging, witha limited amount of training labeled data. Our contribution is a practicalCost-Effective Active Learning approach using dropout at test time as MonteCarlo sampling to model the pixel-wise uncertainty and to analyze the imageinformation to improve the training performance. The source code of thisproject is available athttps://marc-gorriz.github.io/CEAL-Medical-Image-Segmentation/ . ", "id2": "961", "id3": "None"}
{"id": "963", "content": "This paper presents an efficient automatic color image segmentation methodusing a seeded region growing and merging method based on square elementalregions. Our segmentation method consists of the three steps: generating seedregions, merging the regions, and applying a pixel-wise boundary determinationalgorithm to the resultant polygonal regions. The major features of our methodare as follows: the use of square elemental regions instead of pixels as theprocessing unit, a seed generation method based on enhanced gradient values, aseed region growing method exploiting local gradient values, a region mergingmethod using a similarity measure including a homogeneity distance based onTsallis entropy, and a termination condition of region merging using anestimated desired number of regions. Using square regions as the processingunit substantially reduces the time complexity of the algorithm and makes theperformance stable. The experimental results show that our method exhibitsstable performance for a variety of natural images, including heavily texturedareas, and produces good segmentation results using the same parameter values.The results of our method are fairly comparable to, and in some respects betterthan, those of existing algorithms. ", "id2": "962", "id3": "None"}
{"id": "964", "content": "Convolutional neural networks have shown great success on feature extractionfrom raw input data such as images. Although convolutional neural networks areinvariant to translations on the inputs, they are not invariant to othertransformations, including rotation and flip. Recent attempts have been made toincorporate more invariance in image recognition applications, but they are notapplicable to dense prediction tasks, such as image segmentation. In thispaper, we propose a set of methods based on kernel rotation and flip to enablerotation and flip invariance in convolutional neural networks. The kernelrotation can be achieved on kernels of 3 $ times$ 3, while kernel flip can beapplied on kernels of any size. By rotating in eight or four angles, theconvolutional layers could produce the corresponding number of feature mapsbased on eight or four different kernels. By using flip, the convolution layercan produce three feature maps. By combining produced feature maps usingmaxout, the resource requirement could be significantly reduced while stillretain the invariance properties. Experimental results demonstrate that theproposed methods can achieve various invariance at reasonable resourcerequirements in terms of both memory and time. ", "id2": "963", "id3": "None"}
{"id": "965", "content": "While significant attention has been recently focused on designing superviseddeep semantic segmentation algorithms for vision tasks, there are many domainsin which sufficient supervised pixel-level labels are difficult to obtain. Inthis paper, we revisit the problem of purely unsupervised image segmentationand propose a novel deep architecture for this problem. We borrow recent ideasfrom supervised semantic segmentation methods, in particular by concatenatingtwo fully convolutional networks together into an autoencoder--one for encodingand one for decoding. The encoding layer produces a k-way pixelwise prediction,and both the reconstruction error of the autoencoder as well as the normalizedcut produced by the encoder are jointly minimized during training. Whencombined with suitable postprocessing involving conditional random fieldsmoothing and hierarchical segmentation, our resulting algorithm achievesimpressive results on the benchmark Berkeley Segmentation Data Set,outperforming a number of competing methods. ", "id2": "964", "id3": "None"}
{"id": "966", "content": "Motivated by the important archaeological application of exploring culturalheritage objects, in this paper we study the challenging problem ofautomatically segmenting curve structures that are very weakly stamped orcarved on an object surface in the form of a highly noisy depth map. Differentfrom most classical low-level image segmentation methods that are known to bevery sensitive to the noise and occlusions, we propose a new supervisedlearning algorithm based on Convolutional Neural Network (CNN) to implicitlylearn and utilize more curve geometry and pattern information for addressingthis challenging problem. More specifically, we first propose a FullyConvolutional Network (FCN) to estimate the skeleton of curve structures and ateach skeleton pixel, a scale value is estimated to reflect the local curvewidth. Then we propose a dense prediction network to refine the estimated curveskeletons. Based on the estimated scale values, we finally develop an adaptivethresholding algorithm to achieve the final segmentation of curve structures.In the experiment, we validate the performance of the proposed method on adataset of depth images scanned from unearthed pottery sherds dating to theWoodland period of Southeastern North America. ", "id2": "965", "id3": "None"}
{"id": "967", "content": "We present a method for reconstructing images viewed by observers based onlyon their eye movements. By exploring the relationships between gaze patternsand image stimuli, the What Are You Looking At? (WAYLA) system learns tosynthesize photo-realistic images that are similar to the original picturesbeing viewed. The WAYLA approach is based on the Conditional GenerativeAdversarial Network (Conditional GAN) image-to-image translation technique ofIsola et al. We consider two specific applications - the first, ofreconstructing newspaper images from gaze heat maps, and the second, ofdetailed reconstruction of images containing only text. The newspaper imagereconstruction process is divided into two image-to-image translationoperations, the first mapping gaze heat maps into image segmentations, and thesecond mapping the generated segmentation into a newspaper image. We validatethe performance of our approach using various evaluation metrics, along withhuman visual inspection. All results confirm the ability of our network toperform image generation tasks using eye tracking data. ", "id2": "966", "id3": "None"}
{"id": "968", "content": "In interactive medical image segmentation, anatomical structures areextracted from reconstructed volumetric images. The first iterations of userinteraction traditionally consist of drawing pictorial hints as an initialestimate of the object to extract. Only after this time consuming first phase,the efficient selective refinement of current segmentation results begins.Erroneously labeled seeds, especially near the border of the object, arechallenging to detect and replace for a human and may substantially impact theoverall segmentation quality. We propose an automatic seeding pipeline as wellas a configuration based on saliency recognition, in order to skip thetime-consuming initial interaction phase during segmentation. A median Dicescore of 68.22% is reached before the first user interaction on the test dataset with an error rate in seeding of only 0.088%. ", "id2": "967", "id3": "None"}
{"id": "969", "content": "Deep convolutional neural networks (CNNs) have been shown to performextremely well at a variety of tasks including subtasks of autonomous drivingsuch as image segmentation and object classification. However, networksdesigned for these tasks typically require vast quantities of training data andlong training periods to converge. We investigate the design rationale behindend-to-end driving network designs by proposing and comparing three small andcomputationally inexpensive deep end-to-end neural network models that generatedriving control signals directly from input images. In contrast to prior workthat segments the autonomous driving task, our models take on a novel approachto the autonomous driving problem by utilizing deep and thin FullyConvolutional Nets (FCNs) with recurrent neural nets and low parameter countsto tackle a complex end-to-end regression task predicting both steering andacceleration commands. In addition, we include layers optimized forclassification to allow the networks to implicitly learn image semantics. Weshow that the resulting networks use 3x fewer parameters than the most recentcomparable end-to-end driving network and 500x fewer parameters than theAlexNet variations and converge both faster and to lower losses whilemaintaining robustness against overfitting. ", "id2": "968", "id3": "None"}
{"id": "970", "content": "We present DLTK, a toolkit providing baseline implementations for efficientexperimentation with deep learning methods on biomedical images. It builds ontop of TensorFlow and its high modularity and easy-to-use examples allow for alow-threshold access to state-of-the-art implementations for typical medicalimaging problems. A comparison of DLTKs reference implementations of popularnetwork architectures for image segmentation demonstrates new top performanceon the publicly available challenge data Multi-Atlas Labeling Beyond theCranial Vault. The average test Dice similarity coefficient of $81.5$ exceedsthe previously best performing CNN ($75.7$) and the accuracy of the challengewinning method ($79.0$). ", "id2": "969", "id3": "None"}
{"id": "971", "content": "We explore encoding brain symmetry into a neural network for a brain tumorsegmentation task. A healthy human brain is symmetric at a high level ofabstraction, and the high-level asymmetric parts are more likely to be tumorregions. Paying more attention to asymmetries has the potential to boost theperformance in brain tumor segmentation. We propose a method to encode brainsymmetry into existing neural networks and apply the method to astate-of-the-art neural network for medical imaging segmentation. We evaluateour symmetry-encoded network on the dataset from a brain tumor segmentationchallenge and verify that the new model extracts information in the trainingimages more efficiently than the original model. ", "id2": "970", "id3": "None"}
{"id": "972", "content": "In this paper, a novel 3D deep learning network is proposed for brain MRimage segmentation with randomized connection, which can decrease thedependency between layers and increase the network capacity. The convolutionalLSTM and 3D convolution are employed as network units to capture the long-termand short-term 3D properties respectively. To assemble these two kinds ofspatial-temporal information and refine the deep learning outcomes, we furtherintroduce an efficient graph-based node selection and label inference method.Experiments have been carried out on two publicly available databases andresults demonstrate that the proposed method can obtain competitiveperformances as compared with other state-of-the-art methods. ", "id2": "971", "id3": "None"}
{"id": "973", "content": "Accurate and reliable brain tumor segmentation is a critical component incancer diagnosis, treatment planning, and treatment outcome evaluation. Buildupon successful deep learning techniques, a novel brain tumor segmentationmethod is developed by integrating fully convolutional neural networks (FCNNs)and Conditional Random Fields (CRFs) in a unified framework to obtainsegmentation results with appearance and spatial consistency. We train a deeplearning based segmentation model using 2D image patches and image slices infollowing steps: 1) training FCNNs using image patches; 2) training CRFs asRecurrent Neural Networks (CRF-RNN) using image slices with parameters of FCNNsfixed; and 3) fine-tuning the FCNNs and the CRF-RNN using image slices.Particularly, we train 3 segmentation models using 2D image patches and slicesobtained in axial, coronal and sagittal views respectively, and combine them tosegment brain tumors using a voting based fusion strategy. Our method couldsegment brain images slice-by-slice, much faster than those based on imagepatches. We have evaluated our method based on imaging data provided by theMultimodal Brain Tumor Image Segmentation Challenge (BRATS) 2013, BRATS 2015and BRATS 2016. The experimental results have demonstrated that our methodcould build a segmentation model with Flair, T1c, and T2 scans and achievecompetitive performance as those built with Flair, T1, T1c, and T2 scans. ", "id2": "972", "id3": "None"}
{"id2": 1097, "id3": "972", "content": "Accurate and reliable brain tumor segmentation is a critical component incancer diagnosis, treatment planning, and treatment outcome evaluation. Buildupon successful deep learning techniques, a novel brain tumor segmentationmethod is developed by integrating fully convolutional neural networks (FCNNs)and Conditional Random Fields (CRFs) in a unified framework to obtainsegmentation results with appearance and spatial consistency. We train a deeplearning based segmentation model using 2D image patches and image slices infollowing steps: 1) training FCNNs using image patches; 2) training CRFs asRecurrent Neural Networks (CRF-RNN) using image slices with parameters of FCNNsfixed; and 3) fine-tuning the FCNNs and the CRF-RNN using image slices.Particularly, we train 3 segmentation models using 2D image patches and slicesobtained in axial, coronal and sagittal views respectively, and combine them tosegment brain tumors using a voting based fusion strategy. Our method couldsegment brain images slice-by-slice, much faster than those based on imagepatches. We have evaluated our method based on imaging data provided by theMultimodal Brain Tumor Image Segmentation Challenge (BRATS) 2013, BRATS 2015and BRATS 2016. The experimental results have demonstrated that our methodcould build a segmentation model with Flair, T1c, and T2 scans and achievecompetitive performance as those built with Flair, T1, T1c, and T2 scans."}
{"id": "974", "content": "In this paper, a novel label fusion method is proposed for brain magneticresonance image segmentation. This label fusion method is formulated on agraph, which embraces both label priors from atlases and anatomical priors fromtarget image. To represent a pixel in a comprehensive way, three kinds offeature vectors are generated, including intensity, gradient and structuralsignature. To select candidate atlas nodes for fusion, rather than exactsearching, randomized k-d tree with spatial constraint is introduced as anefficient approximation for high-dimensional feature matching. FeatureSensitive Label Prior (FSLP), which takes both the consistency and variety ofdifferent features into consideration, is proposed to gather atlas priors. AsFSLP is a non-convex problem, one heuristic approach is further designed tosolve it efficiently. Moreover, based on the anatomical knowledge, parts of thetarget pixels are also employed as graph seeds to assist the label fusionprocess and an iterative strategy is utilized to gradually update the labelmap. The comprehensive experiments carried out on two publicly availabledatabases give results to demonstrate that the proposed method can obtainbetter segmentation quality. ", "id2": "973", "id3": "None"}
{"id": "975", "content": "In this work, we propose a new segmentation algorithm for images containingconvex objects present in multiple shapes with a high degree of overlap. Theproposed algorithm is carried out in two steps, first we identify the visiblecontours, segment them using concave points and finally group the segmentsbelonging to the same object. The next step is to assign a shape identity tothese grouped contour segments. For images containing objects in multipleshapes we begin first by identifying shape classes of the contours followed byassigning a shape entity to these classes. We provide a comprehensiveexperimentation of our algorithm on two crystal image datasets. One datasetcomprises of images containing objects in multiple shapes overlapping eachother and the other dataset contains standard images with objects present in asingle shape. We test our algorithm against two baselines, with our proposedalgorithm outperforming both the baselines. ", "id2": "974", "id3": "None"}
{"id": "976", "content": "Image analysis using more than one modality (i.e. multi-modal) has beenincreasingly applied in the field of biomedical imaging. One of the challengesin performing the multimodal analysis is that there exist multiple schemes forfusing the information from different modalities, where such schemes areapplication-dependent and lack a unified framework to guide their designs. Inthis work we firstly propose a conceptual architecture for the image fusionschemes in supervised biomedical image analysis: fusing at the feature level,fusing at the classifier level, and fusing at the decision-making level.Further, motivated by the recent success in applying deep learning for naturalimage analysis, we implement the three image fusion schemes above based on theConvolutional Neural Network (CNN) with varied structures, and combined into asingle framework. The proposed image segmentation framework is capable ofanalyzing the multi-modality images using different fusing schemessimultaneously. The framework is applied to detect the presence of soft tissuesarcoma from the combination of Magnetic Resonance Imaging (MRI), ComputedTomography (CT) and Positron Emission Tomography (PET) images. It is found fromthe results that while all the fusion schemes outperform the single-modalityschemes, fusing at the feature level can generally achieve the best performancein terms of both accuracy and computational cost, but also suffers from thedecreased robustness in the presence of large errors in any image modalities. ", "id2": "975", "id3": "None"}
{"id": "977", "content": "We propose an attention mechanism for 3D medical image segmentation. Themethod, named segmentation-by-detection, is a cascade of a detection modulefollowed by a segmentation module. The detection module enables a region ofinterest to come to attention and produces a set of object region candidateswhich are further used as an attention model. Rather than dealing with theentire volume, the segmentation module distills the information from thepotential region. This scheme is an efficient solution for volumetric data asit reduces the influence of the surrounding noise which is especially importantfor medical data with low signal-to-noise ratio. Experimental results on 3Dultrasound data of the femoral head shows superiority of the proposed methodwhen compared with a standard fully convolutional network like the U-Net. ", "id2": "976", "id3": "None"}
{"id": "978", "content": "State-of-the-art approaches for semantic image segmentation are built onConvolutional Neural Networks (CNNs). The typical segmentation architecture iscomposed of (a) a downsampling path responsible for extracting coarse semanticfeatures, followed by (b) an upsampling path trained to recover the input imageresolution at the output of the model and, optionally, (c) a post-processingmodule (e.g. Conditional Random Fields) to refine the model predictions.  Recently, a new CNN architecture, Densely Connected Convolutional Networks(DenseNets), has shown excellent results on image classification tasks. Theidea of DenseNets is based on the observation that if each layer is directlyconnected to every other layer in a feed-forward fashion then the network willbe more accurate and easier to train.  In this paper, we extend DenseNets to deal with the problem of semanticsegmentation. We achieve state-of-the-art results on urban scene benchmarkdatasets such as CamVid and Gatech, without any further post-processing modulenor pretraining. Moreover, due to smart construction of the model, our approachhas much less parameters than currently published best entries for thesedatasets.  Code to reproduce the experiments is available here :https://github.com/SimJeg/FC-DenseNet/blob/master/train.py ", "id2": "977", "id3": "None"}
{"id": "979", "content": "Two of the most popular modelling paradigms in computer vision arefeed-forward neural networks (FFNs) and probabilistic graphical models (GMs).Various connections between the two have been studied in recent works, such ase.g. expressing mean-field based inference in a GM as an FFN. This paperestablishes a new connection between FFNs and GMs. Our key observation is thatany FFN implements a certain approximation of a corresponding Bayesian network(BN). We characterize various benefits of having this connection. Inparticular, it results in a new learning algorithm for BNs. We validate theproposed methods for a classification problem on CIFAR-10 dataset and forbinary image segmentation on Weizmann Horse dataset. We show that statisticallylearned BNs improve performance, having at the same time essentially bettergeneralization capability, than their FFN counterparts. ", "id2": "978", "id3": "None"}
{"id": "980", "content": "The quantitative analysis of 3D confocal microscopy images of the shootapical meristem helps understanding the growth process of some plants. Cellsegmentation in these images is crucial for computational plant analysis andmany automated methods have been proposed. However, variations in signalintensity across the image mitigate the effectiveness of those approaches withno easy way for user correction. We propose a web-based collaborative 3D imagesegmentation application, SEGMENT3D, to leverage automatic segmentationresults. The image is divided into 3D tiles that can be either segmentedinteractively from scratch or corrected from a pre-existing segmentation.Individual segmentation results per tile are then automatically merged viaconsensus analysis and then stitched to complete the segmentation for theentire image stack. SEGMENT3D is a comprehensive application that can beapplied to other 3D imaging modalities and general objects. It also provides aneasy way to create supervised data to advance segmentation using machinelearning models. ", "id2": "979", "id3": "None"}
{"id": "981", "content": "Quantitative image analysis often depends on accurate classification ofpixels through a segmentation process. However, imaging artifacts such as thepartial volume effect and sensor noise complicate the classification process.These effects increase the pixel intensity variance of each constituent class,causing intensities from one class to overlap with another. This increasedvariance makes threshold based segmentation methods insufficient due toambiguous overlap regions in the pixel intensity distributions. The classambiguity becomes even more complex for systems with more than twoconstituents, such as unsaturated moist granular media. In this paper, wepropose an image processing workflow that improves segmentation accuracy formultiphase systems. First, the ambiguous transition regions between classes areidentified and removed, which allows for global thresholding of single-classregions. Then the transition regions are classified using a distance function,and finally both segmentations are combined into one classified image. Thisworkflow includes three methodologies for identifying transition pixels and wedemonstrate on a variety of synthetic images that these approaches are able toaccurately separate the ambiguous transition pixels from the single-classregions. For situations with typical amounts of image noise, misclassificationerrors and area differences calculated between each class of the syntheticimages and the resultant segmented images range from 0.69-1.48% and 0.01-0.74%,respectively, showing the segmentation accuracy of this approach. Wedemonstrate that we are able to accurately segment x-ray microtomography imagesof moist granular media using these computationally efficient methodologies. ", "id2": "980", "id3": "None"}
{"id": "982", "content": "This paper proposes a novel image segmentation approachthat integrates fullyconvolutional networks (FCNs) with a level setmodel. Compared with a FCN, theintegrated method can incorporatesmoothing and prior information to achieve anaccurate segmentation.Furthermore, different than using the level set model asa post-processingtool, we integrate it into the training phase to fine-tune theFCN. Thisallows the use of unlabeled data during training in asemi-supervisedsetting. Using two types of medical imaging data (liver CT andleft ven-tricle MRI data), we show that the integrated method achievesgoodperformance even when little training data is available, outperformingtheFCN or the level set model alone. ", "id2": "981", "id3": "None"}
{"id": "983", "content": "In this paper, we present a novel approach to perform deep neural networkslayer-wise weight initialization using Linear Discriminant Analysis (LDA).Typically, the weights of a deep neural network are initialized with: randomvalues, greedy layer-wise pre-training (usually as Deep Belief Network or asauto-encoder) or by re-using the layers from another network (transferlearning). Hence, many training epochs are needed before meaningful weights arelearned, or a rather similar dataset is required for seeding a fine-tuning oftransfer learning. In this paper, we describe how to turn an LDA into either aneural layer or a classification layer. We analyze the initialization techniqueon historical documents. First, we show that an LDA-based initialization isquick and leads to a very stable initialization. Furthermore, for the task oflayout analysis at pixel level, we investigate the effectiveness of LDA-basedinitialization and show that it outperforms state-of-the-art random weightinitialization methods. ", "id2": "982", "id3": "None"}
{"id": "984", "content": "In this paper we present a new framework for the solution of active contourmodels on graphs. With the use of the Finite Element Method we generalizeactive contour models on graphs and reduce the problem from a partialdifferential equation to the solution of a sparse non-linear system.Additionally, we extend the proposed framework to solve models where the curveevolution is locally constrained around its current location. Based on theprevious extension, we propose a fast algorithm for the solution of a widerange active contour models. Last, we present a supervised extension ofGeodesic Active Contours for image segmentation and provide experimentalevidence for the effectiveness of our framework. ", "id2": "983", "id3": "None"}
{"id": "985", "content": "Convolutional neural networks (CNNs) have achieved state-of-the-artperformance for automatic medical image segmentation. However, they have notdemonstrated sufficiently accurate and robust results for clinical use. Inaddition, they are limited by the lack of image-specific adaptation and thelack of generalizability to previously unseen object classes. To address theseproblems, we propose a novel deep learning-based framework for interactivesegmentation by incorporating CNNs into a bounding box and scribble-basedsegmentation pipeline. We propose image-specific fine-tuning to make a CNNmodel adaptive to a specific test image, which can be either unsupervised(without additional user interactions) or supervised (with additionalscribbles). We also propose a weighted loss function considering network andinteraction-based uncertainty for the fine-tuning. We applied this framework totwo applications: 2D segmentation of multiple organs from fetal MR slices,where only two types of these organs were annotated for training; and 3Dsegmentation of brain tumor core (excluding edema) and whole brain tumor(including edema) from different MR sequences, where only tumor cores in one MRsequence were annotated for training. Experimental results show that 1) ourmodel is more robust to segment previously unseen objects than state-of-the-artCNNs; 2) image-specific fine-tuning with the proposed weighted loss functionsignificantly improves segmentation accuracy; and 3) our method leads toaccurate results with fewer user interactions and less user time thantraditional interactive segmentation methods. ", "id2": "984", "id3": "None"}
{"id": "986", "content": "Accurate segmentation of the heart is an important step towards evaluatingcardiac function. In this paper, we present a fully automated framework forsegmentation of the left (LV) and right (RV) ventricular cavities and themyocardium (Myo) on short-axis cardiac MR images. We investigate various 2D and3D convolutional neural network architectures for this task. We investigate thesuitability of various state-of-the art 2D and 3D convolutional neural networkarchitectures, as well as slight modifications thereof, for this task.Experiments were performed on the ACDC 2017 challenge training datasetcomprising cardiac MR images of 100 patients, where manual referencesegmentations were made available for end-diastolic (ED) and end-systolic (ES)frames. We find that processing the images in a slice-by-slice fashion using 2Dnetworks is beneficial due to a relatively large slice thickness. However, theexact network architecture only plays a minor role. We report mean Dicecoefficients of $0.950$ (LV), $0.893$ (RV), and $0.899$ (Myo), respectivelywith an average evaluation time of 1.1 seconds per volume on a modern GPU. ", "id2": "985", "id3": "None"}
{"id": "987", "content": "Digital image segmentation is the process of assigning distinct labels todifferent objects in a digital image, and the fuzzy segmentation algorithm hasbeen successfully used in the segmentation of images from a wide variety ofsources. However, the traditional fuzzy segmentation algorithm fails to segmentobjects that are characterized by textures whose patterns cannot besuccessfully described by simple statistics computed over a very restrictedarea. In this paper, we propose an extension of the fuzzy segmentationalgorithm that uses adaptive textural affinity functions to perform thesegmentation of such objects on bidimensional images. The adaptive affinityfunctions compute their appropriate neighborhood size as they compute thetexture descriptors surrounding the seed spels (spatial elements), according tothe characteristics of the texture being processed. The algorithm then segmentsthe image with an appropriate neighborhood for each object. We performedexperiments on mosaic images that were composed using images from the Brodatzdatabase, and compared our results with the ones produced by a recentlypublished texture segmentation algorithm, showing the applicability of ourmethod. ", "id2": "986", "id3": "None"}
{"id": "988", "content": "Efficient and real time segmentation of color images has a variety ofimportance in many fields of computer vision such as image compression, medicalimaging, mapping and autonomous navigation. Being one of the mostcomputationally expensive operation, it is usually done through software imple-mentation using high-performance processors. In robotic systems, however, withthe constrained platform dimensions and the need for portability, low powerconsumption and simultaneously the need for real time image segmentation, weenvision hardware parallelism as the way forward to achieve higheracceleration. Field-programmable gate arrays (FPGAs) are among the best suitedfor this task as they provide high computing power in a small physical area.They exceed the computing speed of software based implementations by breakingthe paradigm of sequential execution and accomplishing more per clock cycleoperations by enabling hardware level parallelization at an architecturallevel. In this paper, we propose three novel architectures of a well knownEfficient Graph based Image Segmentation algorithm. These proposedimplementations optimizes time and power consumption when compared to softwareimplementations. The hybrid design proposed, has notable furtherance ofacceleration capabilities delivering atleast 2X speed gain over other implemen-tations, which henceforth allows real time image segmentation that can bedeployed on Mobile Robotic systems. ", "id2": "987", "id3": "None"}
{"id": "989", "content": "In this paper, we propose spatial propagation networks for learning theaffinity matrix for vision tasks. We show that by constructing a row/columnlinear propagation model, the spatially varying transformation matrix exactlyconstitutes an affinity matrix that models dense, global pairwise relationshipsof an image. Specifically, we develop a three-way connection for the linearpropagation model, which (a) formulates a sparse transformation matrix, whereall elements can be the output from a deep CNN, but (b) results in a denseaffinity matrix that effectively models any task-specific pairwise similaritymatrix. Instead of designing the similarity kernels according to image featuresof two points, we can directly output all the similarities in a purelydata-driven manner. The spatial propagation network is a generic framework thatcan be applied to many affinity-related tasks, including but not limited toimage matting, segmentation and colorization, to name a few. Essentially, themodel can learn semantically-aware affinity values for high-level vision tasksdue to the powerful learning capability of the deep neural network classifier.We validate the framework on the task of refinement for image segmentationboundaries. Experiments on the HELEN face parsing and PASCAL VOC-2012 semanticsegmentation tasks show that the spatial propagation network provides ageneral, effective and efficient solution for generating high-qualitysegmentation results. ", "id2": "988", "id3": "None"}
{"id": "990", "content": "Marking tumors and organs is a challenging task suffering from both inter-and intra-observer variability. The literature quantifies observer variabilityby generating consensus among multiple experts when they mark the same image.Automatically building consensus contours to establish quality assurance forimage segmentation is presently absent in the clinical practice. As the emph big data  becomes more and more available, techniques to access a largenumber of existing segments of multiple experts becomes possible. Fastalgorithms are, hence, required to facilitate the search for similar cases. Thepresent work puts forward a potential framework that tested with small datasets(both synthetic and real images) displays the reliability of finding similarimages. In this paper, the idea of content-based barcodes is used to retrievesimilar cases in order to build consensus contours in medical imagesegmentation. This approach may be regarded as an extension of the conventionalatlas-based segmentation that generally works with rather small atlases due torequired computational expenses. The fast segment-retrieval process viabarcodes makes it possible to create and use large atlases, something thatdirectly contributes to the quality of the consensus building. Because theaccuracy of experts contours must be measured, we first used 500 syntheticprostate images with their gold markers and delineations by 20 simulated users.The fast barcode-guided computed consensus delivered an average error of$8 % ! pm !5 %$ compared against the gold standard segments. Furthermore, weused magnetic resonance images of prostates from 15 patients delineated by 5oncologists and selected the best delineations to serve as the gold-standardsegments. The proposed barcode atlas achieved a Jaccard overlap of$87 % ! pm !9 %$ with the contours of the gold-standard segments. ", "id2": "989", "id3": "None"}
{"id": "991", "content": "Side-look synthetic aperture sonar (SAS) can produce very high quality imagesof the sea-floor. When viewing this imagery, a human observer can often easilyidentify various sea-floor textures such as sand ripple, hard-packed sand, seagrass and rock. In this paper, we present the Possibilistic Fuzzy LocalInformation C-Means (PFLICM) approach to segment SAS imagery into sea-floorregions that exhibit these various natural textures. The proposed PFLICM methodincorporates fuzzy and possibilistic clustering methods and leverages (local)spatial information to perform soft segmentation. Results are shown on severalSAS scenes and compared to alternative segmentation approaches. ", "id2": "990", "id3": "None"}
{"id": "992", "content": "Automatic skin lesion segmentation on dermoscopic images is an essential stepin computer-aided diagnosis of melanoma. However, this task is challenging dueto significant variations of lesion appearances across different patients. Thischallenge is further exacerbated when dealing with a large amount of imagedata. In this paper, we extended our previous work by developing a deepernetwork architecture with smaller kernels to enhance its discriminant capacity.In addition, we explicitly included color information from multiple colorspaces to facilitate network training and thus to further improve thesegmentation performance. We extensively evaluated our method on the ISBI 2017skin lesion segmentation challenge. By training with the 2000 challengetraining images, our method achieved an average Jaccard Index (JA) of 0.765 onthe 600 challenge testing images, which ranked itself in the first place in thechallenge ", "id2": "991", "id3": "None"}
{"id": "993", "content": "In this research, we propose a deep learning based approach for speeding upthe topology optimization methods. The problem we seek to solve is the layoutproblem. The main novelty of this work is to state the problem as an imagesegmentation task. We leverage the power of deep learning methods as theefficient pixel-wise image labeling technique to perform the topologyoptimization. We introduce convolutional encoder-decoder architecture and theoverall approach of solving the above-described problem with high performance.The conducted experiments demonstrate the significant acceleration of theoptimization process. The proposed approach has excellent generalizationproperties. We demonstrate the ability of the application of the proposed modelto other problems. The successful results, as well as the drawbacks of thecurrent method, are discussed. ", "id2": "992", "id3": "None"}
{"id": "994", "content": "An autonomous robot should be able to evaluate the affordances that areoffered by a given situation. Here we address this problem by designing asystem that can densely predict affordances given only a single 2D RGB image.This is achieved with a convolutional neural network (ResNet), which we combinewith refinement modules recently proposed for addressing semantic imagesegmentation. We define a novel cost function, which is able to handle(potentially multiple) affordances of objects and their parts in a pixel-wisemanner even in the case of incomplete data. We perform qualitative as well asquantitative evaluations with simulated and real data assessing 15 differentaffordances. In general, we find that affordances, which are well-enoughrepresented in the training data, are correctly recognized with a substantialfraction of correctly assigned pixels. Furthermore, we show that our modeloutperforms several baselines. Hence, this method can give clear actionguidelines for a robot. ", "id2": "993", "id3": "None"}
{"id": "995", "content": "This paper proposes an end-to-end trainable network, SegFlow, forsimultaneously predicting pixel-wise object segmentation and optical flow invideos. The proposed SegFlow has two branches where useful information ofobject segmentation and optical flow is propagated bidirectionally in a unifiedframework. The segmentation branch is based on a fully convolutional network,which has been proved effective in image segmentation task, and the opticalflow branch takes advantage of the FlowNet model. The unified framework istrained iteratively offline to learn a generic notion, and fine-tuned onlinefor specific objects. Extensive experiments on both the video objectsegmentation and optical flow datasets demonstrate that introducing opticalflow improves the performance of segmentation and vice versa, against thestate-of-the-art algorithms. ", "id2": "994", "id3": "None"}
{"id": "996", "content": "Accurate medical image segmentation is essential for diagnosis, surgicalplanning and many other applications. Convolutional Neural Networks (CNNs) havebecome the state-of-the-art automatic segmentation methods. However, fullyautomatic results may still need to be refined to become accurate and robustenough for clinical use. We propose a deep learning-based interactivesegmentation method to improve the results obtained by an automatic CNN and toreduce user interactions during refinement for higher accuracy. We use one CNNto obtain an initial automatic segmentation, on which user interactions areadded to indicate mis-segmentations. Another CNN takes as input the userinteractions with the initial segmentation and gives a refined result. Wepropose to combine user interactions with CNNs through geodesic distancetransforms, and propose a resolution-preserving network that gives a betterdense prediction. In addition, we integrate user interactions as hardconstraints into a back-propagatable Conditional Random Field. We validated theproposed framework in the context of 2D placenta segmentation from fetal MRIand 3D brain tumor segmentation from FLAIR images. Experimental results showour method achieves a large improvement from automatic CNNs, and obtainscomparable and even higher accuracy with fewer user interventions and less timecompared with traditional interactive methods. ", "id2": "995", "id3": "None"}
{"id": "997", "content": "In the isointense stage, the accurate volumetric image segmentation is achallenging task due to the low contrast between tissues. In this paper, wepropose a novel very deep network architecture based on a densely convolutionalnetwork for volumetric brain segmentation. The proposed network architectureprovides a dense connection between layers that aims to improve the informationflow in the network. By concatenating features map of fine and coarse denseblocks, it allows capturing multi-scale contextual information. Experimentalresults demonstrate significant advantages of the proposed method over existingmethods, in terms of both segmentation accuracy and parameter efficiency inMICCAI grand challenge on 6-month infant brain MRI segmentation. ", "id2": "996", "id3": "None"}
{"id2": 1098, "id3": "996", "content": "In the isointense stage, the accurate volumetric image segmentation is achallenging task due to the low contrast between tissues. In this paper, wepropose a novel very deep network architecture based on a densely convolutionalnetwork for volumetric brain segmentation. The proposed network architectureprovides a dense connection between layers that aims to improve the informationflow in the network. By concatenating features map of fine and coarse denseblocks, it allows capturing multi-scale contextual information. Experimentalresults demonstrate significant advantages of the proposed method over existingmethods, in terms of both segmentation accuracy and parameter efficiency inMICCAI grand challenge on 6-month infant brain MRI segmentation."}
{"id": "998", "content": "For complex segmentation tasks, fully automatic systems are inherentlylimited in their achievable accuracy for extracting relevant objects.Especially in cases where only few data sets need to be processed for a highlyaccurate result, semi-automatic segmentation techniques exhibit a clear benefitfor the user. One area of application is medical image processing during anintervention for a single patient. We propose a learning-based cooperativesegmentation approach which includes the computing entity as well as the userinto the task. Our system builds upon a state-of-the-art fully convolutionalartificial neural network (FCN) as well as an active user model for training.During the segmentation process, a user of the trained system can iterativelyadd additional hints in form of pictorial scribbles as seed points into the FCNsystem to achieve an interactive and precise segmentation result. Thesegmentation quality of interactive FCNs is evaluated. Iterative FCN approachescan yield superior results compared to networks without the user input channelcomponent, due to a consistent improvement in segmentation quality after eachinteraction. ", "id2": "997", "id3": "None"}
{"id2": 1099, "id3": "997", "content": "For complex segmentation tasks, fully automatic systems are inherentlylimited in their achievable accuracy for extracting relevant objects.Especially in cases where only few data sets need to be processed for a highlyaccurate result, semi-automatic segmentation techniques exhibit a clear benefitfor the user. One area of application is medical image processing during anintervention for a single patient. We propose a learning-based cooperativesegmentation approach which includes the computing entity as well as the userinto the task. Our system builds upon a state-of-the-art fully convolutionalartificial neural network (FCN) as well as an active user model for training.During the segmentation process, a user of the trained system can iterativelyadd additional hints in form of pictorial scribbles as seed points into the FCNsystem to achieve an interactive and precise segmentation result. Thesegmentation quality of interactive FCNs is evaluated. Iterative FCN approachescan yield superior results compared to networks without the user input channelcomponent, due to a consistent improvement in segmentation quality after eachinteraction."}
{"id": "999", "content": "Low-shot learning methods for image classification support learning fromsparse data. We extend these techniques to support dense semantic imagesegmentation. Specifically, we train a network that, given a small set ofannotated images, produces parameters for a Fully Convolutional Network (FCN).We use this FCN to perform dense pixel-level prediction on a test image for thenew semantic class. Our architecture shows a 25% relative meanIoU improvementcompared to the best baseline methods for one-shot segmentation on unseenclasses in the PASCAL VOC 2012 dataset and is at least 3 times faster. ", "id2": "998", "id3": "None"}
{"id": "1000", "content": "Semantic image segmentation is an important computer vision task that isdifficult because it consists of both recognition and segmentation. The task isoften cast as a structured output problem on an exponentially largeoutput-space, which is typically modeled by a discrete probabilistic model. Thebest segmentation is found by inferring the Maximum a-Posteriori (MAP) solutionover the output distribution defined by the model. Due to limitations inoptimization, the model cannot be arbitrarily complex. This leads to atrade-off: devise a more accurate model that incorporates rich high-orderinteractions between image elements at the cost of inaccurate and possiblyintractable optimization OR leverage a tractable model which produces lessaccurate MAP solutions but may contain high quality solutions as other modes ofits output distribution.  This thesis investigates the latter and presents a two stage approach tosemantic segmentation. In the first stage a tractable segmentation modeloutputs a set of high probability segmentations from the underlyingdistribution that are not just minor perturbations of each other. Criticallythe output of this stage is a diverse set of plausible solutions and not just asingle one. In the second stage, a discriminatively trained re-ranking modelselects the best segmentation from this set. The re-ranking stage can use muchmore complex features than what could be tractably used in the segmentationmodel, allowing a better exploration of the solution space than simplyreturning the MAP solution. The formulation is agnostic to the underlyingsegmentation model (e.g. CRF, CNN, etc.) and optimization algorithm, whichmakes it applicable to a wide range of models and inference methods. Evaluationof the approach on a number of semantic image segmentation benchmark datasetshighlight its superiority over inferring the MAP solution. ", "id2": "999", "id3": "None"}
{"id2": 1100, "id3": "999", "content": "Semantic image segmentation is an important computer vision task that isdifficult because it consists of both recognition and segmentation. The task isoften cast as a structured output problem on an exponentially largeoutput-space, which is typically modeled by a discrete probabilistic model. Thebest segmentation is found by inferring the Maximum a-Posteriori (MAP) solutionover the output distribution defined by the model. Due to limitations inoptimization, the model cannot be arbitrarily complex. This leads to atrade-off: devise a more accurate model that incorporates rich high-orderinteractions between image elements at the cost of inaccurate and possiblyintractable optimization OR leverage a tractable model which produces lessaccurate MAP solutions but may contain high quality solutions as other modes ofits output distribution. This thesis investigates the latter and presents a two stage approach tosemantic segmentation. In the first stage a tractable segmentation modeloutputs a set of high probability segmentations from the underlyingdistribution that are not just minor perturbations of each other. Criticallythe output of this stage is a diverse set of plausible solutions and not just asingle one. In the second stage, a discriminatively trained re-ranking modelselects the best segmentation from this set. The re-ranking stage can use muchmore complex features than what could be tractably used in the segmentationmodel, allowing a better exploration of the solution space than simplyreturning the MAP solution. The formulation is agnostic to the underlyingsegmentation model (e.g. CRF, CNN, etc.) and optimization algorithm, whichmakes it applicable to a wide range of models and inference methods. Evaluationof the approach on a number of semantic image segmentation benchmark datasetshighlight its superiority over inferring the MAP solution."}
